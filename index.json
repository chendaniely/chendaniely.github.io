[{"authors":["admin"],"categories":null,"content":"I am a PhD student at Virginia Tech (VT) in Genetics, Bioinformatics, and Computational Biology (GBCB) working with Anne Brown and DataBridge studying data science education and pedagogyfor medicial practitioners.\nFormer RStudio intern working on the gradethis package.\nArch Linux user maintaining the RStudio Preview, nteract, and Rodeo packages in the AUR.\nAuthor of Pandas for Everyone.\nEnjoys: photography, curling, snowboarding, and scuba diving into caves.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1579121255,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at Virginia Tech (VT) in Genetics, Bioinformatics, and Computational Biology (GBCB) working with Anne Brown and DataBridge studying data science education and pedagogyfor medicial practitioners.\nFormer RStudio intern working on the gradethis package.\nArch Linux user maintaining the RStudio Preview, nteract, and Rodeo packages in the AUR.\nAuthor of Pandas for Everyone.\nEnjoys: photography, curling, snowboarding, and scuba diving into caves.","tags":null,"title":"Daniel Chen","type":"authors"},{"authors":null,"categories":null,"content":"Here you can find my Python related courses.\nA table of public courses can be found here:\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1561957487,"objectID":"298ad7f7e1cf34d15656d8b0546e7486","permalink":"/courses/python/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/python/","section":"courses","summary":"Python, Pandas, and Machine Learning","tags":null,"title":"Python","type":"docs"},{"authors":null,"categories":null,"content":"R related courses.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1561957487,"objectID":"ed30ee946686e7de08f389bf9a77a3a1","permalink":"/courses/r/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/r/","section":"courses","summary":"R thing(s)","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1561957487,"objectID":"2ee1bda79c77aa575e6c2506fb21d511","permalink":"/courses/git/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/git/","section":"courses","summary":"Learn Git things","tags":null,"title":"Git","type":"docs"},{"authors":null,"categories":null,"content":" Git Essentials LiveLessons\nby Daniel Y. Chen\nPublisher: Addison-Wesley Professional\nRelease Date: September 2016\nISBN: 9780134655284\nhttps://www.oreilly.com/library/view/git-essentials-livelessons/9780134655284/\nVideo Description Git has emerged as the leading revision control system for open source projects. Git is a distributed revision control and source code management (SCM) system with an emphasis on speed. Description\nGit Essentials LiveLessons teaches the skills necessary to version control with git. The first part of the course begins with the basics of Git and how to use it as an individual programmer. Part 1: Git Fundamentals covers installation and setup, fundamental commands, and how to use remote and branches in git using Github as the online hosting service to lay the foundation for more advanced workflows.\nAfter the basics are covered, Part 2: Collaborating with Git shows how to work collaboratively with teams with git. Three common ways of collaboration are covered, including: adding other users to your project repository, forking a copy of repository and submitting changes for review, and finally using the git flow workflow.\nSkill Level Beginner\nYou Will Learn How To Set up and install git\nUse basic git commands\nUnderstand remotes and branches\nAdd a collaborator\nPush/pull branches and incorporate changes\nFork a repository and make changes while staying in sync\nWork with branches\nUse a Git Flow workflow\nWho Should Take This Course Individuals who want to incorporate version control and more robust collaboration methods into their workflow.\nCourse Pre-Requisites Basic command line skills: - moving to directories - creating and editing plain text files\nTable of Contents Introduction to Git Essentials\nIntroduction to Part I\nPart I: Git Basics\nLesson 1: Setting Up Git\nLearning objectives\n1.1 Understand what can Git do for you\n1.2 Install Git\n1.3 Configure Git\nLesson 2: Understanding and Using Git- The Basics\nLearning objectives\n2.1 Create a Git repository (init, status)\n2.2 Track changes (add, commit, log; what is master)\n2.3 Look at differences\n2.4 Look around\n2.5 Undo changes\n2.6 Ignore things in Git\nLesson 3: Remotes and Branches\nLearning objectives\n3.1 Understand what GitHub is and how it\u0026rsquo;s used\n3.2 Use remotes with https\n3.3 Use remotes with ssh\n3.4 Utilize self collaboration\n3.5 Use Git for project management\n3.6 Use branches on your own\n3.7 Understand Git workflows\n3.8 Incorporating changes after you branch\n3.9 Use Git GUIs\nIntroduction to Part II\nPart II: Collaborating with Git\nLesson 4: Adding a Collaborator\nLearning objectives\n4.1 Add a collaborator, push and pull down changes\n4.2 Push/pull with branches and request a pull\n4.3 Incorporate changes in existing branches\nLesson 5: Forking Workflow of Collaboration\nLearning objectives\n5.1 Fork a repository and making a change as a collaborator\n5.2 Working with changes and staying in sync\n5.3 Working with branches\nLesson 6: Git Flow Workflow of Collaboration\nLearning objectives\n6.1 Forking and making changes on a Git Flow repository\n6.2 Working with changes and staying in sync\n6.3 Staying in sync with conflicts\nSummary\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"41f7353bf02151c98aa5e5149df1b660","permalink":"/courses/git/git_essentials/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/git/git_essentials/","section":"courses","summary":"Git Essentials LiveLessons\nby Daniel Y. Chen\nPublisher: Addison-Wesley Professional\nRelease Date: September 2016\nISBN: 9780134655284\nhttps://www.oreilly.com/library/view/git-essentials-livelessons/9780134655284/\nVideo Description Git has emerged as the leading revision control system for open source projects. Git is a distributed revision control and source code management (SCM) system with an emphasis on speed. Description\nGit Essentials LiveLessons teaches the skills necessary to version control with git. The first part of the course begins with the basics of Git and how to use it as an individual programmer.","tags":null,"title":"Git Essentials LiveLessons","type":"docs"},{"authors":null,"categories":null,"content":" Pandas Data Analysis with Python Fundamentals\nby Daniel Y. Chen\nPublisher: Addison-Wesley Professional\nRelease Date: February 2017\nISBN: 9780134692272\nhttps://www.oreilly.com/library/view/pandas-data-analysis/9780134692272/\nVideo Description 3+ Hours of Video Instruction\nPandas Data Analysis with Python Fundamentals LiveLessons provides analysts and aspiring data scientists with a practical introduction to Python and pandas, the analytics stack that enables you to move from spreadsheet programs such as Excel into automation of your data analysis workflows.\nIn this video training, Daniel starts by introducing Python and pandas and why they are great tools for data analysis. He then covers installing and starting Python. The video then moves into the basics of working with data sets in Python and with pandas, followed by plotting and visualization, data assembly and manipulations, missing data, and tidy data. After watching this video, analysts and those new to data science will understand why Python and pandas are so popular with data scientists and should be able to begin to create automated data workflows.\nSkill Level Beginner to Intermediate\nWhat You Will Learn Installing and starting Python Loading data sets into pandas and beginning to assess and analyze them Using pandas data structures and importing/exporting data Combining multiple data sets Dealing with missing data Tidying and reshaping data\nWho Should Take This Course Analysts and aspiring data scientists looking to move beyond spreadsheets into automated data workflows.\nCourse Requirements Basic understanding of programming and development Some familiarity with basic data analysis\nTable of Contents Lesson 1: Installing and Running Python\nLesson 1 explains why the Python and pandas combination is great for data analysis. It also shows you how to install Python and the analytics stack and how to run Python.\nLesson 2: Pandas Basics\nLesson 2 covers some of the initial steps to take after you are given a dataset to analyze. You load data into pandas and then look at different subsets of the data. Finally, you learn how to perform your first simple set of analyses.\nLesson 3: Pandas Data Structures\nLesson 3 dives a little further into how pandas works. You learn how to create the pandas series and dataframe data structures. Next, you learn how you can use the pandas series object and pandas dataframe object. Last, how you import and export various types of data are covered.\nLesson 4: Introduction to Plotting\nLesson 4 emphasizes why visualization is important. You learn how to create a basic set of plots within matplotlib, Seaborn, and pandas.\nLesson 5: Data Assembly\nNow that you know how to load and look at your data, the next step is assembling the data you need for analysis. Lesson 5 begins with concatenating data, that is, how to append data along the rows or columns. The lessons end with how to merge multiple data sets together.\nLesson 6: Missing Data\nBy now you have seen a few datasets with missing data. In Lesson 6 we begin to discuss what missing data is and how we get missing data. Your start learning how to work with missing data, including ways to find, count, and clean missing data. These are all important considerations when missing data is used in calculations.\nLesson 7: Tidy Data\nLesson 7 is concerned with tidy data. Tidy data describes the shape of your data that makes it easier to manipulate and analyze. The lesson covers Hadley Wickham’s tidy data paper that describes the ways data can be dirty. Finally, it covers the various ways you can reshape data.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"e46f7bf6150ca1cdfc579209929538ef","permalink":"/courses/python/python_pandas_video/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/python/python_pandas_video/","section":"courses","summary":"Pandas Data Analysis with Python Fundamentals\nby Daniel Y. Chen\nPublisher: Addison-Wesley Professional\nRelease Date: February 2017\nISBN: 9780134692272\nhttps://www.oreilly.com/library/view/pandas-data-analysis/9780134692272/\nVideo Description 3+ Hours of Video Instruction\nPandas Data Analysis with Python Fundamentals LiveLessons provides analysts and aspiring data scientists with a practical introduction to Python and pandas, the analytics stack that enables you to move from spreadsheet programs such as Excel into automation of your data analysis workflows.","tags":null,"title":"Pandas Data Analysis with Python Fundamentals","type":"docs"},{"authors":null,"categories":null,"content":" This course is for R users who want to get up to speed with Python!\nhttps://www.datacamp.com/courses/python-for-r-users\nCourse Description Python and R have seen immense growth in popularity in the \u0026ldquo;Machine Learning Age\u0026rdquo;. They both are high-level languages that are easy to learn and write. The language you use will depend on your background and field of study and work. R is a language made by and for statisticians, whereas Python is a more general purpose programming language. Regardless of the background, there will be times when a particular algorithm is implemented in one language and not the other, a feature is better documented, or simply, the tutorial you found online uses Python instead of R. In either case, this would require the R user to work in Python to get his/her work done, or try to understand how something is implemented in Python for it to be translated into R. This course helps you cross the R-Python language barrier.\nThe Basics Learn about some of the most important data types (integers, floats, strings, and booleans) and data structures (lists, dictionaries, numpy arrays, and pandas DataFrames) in Python and how they compare to the ones in R.\nControl flow, Loops, and Functions This chapter covers control flow statements (if-else if-else), for loops and shows you how to write your own functions in Python!\nPandas In this chapter you will learn more about one of the most important Python libraries, Pandas. In addition to DataFrames, pandas provides several data manipulation functions and methods.\nPlotting You will learn about the rich ecosystem of visualization libraries in Python. This chapter covers matplotlib, the core visualization library in Python along with the pandas and seaborn libraries.\nCapstone As a final capstone, you will apply your Python skills on the NYC Flights 2013 dataset.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"2331fa5816424dd5a3eff1e6bf04e2af","permalink":"/courses/r/dc_python_r/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/r/dc_python_r/","section":"courses","summary":"This course is for R users who want to get up to speed with Python!\nhttps://www.datacamp.com/courses/python-for-r-users\nCourse Description Python and R have seen immense growth in popularity in the \u0026ldquo;Machine Learning Age\u0026rdquo;. They both are high-level languages that are easy to learn and write. The language you use will depend on your background and field of study and work. R is a language made by and for statisticians, whereas Python is a more general purpose programming language.","tags":null,"title":"Python for R Users","type":"docs"},{"authors":null,"categories":null,"content":" Pandas Data Cleaning and Modeling with Python\nby Daniel Y. Chen\nPublisher: Addison-Wesley Professional\nRelease Date: January 2018\nISBN: 9780135170199\nhttps://www.oreilly.com/library/view/pandas-data-cleaning/9780135170199/\nVideo Description The perfect follow up to Pandas Data Analysis with Python Fundamentals LiveLessons for the aspiring data scientist\nOverview In Pandas Data Cleaning and Modeling with Python LiveLessons, Daniel Y. Chen builds upon the foundation he built in Pandas Data Analysis with Python Fundamentals LiveLessons. In this LiveLesson Dan teaches you the techniques and skills you need to know to be able to clean and process your data. Dan shows you how to do data munging using some of the built-in Python libraries that can be used to clean data loaded into Pandas. Once your data is clean you are going to want to analyze it, so next Dan introduces you to other libraries that are used for model fitting.\nSkill Level Beginner to Intermediate\nLearn How To Use pandas data types Convert data types Use string methods and regular expressions Apply functions to data Aggregate, transform, and filter data Use pandas and Python date and time methods Model data\nWho Should Take This Course Those new to data science, particularly those with Python programming experience\nCourse Requirements Basic programming skills, particularly in Python\nLesson Descriptions Lesson 1: Pandas Data Types These lessons pick up where Pandas Data Analysis with Python Fundamentals LiveLessons left off. You learned the basics of subsetting, combining, and reshaping data. Now you can start learning how to cleaning your data. That begins with learning data types and how to find them in your data. Next comes the converting from one type to another, including converting data into numeric and string values. The lesson finishes with categorical data.\nLesson 2: Unstructured Text and Strings in Pandas There are vast stores of data available as unstructured text. Understanding how to work with text data in Python is important when your dataset has text data that needs to be processed. The lesson begins with a basic overview of strings and the built-in python string methods. Next, Dan covers how to format strings. This will make your code more legible and can make the output more consistent and “prettier.” Dan then introduces regular expressions with the built-in regular expressions library (2.5) and how you can use regular expressions to do pattern matching. Finally, Dan shows you a quick example of the better, but not built-in, regex library.\nLesson 3: Applying Functions to Data Applying functions is a fundamental skill when working with data. Application of functions incorporates many skills used in programming and data analytics. Instead of writing for loops to perform calculations and data manipulations, we write functions that work on a column-by-column or row-by-row basis. Dan begins with a quick introduction to functions in Python. Then, he turns to using simple functions on a toy dataset to see how apply works. Next, he applies functions on an actual dataset. You then learn how to write vectorized functions, functions that work on an element-wise basis. Finally, Dan takes a look at lambda functions for one-off calculations.\nLesson 4: Breaking Up Computations Using groupby Operations: split-apply-combine groupby operations follow the mantra of split-apply-combine. Where your data is split and partitioned by a variable or variables, functions are applied to each partition, and the results are combined back into a single result. This technique is utilized heavily on distributed systems when the data no longer can fit on a single machine. There are three common operations when performing a groupby. First, there is aggregation where you summarize your data into a single value. For example, calculating the average life expectancy across each year in your data would be aggregation. Transformation is done when you perform a specific calculation for each individual group. Next, there is filtration, where you reduce your data based on a calculation within a group.\nDan also looks further into the groupby object itself and how you can iterate over your groups. And finally, he demonstrates the multi-index and how you can chain multiple groupby calculations together.\nLesson 5: Dates and Times in Python and Pandas One of pandas’ strong suits is handling dates and times in time-series data. There are many convenient functions and methods that make working and processing datetime data much easier in pandas. Dan begins by looking at Python’s datetime object and how to create them. Next, you learn how you can convert columns in your data into datetime objects. He then shows you how you can directly load data into a datetime without having an intermediate step and then convert it later. Once you have your data stored as a proper date and time object, Dan shows you how you can extract various datetime components and how you can perform calculations and create Timedeltas. Then Dan shows you other functions and methods you can perform on datetimes, and how you can download stock data from the internet. Once you have your data processed the way you want, Dan takes you back to the basics and you learn how you can leverage dates and times to subset your data. From there you learn how you can create ranges of dates, followed by an example of shifting date values. Finally, Dan covers how you can resample your dates and how you can convert dates and times across various time zones.\nLesson 6: Modeling: Connecting to the World Outside of Pandas Once you have your data processed the way you want, you can begin modeling your data to gain insights. This lesson begins to expand our world within pandas to other Python libraries used to model data. Dan begins with linear regression and how it is performed in two very popular modeling libraries: statsmodels and scikit-learn. While linear regression is great if your outcome or response variable is continuous, you can use logistic regression when your outcome of interest is a binary variable. When you begin working with count data, you use a Poisson or negative binomial model, depending on the assumptions and characteristics of your data. Next, Dan introduces you to survival models, when you have censored data and want to model the time a particular event will occur. Dan then covers how you can perform model diagnostics and compare model performance by looking at residuals, ANOVA, AIC, BIC, and k-fold cross validation. He then covers how you can have a more parsimonious model that can better predict future data points by using regularization techniques, and the lesson concludes by introducing clustering techniques and how you can use principal components analysis to visualize your k-means results.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"b8c80d8d1b46fe46938dbe3f6a733e86","permalink":"/courses/python/pandas_ml_video/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/python/pandas_ml_video/","section":"courses","summary":"Pandas Data Cleaning and Modeling with Python\nby Daniel Y. Chen\nPublisher: Addison-Wesley Professional\nRelease Date: January 2018\nISBN: 9780135170199\nhttps://www.oreilly.com/library/view/pandas-data-cleaning/9780135170199/\nVideo Description The perfect follow up to Pandas Data Analysis with Python Fundamentals LiveLessons for the aspiring data scientist\nOverview In Pandas Data Cleaning and Modeling with Python LiveLessons, Daniel Y. Chen builds upon the foundation he built in Pandas Data Analysis with Python Fundamentals LiveLessons. In this LiveLesson Dan teaches you the techniques and skills you need to know to be able to clean and process your data.","tags":null,"title":"Pandas Data Cleaning and Modeling with Python","type":"docs"},{"authors":null,"categories":null,"content":" This course will equip you with all the skills you need to clean your data in Python.\nhttps://www.datacamp.com/courses/cleaning-data-in-python\nCourse Description A vital component of data science involves acquiring raw data and getting it into a form ready for analysis. In fact, it is commonly said that data scientists spend 80% of their time cleaning and manipulating data, and only 20% of their time actually analyzing it. This course will equip you with all the skills you need to clean your data in Python, from learning how to diagnose your data for problems to dealing with missing values and outliers. At the end of the course, you\u0026rsquo;ll apply all of the techniques you\u0026rsquo;ve learned to a case study in which you\u0026rsquo;ll clean a real-world Gapminder dataset!\nExploring your data So you\u0026rsquo;ve just got a brand new dataset and are itching to start exploring it. But where do you begin, and how can you be sure your dataset is clean? This chapter will introduce you to the world of data cleaning in Python! You\u0026rsquo;ll learn how to explore your data with an eye for diagnosing issues such as outliers, missing values, and duplicate rows.\nTidying data for analysis Here, you\u0026rsquo;ll learn about the principles of tidy data and more importantly, why you should care about them and how they make subsequent data analysis more efficient. You\u0026rsquo;ll gain first hand experience with reshaping and tidying your data using techniques such as pivoting and melting.\nCombining data for analysis The ability to transform and combine your data is a crucial skill in data science, because your data may not always come in one monolithic file or table for you to load. A large dataset may be broken into separate datasets to facilitate easier storage and sharing. Or if you are dealing with time series data, for example, you may have a new dataset for each day. No matter the reason, it is important to be able to combine datasets so you can either clean a single dataset, or clean each dataset separately and then combine them later so you can run your analysis on a single dataset. In this chapter, you\u0026rsquo;ll learn all about combining data.\nCleaning data for analysis Here, you\u0026rsquo;ll dive into some of the grittier aspects of data cleaning. You\u0026rsquo;ll learn about string manipulation and pattern matching to deal with unstructured data, and then explore techniques to deal with missing or duplicate data. You\u0026rsquo;ll also learn the valuable skill of programmatically checking your data for consistency, which will give you confidence that your code is running correctly and that the results of your analysis are reliable!\nCase study In this final chapter, you\u0026rsquo;ll apply all of the data cleaning techniques you\u0026rsquo;ve learned in this course towards tidying a real-world, messy dataset obtained from the Gapminder Foundation. Once you\u0026rsquo;re done, not only will you have a clean and tidy dataset, you\u0026rsquo;ll also be ready to start working on your own data science projects using the power of Python!\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"c191b42ace9bf6084f62dc210610ba9e","permalink":"/courses/python/dc_cleaning_data_python/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/python/dc_cleaning_data_python/","section":"courses","summary":"This course will equip you with all the skills you need to clean your data in Python.\nhttps://www.datacamp.com/courses/cleaning-data-in-python\nCourse Description A vital component of data science involves acquiring raw data and getting it into a form ready for analysis. In fact, it is commonly said that data scientists spend 80% of their time cleaning and manipulating data, and only 20% of their time actually analyzing it. This course will equip you with all the skills you need to clean your data in Python, from learning how to diagnose your data for problems to dealing with missing values and outliers.","tags":null,"title":"Cleaning Data in Python","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":"\r\rOpen Broadcaster Software (OBS) is a featured and popular used to lay out your screen for recording and streaming.\rV26 of OBS Studio was released mid October 2020,\rand it ships with a virtual camera feature (windows).\rThis allows you to use the OBS Studio output as the camera input for other programs (e.g., video conferencing software).\nFor teaching purposes, this means you have much more control when you are trying to teach\ronline (and in person) since OBS gives you many options on what and how something is presented.\rOBS also allows you to setup “scenes” so you can potentially share an entire screen,\ror a specific set of application windows all with separate screen overlays.\rFor me I was able to more efficiently lay out my screen with what I’m presenting,\rtext overlays with quick links, and a live caption as I was teaching.\nThe virtual camera used to be an optional\rplugin\rthat needed to be installed,\rbut this current version has been cleaned up with bug fixes.\rIt seems like it’s currently Windows only,\rand you may still need to use the old plugins for Mac + Linux.\nFor my dissertation,\rI am writing an online resource that aims to teach\rdata science skills focused towards people who work in the medical and biomedical sciences (ds4biomed).\rI just finished giving my first workshop using these materials which you can find\rhere (part 1)\rand\rhere (part 2).\rI wanted a way to be able to share my screen but also show a live caption at the same time.\nIn the past I would share my screen and open windows up in the screen,\rbut that method wasted a lot of screen space to lay out all the components I wanted.\rOBS give me much more flexibility with how big each components takes up on the canvas.\nHere’s an example of what was presented to the learners over Zoom.\nSetting up OBS\rSetting up OBS Studio is a separate post on its own.\rLuckily it’s used by streamers,\rso there are lots of YouTube guides on how to use OBS\nHere are 2 resources\nGaming Careers Guide\rAlpha Gaming Guide\r\r\rMy setup\rI only have 1 scene when I’m teaching, but OBS gives you the option to have multiple scenes that you can toggle between.\nMy main teaching screen scene:\nVideo Capture Device: Webcam\rText: Quick links to the Etherpad and workshop content\rColor Source: Black bar to cover the Windows taskbar for the Text to show\rWindow Capture: Chrome window opened to Web Captioner\rAudio Output Capture: Used to capture sound from the computer (from applications like zoom or videos)\rAudio Input Capture: My microphone\rDisplay Capture: Screen I use for screen sharing.\r\r\rAn option here could be a series of application window captures instead of sharing the entire screen\rYou can have your own layout of source code and terminal output and not worry about windows overlapping on your end\r\rOther things that could be useful is to have a scene that is setup in advanced to a certain part of the screen,\re.g., the source code, or output,\rso you can zoom in to a certain area for emphasis and clarity.\nYou can also have a separate scene for a timer if you want a more explicit break timer.\nAnother useful scene is just your webcam and audio input capture so your webcam shows up like a normal webcam to people.\rThe Scenes let you swap between all the different views out of OBS and into the virtual camera on the fly.\n\rUsing the Virtual Camera\rOnce you have OBS setup with your scenes and sources,\ryou can click the “Start Virtual Camera” button.\rOnce you started that, you will see a new “webcam” option in your video conference software that will output what your OBS preview shows.\nWhen using video conferencing software, e.g.,. Zoom, to teach,\ryou need to remember you are no longer “sharing” your screen,\rsince your webcam display is already setup to share your screen,\rso you need to tell people to either “pin” your video so it does not loose focus when other people talk,\ror mute microphones so people do not take away your focus.\rHow this is done will vary between applications.\n\rOther OBS tips\rUsing OBS you can set it up to stream to services like Twitch or YouTube,\rand it also gives you the ability to “record” the stream.\rIf you do decide to record out of OBS,\rI suggest you use .mkv format to record since mkv files will still work if something happens and the recording crashes.\rYou can use programs like HandBrake\rto convert video formats.\n\rLearner screen layout\rI wrote a post earlier this year for The Carpentries on how learners\rmay lay out their screens during an online workshop.\rYou can see the blog post here:\rhttps://carpentries.org/blog/2020/06/online-workshop-logistics-and_screen-layouts/\n\rSummary\rUsing the OBS virtual camera for online teaching gives the instructor a lot of flexibility on what and how something would be presented to the learners.\rFor instructors who have limited screen space,\rOBS allows you to lay out individual application windows so you can have your notes over what is presented without it affecting what the learners see.\rAdditionally, OBS’s scenes allow the instructor to pre set up different views that can be changed during the lesson as needed.\rThere is a learning curve to OBS, but because of it’s use in the streaming community,\rthere are a plethora of resources available.\n\r","date":1603756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603806134,"objectID":"99b237fb90eece6680dd4c2419646eb6","permalink":"/2020/10/27/using-obs-for-online-teaching/","publishdate":"2020-10-27T00:00:00Z","relpermalink":"/2020/10/27/using-obs-for-online-teaching/","section":"post","summary":"Open Broadcaster Software (OBS) is a featured and popular used to lay out your screen for recording and streaming.\rV26 of OBS Studio was released mid October 2020,\rand it ships with a virtual camera feature (windows).\rThis allows you to use the OBS Studio output as the camera input for other programs (e.g., video conferencing software).\nFor teaching purposes, this means you have much more control when you are trying to teach\ronline (and in person) since OBS gives you many options on what and how something is presented.","tags":["teaching","obs","zoom"],"title":"Using OBS for Online Teaching","type":"post"},{"authors":[],"categories":["making"],"content":"\r\rDownload software\r\rArduino: https://www.arduino.cc/en/Main/Software\rMarlin and configurations: https://marlinfw.org/meta/download/\r\rYou need the Marlin firmware in the “Downloads” column and the configurations from the “Configuration” column.\r\r\rVideo guide from “The First Layer”: https://www.youtube.com/watch?v=-rOZpOiWYxM\n\rLoad up printer-specific configurations\rIn the Configurations navigate to config \u0026gt; examples \u0026gt; Creality \u0026gt; CR-10s and copy-paste the files into the Marlin-2.0.x \u0026gt; Marlin folder.\rIt should prompt you to overwrite the existing Configuration.h and Configuration_adv.h files.\n\rLoad up the firmware\rRun the Marlin.ino file from the Marlin folder.\rIt should open up the Arduino IDE with the Marlin, Configuration.h, Configuration_adv.h, Version.h, _Bootscreen.h, and _Statusscreen.h files.\n\rSelect the correct Arduino boards\r\rTools \u0026gt; board \u0026gt; Arduino Mega or Mega 2560\rTools \u0026gt; processor \u0026gt; ATmega2560 (Mega2650)\r\r\rMake edits to firmware files\rSee here for the changes I made: https://github.com/chendaniely/3d-printer/pull/1\nThe relevant configurations you’d want to make for manual mesh leveling are:\n\rDisable auto bed leveling bilinear: //#define AUTO_BED_LEVELING_BILINEAR\rEnable mesh bed leveling: #define MESH_BED_LEVELING\rEnable restore mesh: #define RESTORE_LEVELING_AFTER_G28\r\rIf you are upgrading Marlin you’d also want to enable EPROM auto init: #define EEPROM_AUTO_INIT\n\rFlash the firmware\r\rPlug in your printer\rverify\r\rIf you run into any errors, this is a good video walkthrough of potential issues:\rhttps://www.youtube.com/watch?v=lAKyZd63_ns\nI was having an error around #include \u0026lt;U8glib.h\u0026gt;\nU8glib.h\r\rhttps://github.com/olikraus/u8glib\r\rScroll down to “U8glib for Arduino”\nDownload the library, and then in the Arduino editor Sketch \u0026gt; Include Library \u0026gt; Add ZIP library\n\r\r","date":1590278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590356058,"objectID":"55b6e3dab17b4144ac906f40754502e2","permalink":"/2020/05/24/upgrading-marlin-2-0-with-mesh-bed-levling-cr-10s/","publishdate":"2020-05-24T00:00:00Z","relpermalink":"/2020/05/24/upgrading-marlin-2-0-with-mesh-bed-levling-cr-10s/","section":"post","summary":"Download software\r\rArduino: https://www.arduino.cc/en/Main/Software\rMarlin and configurations: https://marlinfw.org/meta/download/\r\rYou need the Marlin firmware in the “Downloads” column and the configurations from the “Configuration” column.\r\r\rVideo guide from “The First Layer”: https://www.youtube.com/watch?v=-rOZpOiWYxM\n\rLoad up printer-specific configurations\rIn the Configurations navigate to config \u0026gt; examples \u0026gt; Creality \u0026gt; CR-10s and copy-paste the files into the Marlin-2.0.x \u0026gt; Marlin folder.\rIt should prompt you to overwrite the existing Configuration.","tags":["3d-print","makers"],"title":"Upgrading Marlin 2.0+ with Mesh Bed Levling CR-10s","type":"post"},{"authors":[],"categories":[],"content":"\r\rAnaconda\r(and these days\rminiconda)\rhas been my go-to for getting Python and the scientific/data science software stack\rinstalled on my computer\r(even on my Arch linux machine!).\nWhen it first came out it was the first time I was able to install and use\rpandas and the rest of the scipy stack.\rI’ve stuck with it ever since.\nIt was also the first time I used (and understood) virtual environments.\nIn this post I’m hoping to show you a quick guide to using virtual environments with conda.\rYou can find more details on how to\rmanage your conda environment\rhere:\nhttps://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\nProblem\rHere are some situations virtual environments solve:\nYou have code that needs to run in a different version of python than the version you have on your computer (e.g., I have Python (3) and need to run a script in Legacy Python (2))\rThe code I want to run uses a different version of package(s) than what I have installed (e.g., you need an older or newer version of pandas)\rYou can’t install and/or import a package because of package dependency conflicts\rAll the code and packages work on your computer, and you want a way to give someone else the same set of packages so they can run your code\rYou want to install an experimental package and do not want to mess up your current environment.\r\r\rWhat are virtual environments\rA virtual environment allows you to create an isolated (i.e., virtual) “environment”\rfor your software, in this case, Python packages.\rThis allows you to “activate” different python environments on your computer\rwithout having to uninstall everything just to run code from different projects.\n\rCreating conda environments\rWe’ll be talking about conda environments in this post.\rDepending on how you installed anaconda/miniconda/conda make sure you’re in an environment\rthat can run conda commands\nGetting in the right place\rYou should see (base) in the beginning of a prompt. This is what it looks like on my computer, the (base) tells me I am in the “base” conda environment.\n(base) $\rYou can see here the default python is the one installed in my miniconda directory.\n(base) $ which python\r/home/dchen/miniconda3/bin/python\rWe can run conda info --envs to get a list of all the environments we have\n(base) $ conda info --envs\r# conda environments:\r#\rbase * /home/dchen/miniconda3\r\rCreating an environment\rI’m currently using Python 3.\n(base) $ python --version\rPython 3.7.6\rLet’s create another environment named py27 that’s Legacy Python (i.e., Python 2).\rWe do this with the conda create command.\rWe pass in the --name or -n flag to give a name to the new environment.\rWe can also pass in python=2.7 to put Python 2.7 in this environment.\rHere you can specify any version of Python you want.\rFor example, it can be another Python 3.7 environment for another Python 3 project.\n(base) $ conda create -n py27 python=2.7\rIt’ll will build the plan for you, all you need to do is hit enter (it defaults to y)\nCollecting package metadata (current_repodata.json): done\rSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\rCollecting package metadata (repodata.json): done\rSolving environment: done\r## Package Plan ##\renvironment location: /home/dchen/miniconda3/envs/py27\radded / updated specs:\r- python=2.7\rThe following packages will be downloaded:\rpackage | build\r---------------------------|-----------------\rcertifi-2019.11.28 | py27_0 149 KB conda-forge\rpip-20.0.2 | py_2 1.0 MB conda-forge\rpython-2.7.15 | h5a48372_1009 12.7 MB conda-forge\rsetuptools-44.0.0 | py27_0 663 KB conda-forge\rwheel-0.34.2 | py_1 24 KB conda-forge\r------------------------------------------------------------\rTotal: 14.6 MB\rThe following NEW packages will be INSTALLED:\r_libgcc_mutex conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\r_openmp_mutex conda-forge/linux-64::_openmp_mutex-4.5-0_gnu\rca-certificates conda-forge/linux-64::ca-certificates-2019.11.28-hecc5488_0\rcertifi conda-forge/linux-64::certifi-2019.11.28-py27_0\rlibffi conda-forge/linux-64::libffi-3.2.1-he1b5a44_1006\rlibgcc-ng conda-forge/linux-64::libgcc-ng-9.2.0-h24d8f2e_2\rlibgomp conda-forge/linux-64::libgomp-9.2.0-h24d8f2e_2\rlibstdcxx-ng conda-forge/linux-64::libstdcxx-ng-9.2.0-hdf63c60_2\rncurses conda-forge/linux-64::ncurses-6.1-hf484d3e_1002\ropenssl conda-forge/linux-64::openssl-1.1.1d-h516909a_0\rpip conda-forge/noarch::pip-20.0.2-py_2\rpython conda-forge/linux-64::python-2.7.15-h5a48372_1009\rreadline conda-forge/linux-64::readline-8.0-hf8c457e_0\rsetuptools conda-forge/linux-64::setuptools-44.0.0-py27_0\rsqlite conda-forge/linux-64::sqlite-3.30.1-hcee41ef_0\rtk conda-forge/linux-64::tk-8.6.10-hed695b0_0\rwheel conda-forge/noarch::wheel-0.34.2-py_1\rzlib conda-forge/linux-64::zlib-1.2.11-h516909a_1006\rProceed ([y]/n)?\rWhen it’s done you’ll actually see a message on how to “activate” your new environment.\nDownloading and Extracting Packages\rsetuptools-44.0.0 | 663 KB | ######################################################################## | 100%\rwheel-0.34.2 | 24 KB | ######################################################################## | 100%\rpip-20.0.2 | 1.0 MB | ######################################################################## | 100%\rpython-2.7.15 | 12.7 MB | ######################################################################## | 100%\rcertifi-2019.11.28 | 149 KB | ######################################################################## | 100%\rPreparing transaction: done\rVerifying transaction: done\rExecuting transaction: done\r#\r# To activate this environment, use\r#\r# $ conda activate py27\r#\r# To deactivate an active environment, use\r#\r# $ conda deactivate\rWe can run conda activate py27 to “activate” (i.e., enable) our new environment.\rDo “deactivate” (i.e., exit) the environment we can use conda deactivate.\n\r\rActivate (i.e., “enter”) the new environment\rWe can see our new environment when we list our envs\n(base) $ conda info --envs\r# conda environments:\r#\rbase * /home/dchen/miniconda3\rpy27 /home/dchen/miniconda3/envs/py27\rTo activate the py27 environment we just created we run conda activate py27.\n(base) $ conda activate py27\r(py27) $\rYou can see when we activate the environment, we get a new prompt.\rThe (base) now changed to let us know which environment we’re in, (py27).\nNow when we run Python commands, it’s actually an entirely different Python from\rwhat we were using before.\nHere’s the location of the Python binary\n(py27) $ which python\r/home/dchen/miniconda3/envs/py27/bin/python\rThe version is different from before\r(because we used python=2.7 when we created this py27 environment).\n(py27) $ python --version\rPython 2.7.15\r\rInstalling packages into environment (conda)\rI’ll write a separate post about setting up\rconda-forge for installing packages\r(the commands to run are on the website).\nIn general, I try to conda install as many packages as I can.\rIf the package I’m trying to install does not exist on conda,\rthen I pip install the package.\nThis is a brand new Legacy Python environment, without any packages.\rIn this example we’ll try to install the networkx package.\rYou can see we currently do not have this package in this environment.\n(py27) $ python\rPython 2.7.15 | packaged by conda-forge | (default, Jul 2 2019, 00:39:44)\r[GCC 7.3.0] on linux2\rType \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information.\r\u0026gt;\u0026gt;\u0026gt; import networkx as nx\rTraceback (most recent call last):\rFile \u0026quot;\u0026lt;stdin\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt;\rImportError: No module named networkx\r\u0026gt;\u0026gt;\u0026gt; quit()\rWe can install networkx with conda install networkx.\rIf you need a specific version of a package, this is where you will install it.\n(py27) $ conda install networkx\rCollecting package metadata (current_repodata.json): done\rSolving environment: done\r## Package Plan ##\renvironment location: /home/dchen/miniconda3/envs/py27\radded / updated specs:\r- networkx\rThe following packages will be downloaded:\rpackage | build\r---------------------------|-----------------\rnetworkx-2.1 | py27_0 1.8 MB conda-forge\r------------------------------------------------------------\rTotal: 1.8 MB\rThe following NEW packages will be INSTALLED:\rdecorator conda-forge/noarch::decorator-4.4.1-py_0\rnetworkx conda-forge/linux-64::networkx-2.1-py27_0\rProceed ([y]/n)?\rDownloading and Extracting Packages\rnetworkx-2.1 | 1.8 MB | ######################################################################## | 100%\rPreparing transaction: done\rVerifying transaction: done\rExecuting transaction: done\rNow we have the package in our environment!\n(py27) $ python\rPython 2.7.15 | packaged by conda-forge | (default, Jul 2 2019, 00:39:44)\r[GCC 7.3.0] on linux2\rType \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information.\r\u0026gt;\u0026gt;\u0026gt; import networkx as nx\r\u0026gt;\u0026gt;\u0026gt; G = nx.Graph()\r\u0026gt;\u0026gt;\u0026gt; quit()\rJust to show that the package is in an entirely different environment,\rwe can deativate our current environment to go back into (base),\rand can see we don’t have networkx installed in our base Python 3 environment.\n(py27) $ conda deactivate\r(base) $ python\rPython 3.7.6 | packaged by conda-forge | (default, Jan 7 2020, 22:33:48)\r[GCC 7.3.0] on linux\rType \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information.\r\u0026gt;\u0026gt;\u0026gt; import networkx as nx\rTraceback (most recent call last):\rFile \u0026quot;\u0026lt;stdin\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt;\rModuleNotFoundError: No module named \u0026#39;networkx\u0026#39;\r\u0026gt;\u0026gt;\u0026gt; quit()\r(base) $\r\rReproducing and sharing environments\rNow that you have all the packages you need in an environment and all your code is working,\rnext is to document your work.\rEven if you’re working on your own,\ryou may be working on your laptop and on a remote server,\rand need to replicate your environment in both locations.\rAnother scenario is if you need to share your code with another person and want to make\rsure your code is executed in the same environment on their machine.\nWe can use the conda env export command to generate all the packages in the current environment.\rSo if we’re in the py27 environment we created,\ryou can get a list of all the packages in it.\n(py27) $ conda env export\rname: py27\rchannels:\r- conda-forge\r- defaults\rdependencies:\r- _libgcc_mutex=0.1=conda_forge\r- _openmp_mutex=4.5=0_gnu\r- ca-certificates=2019.11.28=hecc5488_0\r- certifi=2019.11.28=py27_0\r- decorator=4.4.1=py_0\r- libffi=3.2.1=he1b5a44_1006\r- libgcc-ng=9.2.0=h24d8f2e_2\r- libgomp=9.2.0=h24d8f2e_2\r- libstdcxx-ng=9.2.0=hdf63c60_2\r- ncurses=6.1=hf484d3e_1002\r- networkx=2.1=py27_0\r- openssl=1.1.1d=h516909a_0\r- pip=20.0.2=py_2\r- python=2.7.15=h5a48372_1009\r- readline=8.0=hf8c457e_0\r- setuptools=44.0.0=py27_0\r- sqlite=3.30.1=hcee41ef_0\r- tk=8.6.10=hed695b0_0\r- wheel=0.34.2=py_1\r- zlib=1.2.11=h516909a_1006\rprefix: /home/dchen/miniconda3/envs/py27\rWe can export this to an environment.yml file so all the packages are saved and documented.\rYou can do this by copying and pasting the results in to a file called environment.yml manually,\ror you can pipe the output into the file conda env export \u0026gt; environment.yml.\rIf your python code is in a\rproject\rstructure\ryou would run this command to put the environment.yml file in the root project directory.\nNow you can send your Python project to another person or to another computer.\rTo reproduce the environment, you can run conda env create -f environment.yml\rto use the environment.yml file to automatically recreate the environment.\rThe name field in the first line will be used as the environment name.\nIn this example let’s edit the name to py27copy since we already have a py27 environment.\nNow if we run conda env create -f environment.yml,\rit will “reinstall” all the packages needed as specified in the environment.yml file,\rand we can activate/deactivate just like before\n(py27) $ conda env create -f ~/Desktop/environment.yml\rCollecting package metadata (repodata.json): done\rSolving environment: done\rPreparing transaction: done\rVerifying transaction: done\rExecuting transaction: done\r#\r# To activate this environment, use\r#\r# $ conda activate py27dup\r#\r# To deactivate an active environment, use\r#\r# $ conda deactivate\r\r\rInstalling packages into environment (pip)\rNot every package will be conda installable,\rfor those packages we can pip install them just like normal.\rHere we’ll install the flask package for web development into our py27 environment.\n(py27) $ pip install flask\rDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\rCollecting flask\rUsing cached Flask-1.1.1-py2.py3-none-any.whl (94 kB)\rCollecting Werkzeug\u0026gt;=0.15\rDownloading Werkzeug-1.0.0-py2.py3-none-any.whl (298 kB)\r|████████████████████████████████| 298 kB 1.1 MB/s\rCollecting itsdangerous\u0026gt;=0.24\rDownloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\rCollecting click\u0026gt;=5.1\rDownloading Click-7.0-py2.py3-none-any.whl (81 kB)\r|████████████████████████████████| 81 kB 2.0 MB/s\rCollecting Jinja2\u0026gt;=2.10.1\rDownloading Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)\r|████████████████████████████████| 126 kB 306 kB/s\rCollecting MarkupSafe\u0026gt;=0.23\rDownloading MarkupSafe-1.1.1-cp27-cp27mu-manylinux1_x86_64.whl (24 kB)\rInstalling collected packages: Werkzeug, itsdangerous, click, MarkupSafe, Jinja2, flask\rSuccessfully installed Jinja2-2.11.1 MarkupSafe-1.1.1 Werkzeug-1.0.0 click-7.0 flask-1.1.1 itsdangerous-1.1.0\rNow if you want to export the enviornment to the environment.yml\ryou’ll notice that pip will be part of the installation,\rand also a new pip section that describes the packages that will be installed via\rpip instead of conda.\n(py27) $ conda env export\rname: py27\rchannels:\r- conda-forge\r- defaults\rdependencies:\r- _libgcc_mutex=0.1=conda_forge\r- _openmp_mutex=4.5=0_gnu\r- ca-certificates=2019.11.28=hecc5488_0\r- certifi=2019.11.28=py27_0\r- decorator=4.4.1=py_0\r- libffi=3.2.1=he1b5a44_1006\r- libgcc-ng=9.2.0=h24d8f2e_2\r- libgomp=9.2.0=h24d8f2e_2\r- libstdcxx-ng=9.2.0=hdf63c60_2\r- ncurses=6.1=hf484d3e_1002\r- networkx=2.1=py27_0\r- openssl=1.1.1d=h516909a_0\r- pip=20.0.2=py_2\r- python=2.7.15=h5a48372_1009\r- readline=8.0=hf8c457e_0\r- setuptools=44.0.0=py27_0\r- sqlite=3.30.1=hcee41ef_0\r- tk=8.6.10=hed695b0_0\r- wheel=0.34.2=py_1\r- zlib=1.2.11=h516909a_1006\r- pip:\r- click==7.0\r- flask==1.1.1\r- itsdangerous==1.1.0\r- jinja2==2.11.1\r- markupsafe==1.1.1\r- werkzeug==1.0.0\rprefix: /home/dchen/miniconda3/envs/py27\r\rSummary\rCreating environments with the\rconda create -n \u0026lt;name\u0026gt; python=\u0026lt;version\u0026gt;\ris an easy way to create new environments with a specified version of Python.\rYou can then activate the environment with\rconda activate \u0026lt;name\u0026gt;\rand proceed to install packages with\rconda install \u0026lt;package\u0026gt; or pip install \u0026lt;package.\rWhen you’re ready to document and share your environment with another person\ror on another machine,\ryou can run\rconda env export \u0026gt; environment.yml\rto save the current environment\rto a environment.yml.\rThis file can be used to create a new environment with\rconda env create -f environment.yml\rand now you can replicate your environment across multiple machines!\nYou don’t need to use conda to create your Python environments.\rIf you’re not using conda for your environments,\ryou can create a requirements.txt file\rinstead of an environment.yml with\rpip freeze \u0026gt; requirements.txt\rand install everything into the current environment with\rpip install -r requirements.txt.\rJust know you need to create the environment first as pip will\rnot create the environment for you.\nHope this helps people get started with environments and getting their code working\ron multiple computers.\rYou can checkout the main documentation page on conda environment management here:\nhttps://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\n\r","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582960376,"objectID":"bfada1d8e512d896654ee29abf02503c","permalink":"/2020/02/29/python-environments-with-conda/","publishdate":"2020-02-29T00:00:00Z","relpermalink":"/2020/02/29/python-environments-with-conda/","section":"post","summary":"Anaconda\r(and these days\rminiconda)\rhas been my go-to for getting Python and the scientific/data science software stack\rinstalled on my computer\r(even on my Arch linux machine!).\nWhen it first came out it was the first time I was able to install and use\rpandas and the rest of the scipy stack.\rI’ve stuck with it ever since.\nIt was also the first time I used (and understood) virtual environments.","tags":["python","tutorial"],"title":"Python Environments with Conda","type":"post"},{"authors":[],"categories":[],"content":"Yes. That’s the #rstats hex bowtie.\rYes, I lost it when Hadley was wearing it for the conference opening notes.\n\rThanks to Daniel Chen for the awesome #rstats bow tie! https://t.co/OPnawWrltG pic.twitter.com/XZUj5z7Ox4\r— Hadley Wickham (@hadleywickham) January 29, 2020\r\r\r\rBow Ties are Cool\rThe R hex fabric is by Amelia McNamara,\rwhich you can get on the\rGitHub repo\ror buy directly on her\rSpoonflower page\n\rNow that the cat’s out of the bag 😻… Yes, I made myself an #rstats/#tidyverse dress! 📸 @alexandrabyrne with the angles pic.twitter.com/6m1nJotIwN\r— AmeliaMN (@AmeliaMN) August 16, 2019\r\r\r\rFor my particular use case,\rI needed smaller hexes because bow ties are just smaller than dresses.\rHere’s my modified\r1-inch r hex fabirc\nI knew I wanted to make bowties since\rI gave a lightning talk during\rRStudio\rinternship.\rAnd Hadley is a bow tie aficionado.\rAfter talking to a few people,\rI was directed to get them made on\rEtsy,\rand so the hunt to find someone began in November.\nThe tricky part about finding custom bow ties on Etsy is that many of the\rsellers don’t make adjustable bow ties.\rI eventually found\rMegan’s\rShop,\rwhich allowed me to get adjustable bowties and use custom fabric.\rI placed an order for 20 with the only constraint of one of the bow ties\rmust have ggplot2 and tidyverse visiable when it is tied (for Hadley).\rThis is when Megan told me about the original hex size needed to be smaller\rfor the bow tie.\rI also got a few pocket squares in my order too.\nI went though the process of creating the smaller hex pattern on\rSpoonflower\rand shipeed 6 yards of the pattern for my order in the “Petal Signature Cotton”.\rI had to ship Megan the fabric directly since you need to order a test sample\rbefore other people can purchace the pattern.\rIt was a gamble,\rbut it was the holiday season,\rand the conference was coming up fast.\nMegan was great, I got a series of photos about the entire process.\n\rThe fabric is thicker than what I have used previously as spoonflower recently changed to the ‘petal cotton’. I’m just retooling the pattern and interfacing a bit to accommodate it. :)\n\r\rAlso- here is how the one tie with the prominent ggplotand tidyverse was cut out.\n\rI got the order 2 weeks before the conference and it all turned out wonderfully.\nHere’s the link to her\rshop\rand\rtwitter.\rI even convinced her to write a\rblog post of her own\r.\n\rPyBadge Name Badge\rI went to PyCon for the first time in 2019.\rAnd the\r#pythonhardware\rcommunity is amazing.\rThe conference was filled with people with their own name tags.\rI though I’d introduce this to the R community.\rAnd also, gives a way for people to say hello to me,\rinstead of the other way around :)\nI got the bigger\rPyBadge\r(they also make a\rsmaller one\r).\rI really like how Adafruit put python on their circuit boards using\rCircuitPython.\rIt makes programming way more accessiable than using an Arduino,\rsince you can do all the programming in Python!\nIn my case I dragged my picture into a folder and it just goes through them in a slide show.\n\rThe Rest\rI\rinterned\rfor Rstudio over the summer,\rwhere I was on the\rEducation Team.\rDuring rstudio::conf, I TA’ed for the\rIntroduction to Machine Learning with the Tidyverse\rworkshop.\nI started teaching as an instructor for\rThe Carpentries.\rWhich eventually led me to writing my book,\rPandas for Everyone!\nHope you enjoyed reading about my conference badge!\rI look forward to seeing people in the R community add flair to the badges in the future!\n\r","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603806237,"objectID":"5ec3483ecec671249a6c7df53311d8f2","permalink":"/2020/01/30/r-hex-bowtie/","publishdate":"2020-01-30T00:00:00Z","relpermalink":"/2020/01/30/r-hex-bowtie/","section":"post","summary":"\r\rIf you were at\rrstudio::conf\ryou may have seen me walking around with my conference badge\rand wondered what everything is.\n","tags":["R"],"title":"R Hex Bowtie","type":"post"},{"authors":[],"categories":[],"content":"\r\rI’ve had a lot of time to think about my time as an RStudio intern.\rWhen I do,\rI usually end up with a few words in my head before I’m flooded with\r(good) emotions and struggle with finding the words to convey my thoughts.\rThe last time I tried to write something like this went a little like\rthis.\rWhat I can say is this:\rwhatever you thought it was like working at RStudio,\ris (probably) true.\nFrom a technical perspective,\rthe most jarring part of the work was transitioning from a user of the R language\rto a developer.\rI’ve taught workshops and written R packages,\rbut nothing came close to the type of code I had to write for my internship.\rI think my only saving grace was I was familiar enough with Git,\rso I didn’t have to deal with\rworkflow\rissues.\nThe project\rI was on the\reducation team\rand worked with worked with\rGarrett Grolemund\rand\rBarret Schloerke\r(I can never spell their names properly without looking it up, and it didn’t help one of the other interns was\rMalcolm Barrett\r)\ron gradethis.\rThe package tries to grade code in a\rlearnr document,\rbut the real magic of the package isn’t just reporting a correct or incorrect answer,\rit’s the ability to provide meaningful feedback to the learner.\rFor example, if the solution to a question was sqrt(log(1)) and the student provided sqrt(log(2)),\rit would report the answer as “incorrect” but also return something along the lines of\r“I expected 1 where you wrote 2. Try it again; next time’s the charm!”.\rIsn’t that way nicer than a giant red box telling you the answer is wrong?\rNow if you think about it, isn’t this a really difficult problem to solve???\r(Hint: yes.)\nLuckily, I didn’t have to write this package from scratch.\rGarrett did write the initial working code base for me to improve,\rbut it also essentially meant working with legacy code and\rlearning new R programming paradigms at the same time.\r“R programming paradigms” what’s that? you might ask.\rI mean\rexpressions,\rrlang, and\rtidyeval\r… :x\nHow does one go about leaning how all this works in the package?\rWell…\rby merging a PR that breaks everything,\rthen trying to understand an issue\rthat took a month to fix.\nI spent a better part of that month reading and not understanding expressions,\rand pretty much being guided by a series of pair-programming sessions with Barret.\rBy “guided” I mean he pretty much did the work I just typed characters and acted as a scribe.\nI’ve done pair-programming with my own students,\rbut I never had anyone pair-program with me where I was the one learning.\rThere was so many new things going on during each session that I couldn’t retain every detail,\rbut things got easier towards the end.\rIt made me think about all the times I’d help my students and really got to empathize with their confusion.\nThe single most important package development skill that made my work possible were unit tests.\rSince I was working with an already existing code base,\rall the 100s of unit tests that were there needed to pass as I made changes.\rI can’t emphasize how much time and sanity was saved from running the unit test suite.\rSeemingly small changes would cause some tests to fail where I’d have to either fix my code or fix the test.\rAs the package started to stabilize and I started testing it out on a mock learnr tutorial,\rI even discovered a bug in the == operator!\nEven though everyone was in a remote position, there was so much ancillary learning going on.\rI\rtried\rto keep a\rlist\ras I was going though the\rinternship,\rbut it got impractical.\n\rHanging out\rEvery one of my fellow interns\rare amazing.\rSince RStudio is a remote company,\rwe tried to hold a weekly “virtual coffee” meeting so we can all see and talk to one another.\rIt really made the distance and time zone differences between all of us seem much smaller.\rAs I’m typing this blog post at rstudio::conf,\rit felt like we already knew each other,\reven though most of us haven’t seen the other in person before.\nDue to my conference and travel schedule over the summer,\rI got to meet a handful of the interns in person beforehand.\nTherese and Malcolm in Los Angeles, CA\rI got to meet Therese and Malcolm when I was in LA staying at a friend’s place for another friends wedding.\n\rJulia in San Francisco, CA\rI got the tour of Julia’s lab when I was in San Francisco for the\rPyBay conference.\nMore of Julia’s lab at the bottom of the post.\n\rJoyce in Raleigh, NC.\rMy dad’s job relocated him to Carry, NC so I was visiting him when a Data Science Happy Hour was announced in Raleigh and got to meet Joyce.\n\r📣 Raleigh data science folks (and ambitious Durham folks): 🗓 Data science happy hour at @lynnwoodbrewing (on Whitaker Mill) on Tuesday 9/24 at 5:30 PM with the inimitable @CMastication. (You can also buy food there or bring your own.)\n\u0026mdash; Tim Hopper (@tdhopper) September 10, 2019  \r\r\rEducation and Shiny teams\rI met with the Shiny team for daily stand-ups, and the education team about every week.\rThe sheer amount of people I could ask for help is absolutely insane.\n\rThis summer I got the opportunity work with the @rstudio education and shiny teams. Definitely leveled up my #rstats skills 🍄, and looking forward to sharing it with the rest of the community. So sad this is all over😭. pic.twitter.com/qc7r3j6wg0\n\u0026mdash; Dⓐniel Chen 🐍🏴‍☠️ (@chendaniely) August 25, 2019  \r\rEveryone else\rA lot of effort goes into making a remote company feel less isolating.\rMany of us signed up for the virtual donut bot on Slack\rthat randomly pairs you up with someone else in the company to have a conference call to get to know each other.\nWe also had an opportunity to signup/attend lightning talk sessions where people from the company talked about\rcurrent projects they’re working on for work/home.\rSome people even talked about their hobbies!\nEven crazier, was we sat in on meetings with the President and CEO of the company on multiple occasions.\rFor a company that has such a wide global reach with employees geographically scattered around the world,\rthe community that RStudio has internally is genuinely mirrored in the amazing community they have created\rexternally for the rest of us.\n\rMore than technical training\rMy time with the education group at rstudio really help me find my way.\rI’m pretty much on my third iteration of a PhD dissertation topic,\rand working with the education group helped me realize that I can apply all the lessons I’ve learned\rthrough\rThe Carpentries,\rteaching on my own,\rand working on educational tools\rtowards my dissertation topic.\rFor the first time in a long while I finally have a sense of direction\rand feel in control of my own degree.\nNot only that, but being in intern was so validating.\rI’ve felt like I was spinning my wheels since starting graduate school,\rand it’s nice to know I’ve been doing something right these past few years.\rI’m not going to lie,\rthere is a certain amount of “street cred” involved with being an intern,\rbut let me assure you,\rI still barely grasp a lot of the internals of R.\rIf anything, I feel even more impostor syndrome,\rnow that I’m back out in the wild.\nFor now,\rthe best thing I can do is pay it forward.\rAnd for those who don’t know,\rmy new dissertation topic is on the\reducation and pedagogy of data science in medicine.\nI hope to hear form everyone in the future!\n\rAppendix\rThe other intern final blog posts (list is ongoing):\nMalcolm Barrett\n\rJoyce\rCahoon\n\rDewey Dunnington\n\rMarly Gotti\n\rMaya Gans\n\rYim Register\n\rDesirée De Leon\n\r\rPicture’s around Julia’s lab.\n\r","date":1580256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603806237,"objectID":"0c818e0b44c5848dd2ebb3ca8989f09e","permalink":"/2020/01/29/my-time-as-an-rstudio-intern/","publishdate":"2020-01-29T00:00:00Z","relpermalink":"/2020/01/29/my-time-as-an-rstudio-intern/","section":"post","summary":"I’ve had a lot of time to think about my time as an RStudio intern.\rWhen I do,\rI usually end up with a few words in my head before I’m flooded with\r(good) emotions and struggle with finding the words to convey my thoughts.\rThe last time I tried to write something like this went a little like\rthis.\rWhat I can say is this:\rwhatever you thought it was like working at RStudio,\ris (probably) true.","tags":["rstudio-internship","r"],"title":"My time as an RStudio Intern","type":"post"},{"authors":[],"categories":[],"content":"\r\r\rI’m at the R/Medicine conference (no it’s not a Reddit thing) and\rgot to help Alison Hill with her R Markdown for Medicine workshop.\rOne of the questions I got to tinker with was making tables used to report model results.\nOne technique I learned while doing my MPH was to add variables to your model in blocks.\rIt reduces the number of tests you need to perform, and is more meaningful than saying “I ran stepwise”.\rSo, as a researcher, you will add variables in meaningful blocks and then see if that block of variables\ris “significant” by looking at the F-statistic.\nHere is one example of at least putting your model results into a table.\nCreating the models\rlibrary(reshape2) # using this just for the tips dataset\rlibrary(broom)\rlibrary(purrr)\rlibrary(knitr)\rlibrary(kableExtra)\rhead(tips)\r## total_bill tip sex smoker day time size\r## 1 16.99 1.01 Female No Sun Dinner 2\r## 2 10.34 1.66 Male No Sun Dinner 3\r## 3 21.01 3.50 Male No Sun Dinner 3\r## 4 23.68 3.31 Male No Sun Dinner 2\r## 5 24.59 3.61 Female No Sun Dinner 4\r## 6 25.29 4.71 Male No Sun Dinner 4\r# create a bumch of models\rm1 \u0026lt;- lm(tip ~ total_bill, data = tips)\rm2 \u0026lt;- lm(tip ~ total_bill + sex, data = tips)\rm3 \u0026lt;- lm(tip ~ total_bill + sex + smoker, data = tips)\rm4 \u0026lt;- lm(tip ~ total_bill + smoker + size, data = tips)\rNow that we have the models, we can use the broom package to get a nice consistent dataframe of the coefficients\n# get a dataframe of the model estimates\rmodels \u0026lt;- list(m1, m2, m3, m4)\rbroom_outputs \u0026lt;- purrr::map(models, broom::tidy)\rbroom_outputs\r## [[1]]\r## # A tibble: 2 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.920 0.160 5.76 2.53e- 8\r## 2 total_bill 0.105 0.00736 14.3 6.69e-34\r## ## [[2]]\r## # A tibble: 3 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.933 0.174 5.37 1.84e- 7\r## 2 total_bill 0.105 0.00746 14.1 2.33e-33\r## 3 sexMale -0.0266 0.138 -0.192 8.48e- 1\r## ## [[3]]\r## # A tibble: 4 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.977 0.178 5.48 1.05e- 7\r## 2 total_bill 0.106 0.00748 14.2 1.73e-33\r## 3 sexMale -0.0281 0.138 -0.203 8.39e- 1\r## 4 smokerYes -0.149 0.135 -1.10 2.72e- 1\r## ## [[4]]\r## # A tibble: 4 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.709 0.205 3.46 6.38e- 4\r## 2 total_bill 0.0939 0.00933 10.1 4.26e-20\r## 3 smokerYes -0.0834 0.138 -0.605 5.46e- 1\r## 4 size 0.180 0.0878 2.05 4.11e- 2\rWhat we have now is a list of dataframes, one for each model we fit.\n\rJust the beta estimates\rIf we just want the coefficients, we only need the terms and estimates columns and do a join.\rWe can use the purrr::reduce function to do this\n# join them all together\rmodel_outputs \u0026lt;- purrr::reduce(broom_outputs, dplyr::full_join, by = \u0026#39;term\u0026#39;)\rmodel_outputs\r## # A tibble: 5 x 17\r## term estimate.x std.error.x statistic.x p.value.x estimate.y std.error.y\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Int~ 0.920 0.160 5.76 2.53e- 8 0.933 0.174 ## 2 tota~ 0.105 0.00736 14.3 6.69e-34 0.105 0.00746\r## 3 sexM~ NA NA NA NA -0.0266 0.138 ## 4 smok~ NA NA NA NA NA NA ## 5 size NA NA NA NA NA NA ## # ... with 10 more variables: statistic.y \u0026lt;dbl\u0026gt;, p.value.y \u0026lt;dbl\u0026gt;,\r## # estimate.x.x \u0026lt;dbl\u0026gt;, std.error.x.x \u0026lt;dbl\u0026gt;, statistic.x.x \u0026lt;dbl\u0026gt;,\r## # p.value.x.x \u0026lt;dbl\u0026gt;, estimate.y.y \u0026lt;dbl\u0026gt;, std.error.y.y \u0026lt;dbl\u0026gt;,\r## # statistic.y.y \u0026lt;dbl\u0026gt;, p.value.y.y \u0026lt;dbl\u0026gt;\rThis will join all the dataframes together using the term column as the key.\rVariables that do not exist in the model will be filled with an NA.\rProblem now, is that we have a bunch of columns we don’t need.\rSo the next step is to pull out the columns we want, and rename them accordingly.\nmodel_table \u0026lt;- model_outputs %\u0026gt;%\rdplyr::select(term, tidyselect::starts_with(\u0026#39;estimate\u0026#39;))\rnames(model_table) \u0026lt;- c(\u0026#39;Variable\u0026#39;, \u0026#39;Model 1\u0026#39;, \u0026quot;Model 2\u0026quot;, \u0026quot;Model 3\u0026quot;, \u0026quot;Model 4\u0026quot;)\rmodel_table\r## # A tibble: 5 x 5\r## Variable `Model 1` `Model 2` `Model 3` `Model 4`\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.920 0.933 0.977 0.709 ## 2 total_bill 0.105 0.105 0.106 0.0939\r## 3 sexMale NA -0.0266 -0.0281 NA ## 4 smokerYes NA NA -0.149 -0.0834\r## 5 size NA NA NA 0.180\r\rWhat if I need more variables?\rJust reporting the beta coefficients is not how you want to report your findings,\rtypically you’d want to also display some measure of uncertainty along with the results.\rHere I’m just going to add the std.error column, but you can calculate the 95% confidence intervals using the std.error.\nHere’s the reference from the kableExtra library I’m using to build the coefficient table:\n# this is the exmaple form the kable docs I used for reference\r# https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\rcollapse_rows_dt \u0026lt;- data.frame(C1 = c(rep(\u0026quot;a\u0026quot;, 3), rep(\u0026quot;b\u0026quot;, 2)),\rC2 = c(rep(\u0026quot;c\u0026quot;, 2), rep(\u0026quot;d\u0026quot;, 1), rep(\u0026quot;c\u0026quot;, 1), rep(\u0026quot;d\u0026quot;, 1)),\rC3 = 1:5,\rC4 = sample(c(0,1), 5, replace = TRUE))\rcollapse_rows_dt\r## C1 C2 C3 C4\r## 1 a c 1 0\r## 2 a c 2 1\r## 3 a d 3 1\r## 4 b c 4 0\r## 5 b d 5 0\rkable(collapse_rows_dt, align = \u0026quot;c\u0026quot;) %\u0026gt;%\rkable_styling(full_width = F) %\u0026gt;%\rcolumn_spec(1, bold = T) %\u0026gt;%\rcollapse_rows(columns = 1:2, valign = \u0026quot;top\u0026quot;)\r\r\rC1\r\rC2\r\rC3\r\rC4\r\r\r\r\r\ra\r\rc\r\r1\r\r0\r\r\r\r2\r\r1\r\r\r\rd\r\r3\r\r1\r\r\r\rb\r\rc\r\r4\r\r0\r\r\r\rd\r\r5\r\r0\r\r\r\r\rThis example was important to my solution, becuase I would need to format my coefficient table the same way as collapse_rows_dt.\rThis means that:\n\rC1 would be the variables in my model\rC2 would need to alternate between the estimate and the std.error\rThe other columns would be the actual values for each model.\r\rSince the model outputs the estimate and std.error as columns,\rI know some kind of reshaping/tidying step will need to be involved (melt/gather / pivot_longer).\nWe still start from the same data steps as the previous example\nbroom_outputs\r## [[1]]\r## # A tibble: 2 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.920 0.160 5.76 2.53e- 8\r## 2 total_bill 0.105 0.00736 14.3 6.69e-34\r## ## [[2]]\r## # A tibble: 3 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.933 0.174 5.37 1.84e- 7\r## 2 total_bill 0.105 0.00746 14.1 2.33e-33\r## 3 sexMale -0.0266 0.138 -0.192 8.48e- 1\r## ## [[3]]\r## # A tibble: 4 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.977 0.178 5.48 1.05e- 7\r## 2 total_bill 0.106 0.00748 14.2 1.73e-33\r## 3 sexMale -0.0281 0.138 -0.203 8.39e- 1\r## 4 smokerYes -0.149 0.135 -1.10 2.72e- 1\r## ## [[4]]\r## # A tibble: 4 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 0.709 0.205 3.46 6.38e- 4\r## 2 total_bill 0.0939 0.00933 10.1 4.26e-20\r## 3 smokerYes -0.0834 0.138 -0.605 5.46e- 1\r## 4 size 0.180 0.0878 2.05 4.11e- 2\rThe trick here is to get the data in the correct format that can be used by kableExtra\nbroom_stats_long \u0026lt;- purrr::map(broom_outputs, tidyr::gather, \u0026#39;key\u0026#39;, \u0026#39;value\u0026#39;, estimate, std.error)\rbroom_stats_long\r## [[1]]\r## # A tibble: 4 x 5\r## term statistic p.value key value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 5.76 2.53e- 8 estimate 0.920 ## 2 total_bill 14.3 6.69e-34 estimate 0.105 ## 3 (Intercept) 5.76 2.53e- 8 std.error 0.160 ## 4 total_bill 14.3 6.69e-34 std.error 0.00736\r## ## [[2]]\r## # A tibble: 6 x 5\r## term statistic p.value key value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 5.37 1.84e- 7 estimate 0.933 ## 2 total_bill 14.1 2.33e-33 estimate 0.105 ## 3 sexMale -0.192 8.48e- 1 estimate -0.0266 ## 4 (Intercept) 5.37 1.84e- 7 std.error 0.174 ## 5 total_bill 14.1 2.33e-33 std.error 0.00746\r## 6 sexMale -0.192 8.48e- 1 std.error 0.138 ## ## [[3]]\r## # A tibble: 8 x 5\r## term statistic p.value key value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 5.48 1.05e- 7 estimate 0.977 ## 2 total_bill 14.2 1.73e-33 estimate 0.106 ## 3 sexMale -0.203 8.39e- 1 estimate -0.0281 ## 4 smokerYes -1.10 2.72e- 1 estimate -0.149 ## 5 (Intercept) 5.48 1.05e- 7 std.error 0.178 ## 6 total_bill 14.2 1.73e-33 std.error 0.00748\r## 7 sexMale -0.203 8.39e- 1 std.error 0.138 ## 8 smokerYes -1.10 2.72e- 1 std.error 0.135 ## ## [[4]]\r## # A tibble: 8 x 5\r## term statistic p.value key value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 3.46 6.38e- 4 estimate 0.709 ## 2 total_bill 10.1 4.26e-20 estimate 0.0939 ## 3 smokerYes -0.605 5.46e- 1 estimate -0.0834 ## 4 size 2.05 4.11e- 2 estimate 0.180 ## 5 (Intercept) 3.46 6.38e- 4 std.error 0.205 ## 6 total_bill 10.1 4.26e-20 std.error 0.00933\r## 7 smokerYes -0.605 5.46e- 1 std.error 0.138 ## 8 size 2.05 4.11e- 2 std.error 0.0878\rNow that we have the data formatted the way we need, we now select only the columns we need.\n# filter only the columns we need\rbroom_stats_long_filter \u0026lt;- purrr::map(broom_stats_long, dplyr::select, term, key, value)\rbroom_stats_long_filter\r## [[1]]\r## # A tibble: 4 x 3\r## term key value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) estimate 0.920 ## 2 total_bill estimate 0.105 ## 3 (Intercept) std.error 0.160 ## 4 total_bill std.error 0.00736\r## ## [[2]]\r## # A tibble: 6 x 3\r## term key value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) estimate 0.933 ## 2 total_bill estimate 0.105 ## 3 sexMale estimate -0.0266 ## 4 (Intercept) std.error 0.174 ## 5 total_bill std.error 0.00746\r## 6 sexMale std.error 0.138 ## ## [[3]]\r## # A tibble: 8 x 3\r## term key value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) estimate 0.977 ## 2 total_bill estimate 0.106 ## 3 sexMale estimate -0.0281 ## 4 smokerYes estimate -0.149 ## 5 (Intercept) std.error 0.178 ## 6 total_bill std.error 0.00748\r## 7 sexMale std.error 0.138 ## 8 smokerYes std.error 0.135 ## ## [[4]]\r## # A tibble: 8 x 3\r## term key value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) estimate 0.709 ## 2 total_bill estimate 0.0939 ## 3 smokerYes estimate -0.0834 ## 4 size estimate 0.180 ## 5 (Intercept) std.error 0.205 ## 6 total_bill std.error 0.00933\r## 7 smokerYes std.error 0.138 ## 8 size std.error 0.0878\rNext, you need to create some kind of ID value for each model you’re creating,\rthat way when you’re combining everything into a single dataframe,\rthe estimates are traveling together with the standard error.\rHere I’m labeling each model as a number starting with 1.\ntidy_tables \u0026lt;- purrr::map2_df(broom_stats_long_filter, 1:length(broom_stats_long_filter),\rfunction(x, y) dplyr::mutate(x, mod = y))\rtidy_tables\r## # A tibble: 26 x 4\r## term key value mod\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 (Intercept) estimate 0.920 1\r## 2 total_bill estimate 0.105 1\r## 3 (Intercept) std.error 0.160 1\r## 4 total_bill std.error 0.00736 1\r## 5 (Intercept) estimate 0.933 2\r## 6 total_bill estimate 0.105 2\r## 7 sexMale estimate -0.0266 2\r## 8 (Intercept) std.error 0.174 2\r## 9 total_bill std.error 0.00746 2\r## 10 sexMale std.error 0.138 2\r## # ... with 16 more rows\rAlmost there!\nLastly, we need to sort the columns by using arrange.\rHere is where the ID column, mod get’s used to make sure model values are traveling together.\ndf \u0026lt;- tidy_tables %\u0026gt;%\rdplyr::arrange(term, mod, key, value) %\u0026gt;%\rtidyr::spread(mod, value, drop = FALSE)\rdf\r## # A tibble: 10 x 6\r## term key `1` `2` `3` `4`\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) estimate 0.920 0.933 0.977 0.709 ## 2 (Intercept) std.error 0.160 0.174 0.178 0.205 ## 3 sexMale estimate NA -0.0266 -0.0281 NA ## 4 sexMale std.error NA 0.138 0.138 NA ## 5 size estimate NA NA NA 0.180 ## 6 size std.error NA NA NA 0.0878 ## 7 smokerYes estimate NA NA -0.149 -0.0834 ## 8 smokerYes std.error NA NA 0.135 0.138 ## 9 total_bill estimate 0.105 0.105 0.106 0.0939 ## 10 total_bill std.error 0.00736 0.00746 0.00748 0.00933\rkable(df, align=\u0026#39;c\u0026#39;) %\u0026gt;%\rkable_styling(full_width = F) %\u0026gt;%\rcolumn_spec(1, bold = T) %\u0026gt;%\rcollapse_rows(columns = 1, valign = \u0026quot;top\u0026quot;)\r\r\rterm\r\rkey\r\r1\r\r2\r\r3\r\r4\r\r\r\r\r\r(Intercept)\r\restimate\r\r0.9202696\r\r0.9332785\r\r0.9770353\r\r0.7090155\r\r\r\rstd.error\r\r0.1597347\r\r0.1737557\r\r0.1781633\r\r0.2048813\r\r\r\rsexMale\r\restimate\r\rNA\r\r-0.0266087\r\r-0.0280926\r\rNA\r\r\r\rstd.error\r\rNA\r\r0.1383340\r\r0.1382793\r\rNA\r\r\r\rsize\r\restimate\r\rNA\r\rNA\r\rNA\r\r0.1803316\r\r\r\rstd.error\r\rNA\r\rNA\r\rNA\r\r0.0878033\r\r\r\rsmokerYes\r\restimate\r\rNA\r\rNA\r\r-0.1491923\r\r-0.0834326\r\r\r\rstd.error\r\rNA\r\rNA\r\r0.1354353\r\r0.1380005\r\r\r\rtotal_bill\r\restimate\r\r0.1050245\r\r0.1052324\r\r0.1059431\r\r0.0938884\r\r\r\rstd.error\r\r0.0073648\r\r0.0074582\r\r0.0074827\r\r0.0093314\r\r\r\r\r\rkableExtra\rThere kableExtra is great for tables like this. Here are links to more information\n\rhttps://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\rhttp://haozhu233.github.io/kableExtra/\r\r\r","date":1568332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603806237,"objectID":"8240f84054134ca41b848b1ab816d375","permalink":"/2019/09/13/table-of-model-results-using-kable-and-kableextra/","publishdate":"2019-09-13T00:00:00Z","relpermalink":"/2019/09/13/table-of-model-results-using-kable-and-kableextra/","section":"post","summary":"I’m at the R/Medicine conference (no it’s not a Reddit thing) and\rgot to help Alison Hill with her R Markdown for Medicine workshop.\rOne of the questions I got to tinker with was making tables used to report model results.\nOne technique I learned while doing my MPH was to add variables to your model in blocks.\rIt reduces the number of tests you need to perform, and is more meaningful than saying “I ran stepwise”.","tags":["R","rmarkdown"],"title":"Table of Model Results using kable and kableExtra","type":"post"},{"authors":[],"categories":[],"content":"\r\rI’ve been asked a few times lately about whether one should learn R or Python.\nChanneling\rDavid Robinson’s post,\rI’m writing a blog post about it.\nWhen you’ve written the same code 3 times, write a function\nWhen you’ve given the same in-person advice 3 times, write a blog post\r— David Robinson (@drob) November 9, 2017\r\r\rThe only definitive answer I have is if you’re planning to do web deveopment,\rand you’re somehow only picking between R and Python, pick Python\r(any of the Python web frameworks, Flask, Django, Pyramid, and then ask yourself why not JavaScript?).\rIf you’re doing hardward things, also pick Python\r(see #pythonhardware).\rOtherwise, pick what you already know or took a class in,\ror look at a few job postings in your area and pick the one that is most prevelent.\rIn the end all the data science and data cleaning skills you learn in one language will\rcarry over into the other.\nI personally perfer R for dashboarding\r(shiny)\rand publication\r(RMarkdown + knitr),\rbut that’s mainly because back in 2014, when I was first learning data science skills I was taught in R,\rand there were not good equlivilants in the Python world\r(other than what is now Jupyter).\nAlso, most data scientists in industry are working in SQL anyway ( :\nFor data science/data cleaning/data analysis programs,\rthe best way is to practice working with data.\rHaving said that, it’s really hard to find projects,\ror sometimes your own projects are “too big and complicated” so you might be at a loss of where to begin.\nMy first recommendation is take a look at\rKaggle.\rYou’ll see a lot of datasets there and you might find something interesting to play around with.\rThe forums on the competitions are also good place to get ideas on some of the machine learning and model fitting side.\rOne of my first exposures to data science was one of those competitions,\rand I learned a lot of web scraping and string parsing skills there.\nAnother great resource where you can practice some of your data skills come from the R community.\rIt’s called\rTidy\rTuesday.\rEvery Tuesday a new dataset gets released, and people\r(all over twitter)\rshare their findings.\rIt does not need to be a fully worked out machine learning pipeline.\rYou are just getting a new dataset to explore.\rDavid Robinson does a\r1-hour livestream every week.\rEven though it’s in R, you can do it all in Python as well (maybe try replicating someone’s R work?).\rAfter you get used to exploring data, and when you get more practice in either R or Python,\rit’s much easier to see how you can apply it in your daily life.\rAnd that motivation will help you practice and learn more.\nRachael Tatman,\rfrom Kaggle, also does weekly livestreams where\rshe also hosts a journal club to discuss an academic paper about\ra machine learning method.\nIf you need some more of the basic programming knowledge,\rtake a look at the\rsoftware-carpentry and\rdata-carpentry\rR and Python lessons.\rJake VanderPlas’ book\r“Python Data Science Handbook”\ris free online as well as\ras Garrett Grolemund and Hadley Wickham’s\rR for Data Science.\rHadley Wickham also as a free book on\rAdvanced R,\rbut that’s not necessary when getting started.\rI also try to keep a list of free (python)\rresources\ronline as well.\nSince I work on the data side of most things,\rit’s easy for me to suggest learning things from that point of view.\rAll I can say is, that it if you stick with one of the learning paths,\ryou will actually pick up bits of knowledge that will help you in others.\rI first learned Python as a normal scriping language, and learned how to do data analysis in R.\rI only started to do data manipulation in Python when I understoon the concept of\rtidy data.\nTidy data is probably the most important topic when working with data.\rSo many of the skills you need to clean and tidy your data involve other aspects of the langage\r(e.g., writing vectorized functions),\rthat you’ll learn basic programming concepts by making your data tidy.\nFor example, while learning pandas and data manipulations,\rwill get you working with Python classes,\rwhich is an Object Oriented Programming concept.\rThat will all translate if you need to so more software work in Python or if you want to learn Django.\rIn R, you learn how to work with dataframes, functions, and how to write your own.\nIn the end, find somewhere to start.\rBecause of my background, I say start with loading data and playing with it.\rThe more you do it, the more questions you will have on your data,\rand the more skills you’ll acquire to answer those questions.\rThose skills will carry over to other aspects of the language, and even to other languages\r(e.g., R, Python, even Julia!).\nYou’ll always be learning, it never ends, so don’t worry about ever trying to “know it all”.\rI’m constantly finding new things and ways to do things in R and Pandas; I am always googling and stackoverflow-ing…\rEven though I have a book about Pandas and worked as an intern for RStudio! :)\n","date":1566950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582960376,"objectID":"85b1acda1624da6c2928142b97f197a5","permalink":"/2019/08/28/r-or-python-which-one-to-learn-first/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/2019/08/28/r-or-python-which-one-to-learn-first/","section":"post","summary":"I’ve been asked a few times lately about whether one should learn R or Python.\nChanneling\rDavid Robinson’s post,\rI’m writing a blog post about it.\nWhen you’ve written the same code 3 times, write a function\nWhen you’ve given the same in-person advice 3 times, write a blog post\r— David Robinson (@drob) November 9, 2017\r\r\rThe only definitive answer I have is if you’re planning to do web deveopment,\rand you’re somehow only picking between R and Python, pick Python\r(any of the Python web frameworks, Flask, Django, Pyramid, and then ask yourself why not JavaScript?","tags":["R","python"],"title":"R or Python, which One to Learn (First)?","type":"post"},{"authors":[],"categories":[],"content":"\r\rI have a love-hate relationship with Git.\rIt took me years of following cookbooks and following strict set of commands to\rget a sense of how it works.\rI still have to spend time wrangling Git at times,\rbut I’ve gotten to the point where I’m just annoyed, instead of scared.\rHere is the branching-squash-and-merge workflow I’ve been using at RStudio.\nAlso, sorry I may be glossing over some of the Git basics in this post…\nAt RStudio, I’m using a brancing workflow where I am added directly as a collaborator to a project.\rI still have to create pull requests for review,\rbut contrast this with me forking the repository and creating a pull regrest from my copy (origin)\rto the parent repository (upstream).\nEvery workflow has benefits and drawbacks.\rThe current workflow I’m using is a “squash and merge”.\rWhere all the commits in a pull request are “squashed” into a single commit when it it merged into master.\rThis means the main commit history is super clean,\rand potentially sets up the repository to create automatic change logs or NEWS reports.\rThe downside is if you start branching off other branches (instead of the main master),\ryou’ll have to rebase or cherry-pick to update the branches after a branch gets merged.\nThe general problem\rm1-m2-m3 # the master branch\r\\\rx11-x12-x13 # example branch_1\r\\\ry21-y22-y23 # example branch_2 that depends on branch_1\rAfter a squash and merge the Git DAG looks like this\nm1-m2-m3-s1 # master branch + squash-merge\r\\\rx11-x12-x13 # commits from branch_1 still exist\r\\\ry21-y22-y23 # commits from branch_2 still exist\rYou can’t simply just squash merge branch_2, becuase the commits from branch_1\rare technically still part of the history of branch_2,\rso a rebase of branch_2 against the new master will cause conflicts,\rand I’ll have duplicate messages if I do another squash+merge of branch_2.\nIf you did a rebase you’ll end up with this:\ngit checkout branch_2 # make sure you\u0026#39;re on branch_2\rgit rebase master\r# fix conflicts\r# so many conflicts\rm1-m2-m3-s1 # master branch + squash-merge\r\\\rx11-x12-x13-y21-y22-y23 # commits from branch_1 and branch_2\rInstead, you need to git cherry-pick the commits against the updated master\rwith just the commits from branch_2.\ngit checkout master # go to master branch so you can cherry-pick commits on top\rgit cherry-pick y21^..y23 # this is how to cherry-pick multiple commits\r# (note the ^ to also include commit y21, otherwise it is left exclusive)\rm1-m2-m3-s1 # master branch + squash-merge\r\\ \\\r\\ y21-y22-y23 # cherry-pick commits from branch_2 # this is good\r\\\rx11-x12-x13 # commits from branch_1 still exist # already in s1\rWe don’t need the commits from branch one anymore (x11 to x13 in the example),\rso we can deleate that branch using git branch -D branch_1,\rnote you have to use a force delete with the -D flag,\rbecause git does not think that branch_1 was merged because we did a squash+merge.\rOtherwise you would’ve been able to use the lowercase -d flag to delete the branch.\nm1-m2-m3-s1 # master branch + squash-merge\r\\\ry21-y22-y23 # cherry-pick commits from branch_2 # yay!\rNow we finally get everthing up to date, and can re-update the pull request for branch 2\rby doing a force push: git push -f origin master.\n\rIn the real world\rThe PR process is the same as usual:\nCreate a branch\rPush the branch to the remote (i.e., Github, in this case)\rPull request!\r\rHowever, Github now gives you an option of which green button you want!\rSpecifically, the option to “Squash and merge”\nThis is fine when you have a totally independent PR\rand there’s no other work that is based of the current work on that branch.\rHowever here’s an example where I created separate branches for each “feature”,\rbut they all rely on the previous feature.\nOutput of my git log --oneline --graph --decorate --all, which I have aliased as git l\nHere’s the current state we’re in after we perform a squash+merge on GitHub\n vignettes\r/\rolder_commits\r\\\rmaster\rHere’s what the actual state of the repository looks like.\ndchen@longclaw [~/git/hub/gradethis] (vignettes) [10:51:06] $ git l\r* 16f4b6b (HEAD -\u0026gt; vignettes) intro to gradethis for learnr docs\r* b4e2873 (origin/vignettes) emphasize not to use ==\r* f66aa39 i don\u0026#39;t want to stash\r| * feb82b3 (origin/gh-pages) Deploy rstudio-education/gradethis to github.com/rstudio-education/gradethis.git:gh-pages\r| * a195fa3 (origin/master, origin/HEAD) fix gradethis demo (#70)\r| | * 4aaee31 (origin/compare_default_formals, compare_default_formals) call_standarise_formals\r| | * 19354dd refactor is_code_same to is_code_identical\r| | * 5591ae8 rename fxn code_is_same -\u0026gt; is_code_same\r| |/ | | * 91cf16e (update_grading_demo) fix gradethis demo\r| |/ | * 9083fc8 (me/master, master) rename functions per #65 update docs and tests (#72)\r| * 5ec1144 Docs to markdown per #68 by @andrie + Barret changes (#69)\r| * 9cc3d6d Test result api fixes#60 (#63)\r|/ * d5cf9de Doc fixes (#62) and function pruning\r\rWhat we want is something like this:\nolder_commits\r\\\rmaster\r\\\rvignettes\rFirst we pull and update master\ndchen@longclaw [~/git/hub/gradethis] (master) [10:51:18] $ git pull origin master\rFrom github.com:rstudio-education/gradethis\r* branch master -\u0026gt; FETCH_HEAD\rUpdating 9083fc8..a195fa3\rFast-forward\rinst/tutorials/grading-demo/grading-demo.Rmd | 249 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-----------------------------------------------------------------------------------------------------\r1 file changed, 112 insertions(+), 137 deletions(-)\rNow that master is up to date,\n vignettes\r/\rolder_commits\r\\\rmaster-master # master has moved up a commit from pull\rwe can cherry-pick the commits of interest\ndchen@longclaw [~/git/hub/gradethis] (master) [10:51:30] $ git cherry-pick f66aa39^..16f4b6b\r[master 28c7ebe] i don\u0026#39;t want to stash\rDate: Wed Aug 14 10:03:25 2019 -0400\r3 files changed, 72 insertions(+)\rcreate mode 100644 vignettes/000-introduction_to_gradethis.Rmd\rcreate mode 100644 vignettes/010-using_equal.Rmd\rcreate mode 100644 vignettes/990-developers_guide.Rmd\r[master 9b84be6] emphasize not to use ==\rDate: Wed Aug 14 11:44:36 2019 -0400\r1 file changed, 4 insertions(+), 4 deletions(-)\r[master 846bb18] intro to gradethis for learnr docs\rDate: Tue Aug 27 10:51:06 2019 -0400\r1 file changed, 151 insertions(+), 9 deletions(-)\r\rNote that I used the \u0026lt;commit\u0026gt;^..\u0026lt;commit\u0026gt; notation while cherry-picking.\rThe ^ means to also include that commit, otherwise it will start from that commit,\rbut not include it (i.e., left exclusive).\nIf you wanted to not use the ^ you need to pick the previous commit hash.\rIn my case it would’ve been d5cf9de.\nNow, we need to clean up the references to our branch.\rSince we cherry-picked commits on master we’re actually still on master.\rSo we need to do update where our branches are pointing to by using `git reset –hard \nFirst we checkout the vignettes branch,\ndchen@longclaw [~/git/hub/gradethis] (master) [10:51:46] $ git checkout vignettes Switched to branch \u0026#39;vignettes\u0026#39;\rdchen@longclaw [~/git/hub/gradethis] (vignettes) [10:51:52] $ git l\r* 846bb18 (master) intro to gradethis for learnr docs\r* 9b84be6 emphasize not to use ==\r* 28c7ebe i don\u0026#39;t want to stash\r* a195fa3 (origin/master, origin/HEAD) fix gradethis demo (#70)\r| * 16f4b6b (HEAD -\u0026gt; vignettes) intro to gradethis for learnr docs\r| * b4e2873 (origin/vignettes) emphasize not to use ==\r| * f66aa39 i don\u0026#39;t want to stash\r| | * feb82b3 (origin/gh-pages) Deploy rstudio-education/gradethis to github.com/rstudio-education/gradethis.git:gh-pages\r| | * 4aaee31 (origin/compare_default_formals, compare_default_formals) call_standarise_formals\r| | * 19354dd refactor is_code_same to is_code_identical\r| | * 5591ae8 rename fxn code_is_same -\u0026gt; is_code_same\r| |/ |/| | | * 91cf16e (update_grading_demo) fix gradethis demo\r| |/ |/| * | 9083fc8 (me/master) rename functions per #65 update docs and tests (#72)\r* | 5ec1144 Docs to markdown per #68 by @andrie + Barret changes (#69)\r* | 9cc3d6d Test result api fixes#60 (#63)\r|/ * d5cf9de Doc fixes (#62) and function pruning\rand then set it to the correct commit location.\ndchen@longclaw [~/git/hub/gradethis] (vignettes) [10:52:04] $ git reset --hard 846bb18\rHEAD is now at 846bb18 intro to gradethis for learnr docs\rWe then repeat the process for the master branch.\nFirst checkout master\ndchen@longclaw [~/git/hub/gradethis] (vignettes) [10:52:07] $ git checkout master\rSwitched to branch \u0026#39;master\u0026#39;\rYour branch is ahead of \u0026#39;origin/master\u0026#39; by 3 commits.\r(use \u0026quot;git push\u0026quot; to publish your local commits)\rThen reset --hard to the correct location:\ndchen@longclaw [~/git/hub/gradethis] (master) [10:52:11] $ git reset --hard a195fa3\rHEAD is now at a195fa3 fix gradethis demo (#70)\rNow everything is in the right place\nolder_commits\r\\\rmaster\r\\\rvignettes\rdchen@longclaw [~/git/hub/gradethis] (master) [10:52:21] $ git l\r* 846bb18 (vignettes) intro to gradethis for learnr docs\r* 9b84be6 emphasize not to use ==\r* 28c7ebe i don\u0026#39;t want to stash\r* a195fa3 (HEAD -\u0026gt; master, origin/master, origin/HEAD) fix gradethis demo (#70)\r| * feb82b3 (origin/gh-pages) Deploy rstudio-education/gradethis to github.com/rstudio-education/gradethis.git:gh-pages\r| * 4aaee31 (origin/compare_default_formals, compare_default_formals) call_standarise_formals\r| * 19354dd refactor is_code_same to is_code_identical\r| * 5591ae8 rename fxn code_is_same -\u0026gt; is_code_same\r|/ | * 91cf16e (update_grading_demo) fix gradethis demo\r|/ * 9083fc8 (me/master) rename functions per #65 update docs and tests (#72)\r* 5ec1144 Docs to markdown per #68 by @andrie + Barret changes (#69)\r* 9cc3d6d Test result api fixes#60 (#63)\r| * b4e2873 (origin/vignettes) emphasize not to use ==\r| * f66aa39 i don\u0026#39;t want to stash\r|/ * d5cf9de Doc fixes (#62) and function pruning\rFinally, we can update the GitHub remote by force bushing the branch\r(remember to be on the branch before pushing).\ndchen@longclaw [~/git/hub/gradethis] (master) [10:53:25] $ git checkout vignettes Switched to branch \u0026#39;vignettes\u0026#39;\rdchen@longclaw [~/git/hub/gradethis] (vignettes) [10:53:30] $ git push -f origin vignettes Enumerating objects: 15, done.\rCounting objects: 100% (15/15), done.\rDelta compression using up to 4 threads\rCompressing objects: 100% (14/14), done.\rWriting objects: 100% (14/14), 3.27 KiB | 3.27 MiB/s, done.\rTotal 14 (delta 7), reused 0 (delta 0)\rremote: Resolving deltas: 100% (7/7), completed with 1 local object.\rTo github.com:rstudio-education/gradethis.git\r+ b4e2873...846bb18 vignettes -\u0026gt; vignettes (forced update)\rThe final Graph now looks like this\ndchen@longclaw [~/git/hub/gradethis] (vignettes) [10:54:13] $ git l\r* 846bb18 (HEAD -\u0026gt; vignettes, origin/vignettes) intro to gradethis for learnr docs\r* 9b84be6 emphasize not to use ==\r* 28c7ebe i don\u0026#39;t want to stash\r* a195fa3 (origin/master, origin/HEAD, master) fix gradethis demo (#70)\r| * feb82b3 (origin/gh-pages) Deploy rstudio-education/gradethis to github.com/rstudio-education/gradethis.git:gh-pages\r| * 4aaee31 (origin/compare_default_formals, compare_default_formals) call_standarise_formals\r| * 19354dd refactor is_code_same to is_code_identical\r| * 5591ae8 rename fxn code_is_same -\u0026gt; is_code_same\r|/ | * 91cf16e (update_grading_demo) fix gradethis demo\r|/ * 9083fc8 (me/master) rename functions per #65 update docs and tests (#72)\r* 5ec1144 Docs to markdown per #68 by @andrie + Barret changes (#69)\r\rConclusion\rIt’s taken me many years to be comfortable with Git.\rMy previous go-to method was to rebase one branch on top of the other,\rbut with the squash-and-merge method, a lot of commits get repeated,\rand it just makes rebaseing a pain with all the commits and merge conflicts.\nIn this particular workflow I found cherry-pick to be way more convenient,\rand led to fewer merge conflicts.\nAgain, the notation for cherry-pick is weird, in that it is left exclusive unless you put in the ^,\ri.e.,\ngit cherry-pick \u0026lt;commit-start\u0026gt;^..\u0026lt;commit-end\u0026gt;\rnot\ngit cherry-pick \u0026lt;commit-start\u0026gt;..\u0026lt;commit-end\u0026gt;\r\r","date":1566864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582960376,"objectID":"a3f80dcd89b77ed362bb4e7a03b8cc70","permalink":"/2019/08/27/git-squash-and-merge-workflow/","publishdate":"2019-08-27T00:00:00Z","relpermalink":"/2019/08/27/git-squash-and-merge-workflow/","section":"post","summary":"I have a love-hate relationship with Git.\rIt took me years of following cookbooks and following strict set of commands to\rget a sense of how it works.\rI still have to spend time wrangling Git at times,\rbut I’ve gotten to the point where I’m just annoyed, instead of scared.\rHere is the branching-squash-and-merge workflow I’ve been using at RStudio.\nAlso, sorry I may be glossing over some of the Git basics in this post…","tags":["rstudio-internship","git","workflow"],"title":"Git Squash and Merge Workflow","type":"post"},{"authors":[],"categories":[],"content":"\r\rOne of the cool things about working on gradethis\r(grader need to be renamed)\ris that we end up doing things that aren’t common in R (i.e., grading and comparing code).\nI discovered an inconsistency with the == operator when comparing (long) R expressions.\nA quick primer on expressions\rIn R, you can create an expression using the\rquote() function.\rThis is essentially the code that R will execute.\rIt is similar to the “string” that will be executed,\rbut an actual string in R will return a string, not a command or set of instructions that R will execute.\nCompare:\n3 + 3\r## [1] 6\rWhich will return the executed result of 3 + 3 and\n\u0026quot;3 + 3\u0026quot;\r## [1] \u0026quot;3 + 3\u0026quot;\rwhich will return the string \"3 + 3\"\nwith:\nquote(3 + 3)\r## 3 + 3\rwhich returns the expression 3 + 3 that is the instruction to R without actually evaluating it.\nIf we wanted to evaluate the expression, we can call eval.\neval(quote(3 + 3))\r## [1] 6\rYou can read more about expressions in the\rExpressions Chapter in Advanced R.\n\rThe “bug”\rThe bug was detected in gradethis where we want to compare student submitted code with the instructor solution.\rThere are multiple steps in the comparison process,\rbut the first step is to simply check if the two bits of code are the same.\rThat way, we can stop there and not have to go through the process to detect where the actual differences are.\nThe comparison code was originally written to use == to compare the expressions.\nuser \u0026lt;- quote(3 + 3)\rsolution \u0026lt;- quote(3 + 3)\ruser == solution\r## [1] TRUE\rGarrett Grolemund put in a bunch of examples that show some weird behaviour.\rI initially thought it had to do with name spacing the function name, or after using the : notation to select columns in a dataframe via tidyselect.\nWhen the two expressions are the same, we get TRUE as expected\n# supposed to return TRUE\ru \u0026lt;- quote(tidyr::gather(key = key, value = value, new_sp_m014:newrel_f65, na.rm = TRUE))\rs \u0026lt;- quote(tidyr::gather(key = key, value = value, new_sp_m014:newrel_f65, na.rm = TRUE))\ru == s\r## [1] TRUE\rBut when we change the values for na.rm, we also get TRUE when the expressions are not the same.\n# supposed to return FALSE\ru \u0026lt;- quote(tidyr::gather(key = key, value = value, new_sp_m014:newrel_f65, na.rm = TRUE))\rs \u0026lt;- quote(tidyr::gather(key = key, value = value, new_sp_m014:newrel_f65, na.rm = FALSE))\ru == s\r## [1] TRUE\rBut it seems if we get rid of the tidyselect column selector, we get the correct result.\n# If we remove the third argument the error goes away\ru \u0026lt;- quote(tidyr::gather(key = key, value = value, na.rm = TRUE))\rs \u0026lt;- quote(tidyr::gather(key = key, value = value, na.rm = FALSE))\ru == s\r## [1] FALSE\rI brought this up on our daily shiny-core stand-ups and Winston Chang thought it may have something to do with the deparse function since it doesn’t actually matter what the expressions being compared are.\nu \u0026lt;- quote(f(x123456789012345678901234567890123456789012345678901234567890, 1))\rs \u0026lt;- quote(f(x123456789012345678901234567890123456789012345678901234567890, 2))\ru == s\r## [1] TRUE\rYou can see Winston’s comment and link to R code in question\rhere.\nPretty much when == is used to compare expressions, the expressions are passed through deparse.\rWhen deparse is run on an expression, it breaks it up into vectors that are 60L characters long,\rwhich is fine, but the R bug is when the comparison is only performed with the first element of the vector.\rThat’s why only the end of the expressions seem to “not matter”.\n\rReporting the bug\rI reported the findings to the r-devel mailing list\nWhere, even after botching my first listserv submission, I got a response from Martin Maechler (R-core)\n\rLooking at that and its context, I think we (R core) should\rreconsider that implementation of ‘==’ which indeed does about\rthe same thing as deparse {which also truncates at some point by\rdefault; something very very reasonable for error messages, but\rundesirable in other cases}.\nBut I think it’s fair expectation that comparing calls [“language”]\rwith ‘==’ should compare the full call’s syntax even if that may\roccasionally be very long.\n\rSo it is actually a behavior that will get patched one day.\n\rThe fix\rWe ended up making changes\rto gradethis by using identical() while comparing quoted expressions.\nu \u0026lt;- quote(f(x123456789012345678901234567890123456789012345678901234567890, 1))\rs \u0026lt;- quote(f(x123456789012345678901234567890123456789012345678901234567890, 2))\ridentical(u, s)\r## [1] FALSE\rUsing identical() is a much better way when we are comparing code and results, because == will return a matrix when comparing 2 dataframes where using all has problems when there are NA missing values.\nWe want to see if the 2 vectors are the same\nu \u0026lt;- c(1, 2, 3)\rs \u0026lt;- c(1, 2, NA)\rall(u == s)\r## [1] NA\rWe can remove missing values, but now when either the user code or solution code does contains an NA it gets ignored.\nu \u0026lt;- c(1, 2, 3)\rs \u0026lt;- c(1, 2, NA)\rall(u == s, na.rm = TRUE)\r## [1] TRUE\ru \u0026lt;- c(1, 2, NA)\rs \u0026lt;- c(1, 2, 3)\rall(u == s, na.rm = TRUE)\r## [1] TRUE\rNow, we\rnudge toward using identical and\rraise a warning\rwhen we detect ==.\nu \u0026lt;- c(1, 2, NA)\rs \u0026lt;- c(1, 2, 3)\ridentical(u, s)\r## [1] FALSE\rDoes Donald Knuth owe me a dollar now?\n\r","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582960376,"objectID":"df67ae366d7beb9b3bb324244310d085","permalink":"/2019/08/06/inconsistencies-with-in-r/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/2019/08/06/inconsistencies-with-in-r/","section":"post","summary":"One of the cool things about working on gradethis\r(grader need to be renamed)\ris that we end up doing things that aren’t common in R (i.e., grading and comparing code).\nI discovered an inconsistency with the == operator when comparing (long) R expressions.\nA quick primer on expressions\rIn R, you can create an expression using the\rquote() function.\rThis is essentially the code that R will execute.","tags":["rstudio-internship","r"],"title":"Inconsistencies with the `==` operator in R","type":"post"},{"authors":[],"categories":[],"content":"\r\rI was initially going to get some small bookshelves to put my Oculus sensors on,\rbut you can take your Oculus sensors off the stand!\rThis way you wont have an awkward way to mount the sensors against the wall.\nI also have a 3rd rear sensor, but when I was (re)setting up my sensors,\rI ended up just leaving it on my desk.\rThe parts list below was from me shopping at Home Depot assuming I was going to mount all 3 against the wall.\nNeed:\n1/4in-20 X 1in Machine Screws (4 pack – it comes with screws and nuts, you need both): $1.18\r4in Corner Brace (4 pack): $7.24\r\rOptional:\nTemporary sensor setup: Painters tape, or anything that won’t strip off your paint.\rTo mount the brackets against the wall (Home Depot): 3 1/2\" x 3/4\" Velcro strips (4 pack): $2.98\rCable management: 1/2\" plastic KWIK Clip (4 pack) (Home Depot): 2 x 3.28 = $6.56\rUSB extension for sensors (Micro Center): 2 x 10Ft QVS USB 2.0 Extension Cable A to A: 2 x 19.99 = $39.98\r\rTools:\nYou’re also going to need a Phillips head screwdriver and pliers\rto tighten the nut without moving and/or loosening the sensor\nThe Parts\rHere’s what the parts look like:\n1/4in-20 X 1in Machine Screws (4 pack): $1.18\n4in Corner Brace (4 pack): $7.24\n\rNow the Assembly\rRemove the Sensor\rMount the sensors on the brace\rFinalize the mounts\r\rRemove the sensor\rThe top of the sensor screws off.\rI ended up taking just the base stand with me to Home Depot to check the type of screw that was needed.\r1/4in-20 worked for me. I found the 1\" long screws was enough to connect everything together nicely.\n\rMount the sensors on the brace\rYou can mount the sensors to the brace any way you want,\rbut I found that it was better to mount the sensors upside-down\rsince the sensors pivot upwards (on the stand) more than the other direction.\rAlso the entire rig is pretty light, so something like painters tape can hold up the entire unit against the wall\r(it’s too heavy for painters tape to hold on the ceiling)\rfor you to try to get everything in the right place.\rI had to do a lot of adjustments during the Oculus sensor setup.\nSensors right-side up:\nSensors upside-down:\n\rFinalize the mounts\rSeriously, don’t screw or mount (I used the 3 1/2in Velcro strips) until after you go though the Oculus sensor setup.\rI had to go back and adjust placement and angles multiple times.\rI even had to scrap my initial idea of mounting the rear sensor against the wall because it was too far away from the other sensors, and I found it awkward to mount it on the ceiling.\nHere’s what the front 2 sensors look like, you actually need to pivot them inwards a lot more than I thought,\rthey’re almost at 45-degrees and pretty much point right at you when you’re in the middle of the play area.\nHere’s the entire setup looks like, with the 3rd (rear) sensor on my desk by the microphone.\rWhat’s not pictured: the train wreck of packaging, boxes, papers, cables beind me :p\n\r\rWhat I would do differently\rIf I was more motivated, I would’ve found a way to mount the rear sensor on the ceiling behind me.\rBut It didn’t seem like the Oculus sensor could pivot downward enough with the braces up there.\rI also wasn’t that motivated to have it on the ceiling,\rbecause now I get to return my 39ft USB extension cable from Micro Center ($$$$).\nIf I living in a more permanent location I would’ve gotten a better cable management system than the plastic clips.\rBy the electrical section in Home Depot there were long adhesive wire management cases that would hide the black cables much better.\nThe way my computer was setup, one of the 10ft USB extensions was too long,\rand the other was too short.\rI had to use a another USB extension for the sensor that was further away.\nAlso, my standing desk was my old childhood dining room table that I just (i.e., last night) upgraded with\rthe Autonomous.ai Home Edition DIY legs\n\r","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582960376,"objectID":"561175adaf3e982d1cbf8fe8a3936c30","permalink":"/2019/08/03/mounting-oclus-rift-sensors-on-the-wall/","publishdate":"2019-08-03T00:00:00Z","relpermalink":"/2019/08/03/mounting-oclus-rift-sensors-on-the-wall/","section":"post","summary":"I was initially going to get some small bookshelves to put my Oculus sensors on,\rbut you can take your Oculus sensors off the stand!\rThis way you wont have an awkward way to mount the sensors against the wall.\nI also have a 3rd rear sensor, but when I was (re)setting up my sensors,\rI ended up just leaving it on my desk.\rThe parts list below was from me shopping at Home Depot assuming I was going to mount all 3 against the wall.","tags":["home"],"title":"Mounting Oclus Rift Sensors on the Wall","type":"post"},{"authors":[],"categories":[],"content":"I finally got everything moved over to blogdown with the Hugo Academic theme. Thanks so much to Allison Hill, who ran the summer-of-blogdown tutorial for us RStudio interns.\nThe transition was pretty seamless. Mainly because I didn\u0026rsquo;t really have that much content to move over. The biggest change was I had to commout my categories tag in my YAML post headers becuase they were causing the site to not build.\nWhile writing this post, I just realized that the way categories and tags are specified in hugo are not the same as jekyll.\nIn jekyll I could write categories as\ncategories: R  or\ncategories: - R  But in hugo it seems it\u0026rsquo;s a list written as\ncategories: ['R']  Anyway, it was fun going through and enabling/disabling different widgets. I\u0026rsquo;ll keep it simple for now, and build on the site accordinly. I\u0026rsquo;m just happy I can maintain my website without worrying how I had Jekyll setup to properly serve the site. The folks at RStudio truely make things easy for us end users.\nThe other change I did was to make my old repository, chendaniely.github.io a submodule of my blogdown site repo and change the target folder the published contents will be from the default public by addding this to my main config.toml file\npublishDir = \u0026quot;chendaniely.github.io\u0026quot;  This way my old url will still work (I\u0026rsquo;m really attached to the .github.io domain), but I can still have netlify deploy everything and have my rbind.io domain as well. Everything will look the same, and I\u0026rsquo;ll work on redirecting everything to a single domain sometime later.\nI also needed to change the [build] section in my netlify.toml file to point to the new build location\n[build] # publish = \u0026quot;public\u0026quot; publish = \u0026quot;chendaniely.github.io\u0026quot;  Netlify is a little wonky when you have SSH keys enabled with the submodule. I get an error message because when Netlify tries to clone the submodule I get a permission error\nfatal: clone of 'git@github.com:chendaniely/chendaniely.github.io.git' into submodule path '/opt/build/repo/chendaniely.github.io' failed  That makes sense, the Netlify servers do not have SSH keys setup on my GitHub account. I haven\u0026rsquo;t figured out how to do this yet, so I switched my .gitmodule url to the https link. However, that means, for the time being, I created another remote with my SSH url (git remote add originssh ...) and I have to push using that remote instead of the origin url (which is the https link).\nNow to start that massive backlog of things I\u0026rsquo;ve been doing this summer ( :\n","date":1563840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"5a8dbd4bc21bcd42ccc8844d9d7ea48c","permalink":"/2019/07/23/new-website-a-la-blogdown/","publishdate":"2019-07-23T00:00:00Z","relpermalink":"/2019/07/23/new-website-a-la-blogdown/","section":"post","summary":"I finally got everything moved over to blogdown with the Hugo Academic theme. Thanks so much to Allison Hill, who ran the summer-of-blogdown tutorial for us RStudio interns.\nThe transition was pretty seamless. Mainly because I didn\u0026rsquo;t really have that much content to move over. The biggest change was I had to commout my categories tag in my YAML post headers becuase they were causing the site to not build.","tags":["rstudio-internship","r","hugo","blogdown","website"],"title":"New Website a la Blogdown!","type":"post"},{"authors":null,"categories":null,"content":" The main topics and events of last week were:\n Much git. Metaprogramming and non-standard evaluation (NSE) in R Four 1-hour workshops by Allison Hill on the summer-of-blogdown  moving things over from jekyll will take some time   So many of the random things I\u0026rsquo;ve tinkered with in the past have come front and center. As an educator, I know seeing these things again make learning and understanding them easier. You build on your previous knowledge to solidify, fix, and fill in gaps in your mental model. The process repeats until you get an understanding about a topic.\nFor me, I\u0026rsquo;m getting a better foundation to how NSE works and how it all plays together within the Tidyverse.\nGit I got some things merged!\nThe pull-request that was broken and merged on my first day finally was fixed and merged. I also got to work with the lintr package and merged it into grader too.\nThis week was probably the first time I\u0026rsquo;ve used git amend in a long time (if ever?). I\u0026rsquo;ve typically always just made the commit, and run git rebase -i to squash and/or amend my commits. I can see why common operations like making changes to the previous commit would have a shortcut. I typically don\u0026rsquo;t use these features because it\u0026rsquo;s another thing to teach, and understanding git rebase -i is more general than git commit --amend.\nWhat --amend allows you to do is replace the previous git commit with another one. You can fix the commit message, or add/fix file you missed. Theses are all ways to make the commit history cleaner.\nThe recipe looks like this:\ngit add \u0026lt;file\u0026gt; git commit --amend \u0026lt;Fix/modify the commit message\u0026gt; git push -f origin master  The last line does a -f force push, because the commit is actually different from the one before you --amended.\nR package versions There\u0026rsquo;s a convention about version numbering adding a .9000+ after the patch (e.g., v0.1.4.9000) to show a a development version number. You can couple this with the DESCRIPTION file by forcing a particular version to make sure you and the team all have access to the same development features.\nI came across this in grader that has a Imports for learnr (\u0026gt;= 0.9.2.9001).\ngrader progress We\u0026rsquo;re probably going to change the name of the package to gradethis because the package gradeR (note capital R) was submitted to CRAN right as I started.\nHere\u0026rsquo;s what I learned about the library I\u0026rsquo;m working on this week:\n learnr set\u0026rsquo;s the checker function  In the knitr chunk exercise.checker specifies the checker function  tutorial_options(exercise.timelimit = 60, exercise.checker = grade_learnr)  In this example, grade_learnr is the main entrypoint from learnr to grader and my work starts with this function.  checker is called on line 129 in exercise.R in learnr  the checker function (i.e., grade_learnr) returns a value depending on what is passed into it  if missing: returns a list() with message, correct, type, and location keys if error: graded object (named checked_result of class grader_graded) with correct and message or evaluated code    There\u0026rsquo;s a bunch of stuff within the exercise.R function in learnr that captures information from shiny, sets up the knitr environment, and inserts the output and results into the correct place in the DOM. That\u0026rsquo;ll be a separate writeup when I leave the grader world.\nFor the next week or so the goal is to update the check_result API, which got me down the rabbit hole of non-standard evaluation in R (I\u0026rsquo;ll talk about it in a separate set of posts).\nNon-standard Evaluation (NSE) I gave a talk about writing functions in R which touched on NSE but it was pretty superficial. Since NSE is so crucial to grader I\u0026rsquo;ll write a series of posts about this topic and eventually turn it into a talk.\nIn the meantime, here are the materials (in no particular order) I\u0026rsquo;ll be reading to:\n https://dplyr.tidyverse.org/articles/programming.html https://tidyeval.tidyverse.org/ https://ggplot2.tidyverse.org/dev/articles/ggplot2-in-packages.html#using-aes-and-vars-in-a-package-function https://adv-r.hadley.nz/  Misc Other random things I\u0026rsquo;ve discovered this week\ndplyr::count vs dplyr::tally From the docs:\n tally() is a convenient wrapper for summarise that will either call n() or sum(n) depending on whether you\u0026rsquo;re tallying for the first time, or re-tallying. count() is similar but calls group_by() before and ungroup() after. If the data is already grouped, count() adds an additional group that is removed afterwards.\n dplyr::count can count observations with 0 counts (useful for group_by operations) with the .drop argument\npryr::standarise_call Manipulating the function call is black magic NSE voodoo. This is the stuff that is happening within grader that gets student code, solution code, learnr and grader arguments that are all passed into grade_learnr.\n# code below was copy/pasted from the console my_add \u0026lt;- function(x, y) { x + y } # pass in part of an expression call \u0026lt;- pryr::standardise_call(quote(my_add(x = 3))) # on the fly add more parameters! call$y \u0026lt;- 10 # evaluate the thing eval(call) ## [1] 13  This all uses the global environment, but grader will be doing this type of thing with separate environments for each exercise that will be checked.\nAlso, this is all how match.call works in base R.\ncheckmate package There is a package called checkmate that is unit testing (e.g., testthat) on steroids. It allows you to more specific type and argument checking in R. I haven\u0026rsquo;t work with the package personally yet, but it does seem to be like type hints in Python and allows more specific checks into what objects in R contain.\nCredentials One of the coolest things about being an intern at RStudio is being on the slack channel! I try to keep my questions reserved but one of the things that have always bothered me was how store and access credentials for R. Putting in API keys in .Renviron are common practice, but I piggy-backed on another intern\u0026rsquo;s question by asking about storing passwords more securely than in a plain text file.\nI\u0026rsquo;ve used the rstudioapi, secret, and getPass libraries before, but as Raymond Hettinger always says: There must be a better way.\nThe resource I was given was to look at how database credentials are stored: https://db.rstudio.com/best-practices/managing-credentials/\nSummer of blog down Lastly, the great Allison Hill hosted a series of blogdown workshops for people who were interested, summer-of-blogdown. It was a total of 4 days and we covered the basics of blogdown, how to pick (the academic) themes, deploying it on netlify, and best ways to maintain the site.\nI didn\u0026rsquo;t realize how amazingly flexible the academic theme was until this workshop. I\u0026rsquo;ll be sure to move my own website over to blogdown + academic one of these days.\nI\u0026rsquo;m currently trying to find out how to save urls in a common location so they can be maintained in one place and be used in links throughout the site. The ongoing search for how to use variables in an md document (tl;dr: you can\u0026rsquo;t, but you might still be able to do what I want): https://discourse.gohugo.io/t/variables-in-markdown/7113/12. It almost seemed that site variables were going to be the way to go, but that ended up in a dead end.\nWhat I\u0026rsquo;m most excited about is the ability to write posts in Rmd and jupyter notebooks for R and Python posts.\nThings I\u0026rsquo;ve learned the 4 days:\n Each folder in content is a \u0026ldquo;section\u0026rdquo; and each \u0026ldquo;section\u0026rdquo; has a \u0026ldquo;page\u0026rdquo; /content/home/ contain widgets Learn x in y minutes website for TOML From Greg Wilson: Put a LICENSE and CITATION + orcid on your website. Make the librarians happy.  ","date":1560816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"8f72fcc314c5ce37c3b671e45b53ded5","permalink":"/2019/06/18/rstudio-internship-week-2/","publishdate":"2019-06-18T00:00:00Z","relpermalink":"/2019/06/18/rstudio-internship-week-2/","section":"post","summary":"The main topics and events of last week were:\n Much git. Metaprogramming and non-standard evaluation (NSE) in R Four 1-hour workshops by Allison Hill on the summer-of-blogdown  moving things over from jekyll will take some time   So many of the random things I\u0026rsquo;ve tinkered with in the past have come front and center. As an educator, I know seeing these things again make learning and understanding them easier.","tags":["R","rstudio-internship"],"title":"RStudio internship week 2","type":"post"},{"authors":null,"categories":null,"content":" I\u0026rsquo;m still pinching myself about being one of the RStudio interns this year. It\u0026rsquo;s an unbelievable opportunity and I\u0026rsquo;ve been half panicked and fighting imposter syndrome since the announcement was made in March. My meeting with Greg Wilson on Friday (2019-06-07) went something like this:\nGreg Wilson: How's the internship going? Me: I'm panicked, but really excited. Greg Wilson: Good. That's how interns should feel.  I\u0026rsquo;m working on the grader package (with Garrett Grolemund and Barret Schloerke) which aims to check code against a solution. This package integrates with the learnr package, that allows an RMarkdown document to be run as an interactive tutorial (more on this after the break).\nMy internship began with a broken pull request that needs to be fixed.\nThe learnr/grader packages learnr allows users to write interactive tutorials/lessons (using RMarkdown) that give instructors to write interactive tutorials (using shiny). Learners will be able to follow the tutorial write and execute R code that will be graded. Under the hood, learnr is an RMarkdown document with runtime: shiny. This allows the user (e.g., instructor) to write tutorials using markdown and provide solutions using code chunks and all the magic of capturing and checking the answer with shiny and grader.\nThere are two main ways code can be graded.\n Comparing results Comparing the provided student code with the solution code (i.e., strict checking).  One way this can be done is by looking at the abstract syntax tree. Another way would be some fancy set of string matching / regular expressions.   When I started, there were some API changes that were made and I\u0026rsquo;ve spent my first week going down various rabbit holes fixing tests and getting R CMD check to pass. Barret has been super helpful and supportive by walking me through the code base and helping me fix the broken things. I\u0026rsquo;m pretty sure he did all the work in my PR.\nGeneral goals for the summer This will be a moving target, but it seems like after the first week these may be good goals for the summer:\n A (pkgdown) vignette for the grader user API and a separate one for the developer API. Blog post of how learnr works, and what order to expect error messages (see use-case maps). Put together a grader example without the use of learnr  An example would be to pass grader 2 R scripts (the student code and the solution code). This came up while I was talking to Greg and he brought up \u0026ldquo;Working Effectively with Legacy Code\u0026rdquo; by Michael Feathers  This will help me find the entry points to the package, write isolated tests, and have a clearer goal of the project. Along with use-case maps, this will help me be a better software developer and understand what\u0026rsquo;s actually going on (did I mention I\u0026rsquo;m half panicked and fighting imposter syndrome?)    Other goals would be to have:\n Virtual coffee meetings with other people in the company (there\u0026rsquo;s a channel called \u0026ldquo;virtual-donut\u0026rdquo; that has a scheduler bot) See if I can draft up and hunt down the people for a PhD topic.  Greg really planted a seed in my head a few months ago about putting together materials to help \u0026ldquo;front-line health practitioners (e.g., nurse and general practitioners) use and understand data\u0026rdquo;. I think I can\u0026hellip; I think I can\u0026hellip; I think I can\u0026hellip;   Random things I\u0026rsquo;ve learned so far  I think I really like this 100% remote work schedule, I\u0026rsquo;ve been up way earlier than usual finishing things before a morning meeting and then taking my time the rest of the day planning and implementing things.  I also feel like a teenager in chatrooms talking to everyone (thanks Slack!)  Using Templates with Roxygen2: allows roxygen2 docstrings to be reused by using a @template Rd keyword Use-case maps: document and show the various components of an application, but also show what components are used for a given action  This would be great for learnr and grader since I\u0026rsquo;ll pretty much be doing this for myself just to figure out all the moving parts of the system and how each component works in the full application.  lintr: is a way to do static code analysis in R.  You can use this to check for (potential) errors before runtime Pretty sure I can take the same idea and apply it to grader  All these years practicing my Git-fu seems to be paying off. My mentor, Garrett Grolemund, is RStudio employee number insert number that can be represented with 1 hand :O There\u0026rsquo;s an \u0026ldquo;emoji-psa\u0026rdquo; channel at RStudio. It\u0026rsquo;s active. People really know their emojis. owo  Misc  I\u0026rsquo;ve given up trying to sync the GSuite with Thunderbird and having a unified calendar.  I\u0026rsquo;m using openWMail again for my RStudio calendar/drive. Still have all my emails in Thunderbird.  Not all the interns started on the same date. Most of us started June 3rd, but some started earlier and others will start later.  Us interns have our own slack workspace! We had our first interns-only coffee chat on Friday.  Malcolm Barrett, Julia Blum, Joyce Cahoon, Desiree De Leon, Dewey Dunnington, and Maya Gans all met this week. We\u0026rsquo;re planning to have an intern \u0026ldquo;host\u0026rdquo; one each week so everyone gets a chance to meet some (if not all) of us.   Allison Hill is hosing a \u0026ldquo;Summer of blogdown\u0026rdquo; this week, so I\u0026rsquo;ll probably be doing another website update.  ","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"2ccb3b9f1e5f9355c75d88c1eb62ea34","permalink":"/2019/06/10/and-were-off-rstudio-internship-week-1-complete./","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/2019/06/10/and-were-off-rstudio-internship-week-1-complete./","section":"post","summary":"I\u0026rsquo;m still pinching myself about being one of the RStudio interns this year. It\u0026rsquo;s an unbelievable opportunity and I\u0026rsquo;ve been half panicked and fighting imposter syndrome since the announcement was made in March. My meeting with Greg Wilson on Friday (2019-06-07) went something like this:\nGreg Wilson: How's the internship going? Me: I'm panicked, but really excited. Greg Wilson: Good. That's how interns should feel.  I\u0026rsquo;m working on the grader package (with Garrett Grolemund and Barret Schloerke) which aims to check code against a solution.","tags":["R","rstudio-internship"],"title":"And we're off! RStudio internship week 1, complete.","type":"post"},{"authors":["Daniel Chen"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":" EDIT: I\u0026rsquo;ve added the notes from @dataandme and linked to people\u0026rsquo;s twitter and slides (if I found them). This is probably going to be an ongoing process\u0026hellip;\n Another year and another talk at the NYC R Conference. As always, the conference was filled with excellent speakers (I\u0026rsquo;m biased here becuase I was one of them\u0026hellip;), food, and people.\nBrooke Watson (@brookLYNevery1) did a fantastic job illustrating and summarzing all of the talks. So I\u0026rsquo;ve just linked to all her tweets (after the break).\nOh\u0026hellip; and I got my books signed :)\n #rstatsnyc as told by #autographs. #rstats #nycdatamafia #python #datascience\nA post shared by Daniel Chen (@chendaniely) on Apr 22, 2018 at 3:59pm PDT\n \nAccelerating Cancer Research with R Sandy Griffith, Flatiron Health, @sgrifter\n.@sgrifter kicking off the @nyrconf with a bang 🎉 #rstatsnyc pic.twitter.com/9A9R0ozLBc\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nSkin First, Makeup Second: A Metaphor for Data Science Eurry Kim, Glossier, @eurryPlot\n.@eurryPlot metaphors on 💯, skincare on 💯, walk-out song on 💯. #rstatsnyc pic.twitter.com/vt8FpS4EYg\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nNYR 🔥 thus far\u0026hellip; @sgrifter \u0026amp; @eurryPlot starting out 💪#rstatsnyc pic.twitter.com/6v8qZTYE33\n\u0026mdash; Mara Averick (@dataandme) April 20, 2018 \nR for Big Data in the Cloud Marck Vaisman, Microsoft, @wahalulu\n#big #data #in #the #cloud !!!!! And abstracting away the backend, which truly sounds ideal. By @wahalulu #rstatsnyc pic.twitter.com/3z3L2Joiyq\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nThe Lesser Known Stars of the Tidyverse Emily Robinson, DataCamp, @robinson_es\nThe lesser known stars of the tidyverse, from a greater known star of @RLadiesNYC 🌟 @robinson_es at #rstatsnyc pic.twitter.com/zGQQ4y0fUR\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \n👻 Big Data in the ☁ w/ #rstats 🗣 @wahalulu\n\u0026amp;\nLesser known 🌟s of the #tidyverse 💃 @robinson_es\n🗽 #rstatsnyc pic.twitter.com/gc3YkqxgOu\n\u0026mdash; Mara Averick (@dataandme) April 20, 2018 \nDeep Learning vs Machine Learning in R Jared P. Lander, Lander Analytics, @jaredlander\n.@jaredlander speaking truth to deep learning hype. #rstatsnyc pic.twitter.com/emESZ48Ji6\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nDoing Data Science Dan Chen, Virginia Tech, @chendaniely\nslides\n.@chendaniely on using best practices while getting stuff done. TL;DR: in structure there is freedom. #rstatsnyc pic.twitter.com/SssCn47HmW\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nI\u0026#39;ve uploaded and linked my #rstatsnyc slides on doing #datascience, link in the ongoing blogpost: https://t.co/wqFARzBbss. Comes complete with my dog, Hobbes. #rdogladies. #python #rstats #nycdatamafia pic.twitter.com/fiR0h5y8OB\n\u0026mdash; Dⓐniel Chen (@chendaniely) April 23, 2018 \nFirst time converting slides to PDF. First time publishing on Speaker Deck. Hope it turns out okay \u0026gt;.\u0026lt;\n Forecasting at Scale: How and Why We Developed Prophet for Forecasting at Facebook Sean Taylor, Facebook, @seanjtaylor\n.@seanjtaylor talked about scaling machine learning, which is really about making tools more usable and interpretable. #rstatsnyc pic.twitter.com/KJMy0Q4LdL\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nThe Modern R Data Package Noam Ross, ROpenSci \u0026amp; EcoHealth Alliance, @noamross\n.@noamross talked about the best ways to make data available and evergreen for analysis in R. Lots of lessons learned from the data packaging at @EcoHealthNYC! pic.twitter.com/bOJrkahuvT\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nOpen Source Sampling: Building and Remixing Packages in R Brooke Watson, EcoHealth Alliance, @brookLYNevery1\nFinally got around to drawing my own #rstatsnyc talk on the train home. Key point: copy and remix from your unique multitude of interests, and you’ll probably make something new. Thank you for not judging my unrealistic hair expectations! Slides here: https://t.co/ZxA2jBxL2c pic.twitter.com/E7pPoPes8T\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nR in USMA Dusty Turner, United States Military Academy, @DTDusty\n.@DTDusty using shiny apps to make learning more interesting for everyone, from 6th graders to army cadets. #rstatsnyc pic.twitter.com/7praNmFY1q\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nApplying Deep Learning to Satellite Images to Estimate Violence in Syria and Poverty in Mexico Jonathan Hersh, Chapman University, @DogmaticPrior\n.@DogmaticPrior on balancing model accuracy with model usefulness in humanitarian settings. #rstatsnyc pic.twitter.com/zVDgxC95Fr\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nApache Arrow: A Cross-Language Development Platform for In-Memory Analytics Wes McKinney, Two Sigma, @wesmckinn\nslides\n.@wesmckinn thinking BIG about data frame portability. Really excited to see what happens with Ursu labs. #rstatsnyc pic.twitter.com/j3JfIfz3c4\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nRest of #rstatsnyc Day 1 (before my little ❤ couldn\u0026#39;t take it) feat. @jaredlander @chendaniely @seanjtaylor @noamross @brookLYNevery1 @dtdusty @DogmaticPrior @wesmckinn #rstats 🗽 pic.twitter.com/pTOXTPTO5b\n\u0026mdash; Mara Averick (@dataandme) April 23, 2018 \n  Apache Arrow \u0026ndash; Cross-language development platform for in-memory data  from Wes McKinney \nR in Minecraft David Smith, Microsoft, @revodavid\n.@revodavid on tricking The Youth™ into learning R via Minecraft 💫 #rstatsnyc pic.twitter.com/yn77S34frl\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nMay the R be with You: Exploring the Star Wars Universe Evelina Gabasova, Alan Turing Institute, @evelgab\n.@evelgab CRUSHING the final talk of the day. Use interpretable datasets!! #rstatsnyc pic.twitter.com/zO05w8kO2r\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 20, 2018 \nPatterns and Drivers of Ice Shelf Melt Alex Boghosian, Columbia\nDay two of #rstatsnyc kicking off with avocado toast, glaciology, and cold brew. Alex Boghosian kicked it off right 💯 pic.twitter.com/3rV06B7VsT\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nCracking into the Meat of R with lobstr: Console Visualisations that Explain How Stuff Works Hadley Wickham, RStudio, @hadleywickham\n.@hadleywickham on the lobstr package, which helps users understand the unique strengths of R. (Not least of which, that R is made to be read by humans). 💯 #rstatnyc pic.twitter.com/FBCzDC1HRx\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nIntroduction to scikit-learn Andreas Mueller, Columbia, @amuellerml\nslides\nLots of good workflow advice in @amuellerml ‘s #rstatsnyc talk. Lots of data processing and selection goes into both supervised and unsupervised ML - cross validate the WHOLE pipeline, not just the final model. pic.twitter.com/u8ABxwdv70\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nHow Long Will I Live? The Statistics Behind Prognosis in Cancer Research Emily Zabor, Memorial Sloan Kettering, @zabormetrics\nslides\nLIVING for @zabormetrics ‘s Epi and biostats realness this morning. god bless statisticians saving people from themselves 💯 #rstatsnyc pic.twitter.com/CBKq2wWLUo\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nAutomated Versus Do-it-Yourself Methods for Causal Inference: Lessons Learned From a Data Analysis Competition Jennifer Hill, New York University\nNerding out SO hard at the back-to-back Epi stats talks. Jennifer Hill with some real talk about causal inference - it CAN be done in observational studies, if done carefully. #rstatsnyc pic.twitter.com/4cS9PepySX\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nTeach the Tidyverse to Beginners David Robinson, DataCamp, @drob\nGreat talk from @drob from @DataCamp, who really genuinely understand and empathizes with the needs and goals of beginners. My favorite takeaway: if you Mr. Miyagi your students or colleagues, they’ll lose interest before they learn karate (/R). #rstatsnyc pic.twitter.com/e6szgb7vf4\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nOpen Source Success: A Decade of R in Finance Jeff Ryan, Citadel, @lemnica\n.@lemnica on building software in community. “Build places that are nice to hang out in,” and the ecosystem will grow. #rstatsnyc pic.twitter.com/dJSjzFCeeX\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nInvestigating User Experience with Natural Language Analysis Stephanie Kim, Algorithmia, @StephLKim\nExcellent talk from @StephLKim about using NLP techniques to analyze user feedback 📖 #rstatsnyc pic.twitter.com/Z35nqgIBSL\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nPhrasing: Communicating Data Science Through Tweets, Gifs, and Classic Misdirection Mara Averick, RStudio, @dataandme\n.@dataandme crushing it with humor, humility, and Pam Poovey. 🔥🔥🔥#rstatsnyc pic.twitter.com/cDzdEozoEj\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nComparing Posteriors: Estimating Practical Differences Between Models Max Kuhn, RStudio, @topepos\n.@topepos on the tidyposterior package to compare performance between models in R with Stan. 10\u0026frasl;10 talk, 10\u0026frasl;10 phrasing pic.twitter.com/HhGoaUHugp\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nThe What-Where-How-Why of GPU computing with R Kelly O’Briant, MapD, @kellrstats\nFinal talk of the day: @kellrstats, making a Suuuper compelling case for incorporating GPU computing into R pipelines. So much data viz power! #rstatsnyc pic.twitter.com/IzZdBFSiSx\n\u0026mdash; Brooke Watson (@brookLYNevery1) April 21, 2018 \nDay 2: ✍️ gets tired, 📝 get weird\u0026hellip; @hadleywickham @drob @lemnica @topepos \u0026amp; @kellrstats oh my!! /* Also, Mara borks Twitter handles 😬 */#rstatsnyc #rstats pic.twitter.com/daoclum3Qj\n\u0026mdash; Mara Averick (@dataandme) April 23, 2018 \nIt\u0026rsquo;s always fun getting autographed books! \nAnother year at #rstatsnyc, another batch of books to get autographed! #rstats #datascience #nycdatamafia pic.twitter.com/i4EoW0OSa1\n\u0026mdash; Dⓐniel Chen (@chendaniely) April 19, 2018 \nHad a great time at #rstatsnyc, thanks @jaredlander! I even won the R Packages book by @hadleywickham - so here\u0026#39;s to all the future R packages coming out of @turinginst 👍 pic.twitter.com/sxSo3AtZby\n\u0026mdash; Evelina Gabasova (@evelgab) April 22, 2018 \n","date":1524355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564879039,"objectID":"5b6316a7fcc49ddc7595b7ae7b06f2c6","permalink":"/2018/04/22/rstatsnyc-as-told-by-brooklynevery1-dataandme-and-autographs/","publishdate":"2018-04-22T00:00:00Z","relpermalink":"/2018/04/22/rstatsnyc-as-told-by-brooklynevery1-dataandme-and-autographs/","section":"post","summary":"EDIT: I\u0026rsquo;ve added the notes from @dataandme and linked to people\u0026rsquo;s twitter and slides (if I found them). This is probably going to be an ongoing process\u0026hellip;\n Another year and another talk at the NYC R Conference. As always, the conference was filled with excellent speakers (I\u0026rsquo;m biased here becuase I was one of them\u0026hellip;), food, and people.\nBrooke Watson (@brookLYNevery1) did a fantastic job illustrating and summarzing all of the talks.","tags":["R","conference","rstatsnyc"],"title":"rstatsnyc as told by @brookLYNevery1, @dataandme, and autographs","type":"post"},{"authors":null,"categories":null,"content":" One of the most annoying things you hear people say when they are working with some common code base is \u0026ldquo;It works on my machine\u0026hellip;\u0026rdquo;. Conversely, one of the more satisfying things is running a script that you are not actively working on and have it run without problems.\nProject Templates are one way to address this problem. The original post about project templates mainly talks about the folder structure but not so much as the rationale behind why things are the way they are. Also the original post used a user-based subfolder structure under src, which caused some problems when we ended up doing code reviews.\nWhy do we even want to use \u0026ldquo;projects\u0026rdquo;? Software Carpentry has a good set of explanations. When dealing with working directories and workspaces within R and RStudio, even RStudio suggests using projects.\nHopefully the links above are convincing enough to use projects and some standardized folder structure within projects. Below are some of the issues we\u0026rsquo;ve encountered while working on various projects, and hopefully we can slowly migrate to a standardization for all projects and analysis moving forward. Some of the problems and solutions are already mentioned from the Software-Carpentry and RStudio links above.\nProblems  Manually setting a working directory in a script.  Reduces the reproducibility by hard-coding a path specific to each user. When sharing code, each person needs to manually edit at least one line within the file to set the working directory. Results and other saved outputs from code are also tied to the working directory  Sharing code within a project by copy/pasting from another script.  Redundant routines in code should always be encapsulated into a function so they can be re-used within a script and shared across scripts. Since duplication of code is minimized, a fix in one location should provide the same fix in other locations. We\u0026rsquo;ve had situations where code was copy/pasted into multiple scripts and multiple bugs were introduced/fixed in multiple places but neither set of duplicated code contained all the bug fixes.  Username subfolders in the src folder  Promoted code duplication and made it hard to find a particular analysis. Originally set up this way to minimize merge conflicts Individuals can quickly find his/her own work Became a problem because instead of sharing code that needed to be reused, people started copy/pasting code into scripts See duplication of code problem from the previous point During the code review process, finding code that did a certain task was difficult since it was organized by who did a task, versus the task itself.  Manually setting working directories while under a version control systems (VCS)  VCS keep track of changes from files. Even if there is a general understanding of manually setting a working directory among collaborators, every time someone changes the working directory to run on their machine, the version control system will report a change in the file. While the changes can be reverted by the VCS, it\u0026rsquo;s an unnecessary step in the work flow, and can lead to unwanted changes in the code if a file was reverted unexpectedly. Or you will end up with the same lines and files being changed over and over, cluttering the log and history.   Solutions Many of these problems can be solved with using RStudio Projects.\n Prevents scripts from setting a working directory (setwd())  The working directory will be the root of the project. Since the starting point for each script is deterministic, we can implement a folder structure within the project in a way where data has a common access point (e.g., ./data/path/to/data) and code that needs to be run will also have a common access point (e.g., ./src/path/to/script.R)  Code and scripts can be easily be referenced from the root working directory of the project  When someone wants to create a script to hold functions, it has a consistent path to be sourced in another R script This would provide a place for common code bases to be shared without copy/pasting into multiple scripts and without having to be put into a proper R package.  The structure of the project template needs to account for multiple people entering and leaving a project at various points during the lifetime of a project  We have a src folder so people knew where to put his/her R scripts needed a way to organize the folder so we don\u0026rsquo;t have 10s or 100s of files in a single directory. We initially settled on using user name sub-folders.  This prevented a lot of potential merge conflicts, since people are more likely to just work in their own directory but it also caused people to copy/paste from other people\u0026rsquo;s folders. It also made it difficult to find out later on how to find a particular piece of analytics. We would have to know who worked on it to find the script, versus looking up the analysis itself.  Thus, we are moving to a naming system not under user names.  This also will prevent copy/pasting of common code bases. One thing that can happen is more merge conflicts (editing the same line of a file at the same time) while working  but even that can be minimized by coordinating efforts.    Reduces the Git history clutter  Version control systems (VCS) keep track of changes within a file. When we have working directories manually being set on a user-by-user basis, each user will have to set his/her own working directory to get the script to run. This requires commenting out the current working directory set in the code, and either replacing or uncommenting out the \u0026lsquo;correct\u0026rsquo; working directory. This will cause the VCS to note there is a change in a file where the user either needs to commit or revert for each person who is running the script. While this may be a \u0026lsquo;harmless\u0026rsquo; change, it clutters the log and history of the file, when bigger changes need to be reverted or bugs need to be tracked down. The project template should also fix this problem since the working directory no longer needs to be set in the file.   Changes to mindset Section taken from feedback from Ian\nGit The major change from using username src subfolders to analysis-based subfolders is how to navigate the initial hierarchies of the folder. While the most technical hurdle to overcome is the potential of increased Git merge conflicts, it can be minimized by coordinating efforts. We have a system that distinguishes code from data, where code is stored on a Git server (e.g., GitLab) and data is stored on an encrypted LUKS volume. This means learning and using Git will have to be part of everyone\u0026rsquo;s toolkit. It\u0026rsquo;s a technical solution to solve a very complex problem, sharing and version controlling files, but it does offer solutions for project management, if you choose to use it.\nCode structure  The most appealing part of using usernames as folders is that it allows users to think of their code however they like, and never need to reconcile that with other people\u0026rsquo;s understanding of project structure. For instance, Alice can organize folders by the work she did on a month to month basis. Bob can organize scripts into data processing/data loading/plots/misc. Chris can structure code based on who asked her to do whatever. Each person has a system that works for them and doesn\u0026rsquo;t have to work for anyone else. But even in this small example, the three code bases are conceptually incompatible. How do you reconcile folders called \u0026lsquo;June2017\u0026rsquo;, \u0026lsquo;get ACS stuff\u0026rsquo;, and \u0026lsquo;For Sallie\u0026rsquo;? In a project-centered folder structure, everyone needs to be on more or less the same page regarding the structure of the project and what goes on in it. Personally I think this is a good thing, and something we should be doing anyway, but it will certainly be a challenge especially if people don\u0026rsquo;t think to think of it. ```\n ","date":1516665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"2a071fd29bdb31102261ce89dd313d1f","permalink":"/2018/01/23/analysis-based-project-templates/","publishdate":"2018-01-23T00:00:00Z","relpermalink":"/2018/01/23/analysis-based-project-templates/","section":"post","summary":"One of the most annoying things you hear people say when they are working with some common code base is \u0026ldquo;It works on my machine\u0026hellip;\u0026rdquo;. Conversely, one of the more satisfying things is running a script that you are not actively working on and have it run without problems.\nProject Templates are one way to address this problem. The original post about project templates mainly talks about the folder structure but not so much as the rationale behind why things are the way they are.","tags":["sdal","data science","dspg","dspg17","programming practices","sdal-post"],"title":"Analysis-Based Project Templates","type":"post"},{"authors":null,"categories":null,"content":" Since I\u0026rsquo;ve joined SDAL, the lab has undergone a few infrastructure related changes, mainly how applications are run on the servers. From what I remember, we started using Virtual Box virtual machines, then moved to LXC Linux containers, and we are now rebuilding our entire infrastructure using Docker containers.\nHow we got here The whole point of using these visualization and container technologies is so we did not want to install anything directly on the server. There are many benefits to this\n If the server goes down, we should be able to redeploy our infrastructure on a backup server If an installation goes wrong, it will not crash the underlying server  If an installation goes wrong, we should be able to swap out or roll back to a working state  This gives us the ability to test out updates before rolling them out on a production server If something happens with one part of the system, it should not affect another  Virtual Box Virtual Machines We (Aaron) started with Virtual Box Virtual Machines, because it\u0026rsquo;s a very logical first step. If you wanted the same thing on your own computer/laptop, you\u0026rsquo;d use a virtual machine. This meant that it was relatively easy to get started because we already had experience with VB.\nOur main reasons for looking into other solutions were\n performance: since we are using these VMs to run data analytics, we needed performant system. The fact we needed to install an entire OS made it clunky size: Since we wanted multiple containers, one for each application, the fact that an entire OS needed to be installed for each application meant that a lot of OS information was replicated and used a large amount of space VBoxManage: since we had to run our VMs in headless mode, the only way to make changes to the VMs was with the VB command line tool, VBoxManage, which is less than ideal in terms of simplicity and documentation.  LXC Containers When looking to move away from VirtualBox, we started with LXC because at the time Docker just started and didn\u0026rsquo;t seem to be developed enough. During the early days of Docker, the documentation was lacking (one might argue that it still is), and the API/interface was cumbersome (How do I get into my container? why can\u0026rsquo;t I just SSH?). We went with LXC since docker was an extension of it. We later also looked into LXD, since it extended LXC with more and better security, group, and permission features.\nLXC got us pretty far, we had a lighter set of applications running on the servers, since each container was based on the host operating system (in our case this was CenOS 7).\nThe main issue was that it was hard to quickly tear down and redeploy containers. Or have container dependencies to build applications.\n support: support for LXC was lacking, mainly a lack of documentation permissions: permissions from the host were not being carried over into the container host os: each container was limited to whatever the host OS was on.  These limitations had us looking at LXD, LXC successor. LXD did not run as root, so each container was not privileged. It also mapped permissions from the host OS, and we had the ability to install an image using Ubuntu, if needed.\nHowever, after a year we transitioned into Docker, and solved the LXC limitations.\nDocker Containers I\u0026rsquo;m sure a lot of the issues from LXC could\u0026rsquo;ve been worked out with enough blood, sweat, and tears, it is linux after all. The popularity of Docker over the year was hard to ignore and worth trying again.\nThe best part about docker is how it forces you to fully document the setup process, using the Dockerfile. This not only meant that we can readily tear down and spin up the container if something goes wrong. Additionally, we can just as easily start up multiple containers running the same application. This is particularly useful when trying to get RStudio to see newly mounted and mapped drives or permission changes. Instead of restarting the image, which requires nobody to be using it, we can spin up a new container and have the people who needed new data partitions or permission to have a temporary new container without disrupting other users.\nWhat\u0026rsquo;s next? We\u0026rsquo;ll probably look into container orchestration tools such as docker-swarm or kubernetes. While we could use the current docker-compose file and function to create multiple RStudio Servers, there\u0026rsquo;s a lot of copying and pasting involved.\nWe\u0026rsquo;d like everyone to have his/her own RStudio instance, so if there is a permission change, we can simply restart the individual user\u0026rsquo;s container instance, rather than finding workarounds in the middle of the day because we can\u0026rsquo;t do a restart. Relying on zero usage on a server (especially in the middle of a work day) to do maintenance is almost never a good idea. Having a orchestrator will allow us to spin up as many instances without conflicting ports and username-appended container names with a single script.\nIt would also be nice to have a single entry page, so users can pick the service they need, rather than remembering port numbers to connect to (since those can change at any given time).\nFinally, on a recent server hiccup, we were having issues around dead containers.\nHere\u0026rsquo;s the error message when we tried to stop and remove the containers:\n$ docker-compose down Stopping adminer ... done Stopping mro ... done Stopping postgresql ... done Stopping dokuwiki ... done Stopping rpkgs ... done Stopping rstudio ... done Stopping shiny ... done Removing adminer ... error Removing mro ... error Removing postgresql ... error Removing dokuwiki ... error Removing rpkgs ... error Removing rstudio ... error Removing shiny ... error ERROR: for postgresql driver \u0026quot;overlay\u0026quot; failed to remove root filesystem for b91b18c144d40801b1782789ffe3a9352509b720b368947cefd9e443d02edf01: remove /home/vms/docker/overlay/014f942314a693d1984155ded44a30f50b27b67fce62f7ed60233028f998379c/merged: device or resource busy ERROR: for dokuwiki driver \u0026quot;overlay\u0026quot; failed to remove root filesystem for f228cc35d050d8654cc1eb4e424feb111a2b826b3a4bd35d42aa4ff38dcc5524: remove /home/vms/docker/overlay/6dc15c5116a0290bc42bdae7fafdcbdcf718951c8ebcbfbc63eb56d18fd07c23/merged: device or resource busy ERROR: for adminer driver \u0026quot;overlay\u0026quot; failed to remove root filesystem for fb7f33ed5a10a843b9855ffa3aa83c0b68ecfeb793a89274e0277add2bc08262: remove /home/vms/docker/overlay/58c311625ca5d6288359489d36c4653107fd710e3700298cf8400cf85d4c4523/merged: device or resource busy ERROR: for rpkgs driver \u0026quot;overlay\u0026quot; failed to remove root filesystem for 14c0516130623640ebace03e89478b17efb2e802232ddc22a524f42e4fcaef02: remove /home/vms/docker/overlay/d083c1fb3046605fe3a9eaa8e19111b00473c38bebe23f222bfd0cfa81802c6f/merged: device or resource busy ERROR: for mro driver \u0026quot;overlay\u0026quot; failed to remove root filesystem for 7c75a0cc6ba4f80bdb47df37d0aeab38681e621a5052507a5cc8da2a4eac332c: remove /home/vms/docker/overlay/cd7e50fe0e95feca8ced590c0ecd373290fa6710656db75286c1ce8900ff09d1/merged: device or resource busy ERROR: for rstudio driver \u0026quot;overlay\u0026quot; failed to remove root filesystem for 8f90d16c8da45c59519203e582917b24a4249fdfa9a35f5043782320d6d6b2d4: remove /home/vms/docker/overlay/408a938c07d53d5b4cf79376a15f1bf3ffc5cc54eb0c1d2f7f95215764e010b5/merged: device or resource busy ERROR: for shiny driver \u0026quot;overlay\u0026quot; failed to remove root filesystem for dc85077a54862940c0a17906b66d7670d1dbb95f5006083871c36c20586483f3: remove /home/vms/docker/overlay/08e22f468c2f5aa0ca4823a2aa5d9cb400167c49aa21a06f3538a9fa943c4dba/merged: device or resource busy Removing network dockerimages_default Removing network dockerimages_dspg  We got around this error by restart the server, which is far from an ideal solution,\nThere was a promising explanation here and here. Here\u0026rsquo;s an excerpt from Anusha Ragunathan\u0026rsquo;s post about what\u0026rsquo;s going on\nDug up some history: daemon flags changed to default \u0026quot;shared\u0026quot; mount propagation in 2aee081 in an effort to help volume mounts to ensure shared. This change made it to 1.12, which is probably why there's more reports since then. Prior to this, the flags were indeed set to \u0026quot;slave\u0026quot; in eb76cb2 to avoid EBUSY errors on container remove. There's a blog post on why this was done. Although it mentions devicemapper, the problem exists across graphdrivers. http://blog.hashbangbash.com/2014/11/docker-devicemapper-fix-for-device-or-resource-busy-ebusy/  Overview: While this is issue not exclusive to devicemapper, the mechanics currently involved in this driver cause it to be affected by this. A couple of the more commons issues seen when using the ‘devicemapper’ storage driver is when trying to stop and/or remove a contianer. In the docker daemon logs, you may see output like: [error] deviceset.go:792 Warning: error waiting for device ac05cffda663a01cbc37879bc146fcd68d0f95b5b141f60da2b64579add1f4ef to close: Timeout while waiting for device ac05cffda663a01cbc37879bc146fcd68d0f95b5b141f60da2b64579add1f4ef to close or more likely: Cannot destroy container ac05cffda663: Driver devicemapper failed to remove root filesystem ac05cffda663a01cbc37879bc146fcd68d0f95b5b141f60da2b64579add1f4ef: Device is Busy [8ad069f7] -job rm(ac05cffda663) = ERR (1) [error] server.go:1207 Handler for DELETE /containers/{name:.*} returned error: Cannot destroy container ac05cffda663: Driver devicemapper failed to remove root filesystem ac05cffda663a01cbc37879bc146fcd68d0f95b5b141f60da2b64579add1f4ef: Device is Busy [error] server.go:110 HTTP Error: statusCode=500 Cannot destroy container ac05cffda663: Driver devicemapper failed to remove root filesystem ac05cffda663a01cbc37879bc146fcd68d0f95b5b141f60da2b64579add1f4ef: Device is Busy Diagnosis: What’s happening behind the scenes is that devicemapper has established a new thin snapshot device to mount then container on. Sometime during the life of that container another PID on the host, unrelated to docker, has likely started and unshared some namespaces from the root namespace, namely the mount namespace (CLONE_NEWNS). In the mounts referenced in this unshared host PID, it includes the thin snapshot device and its mount for the container runtime. When the container goes to stop and unmount, while it may unmount the device from the root namespace, that umount does not propogate to the unshared host PID. When the container is removed, devicemapper attempts to remove the thin snapshot device, but since the unshared host PID includes a reference to the device in its mountinfo, the kernel sees the device as still busy (EBUSY). Despite the fact the mounts of the root mount namespace may no longer show this device and mount.  Maybe these problems can be fixed with a different container orchestration system like docker swarm or kubernetes, who knows.\n","date":1499385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"a5f04f41dcfbfcad88959b3c89af1542","permalink":"/2017/07/07/from-vms-to-lxc-containers-to-docker-containers/","publishdate":"2017-07-07T00:00:00Z","relpermalink":"/2017/07/07/from-vms-to-lxc-containers-to-docker-containers/","section":"post","summary":"Since I\u0026rsquo;ve joined SDAL, the lab has undergone a few infrastructure related changes, mainly how applications are run on the servers. From what I remember, we started using Virtual Box virtual machines, then moved to LXC Linux containers, and we are now rebuilding our entire infrastructure using Docker containers.\nHow we got here The whole point of using these visualization and container technologies is so we did not want to install anything directly on the server.","tags":["infrastructure","dspg","dspg17","containers","sdal","docker","vms","lxc","devops"],"title":"From VMs to LXC Containers to Docker Containers","type":"post"},{"authors":null,"categories":null,"content":" Project templates provide some standardized way to organize files. Our lab uses a template that is based off the Noble 2009 Paper, \u0026ldquo;A Quick Guide to Organizing Computational Biology Projects\u0026rdquo;. I\u0026rsquo;ve created a simple shell script that automatically generates this folder structure here, and there\u0026rsquo;s an rr-init project by the Reproducible Science Curriculum folks.\nThe structure we have in our lab looks like this:\nproject | |- data # raw and primary data, are not changed once created | | | |- project_data # subfolder that links to an encrypted data storage container | | | | | |- original # raw data, will not be altered | | |- working # intermediate datasets from src code | + +- final # datasets used in analysis | |- src / # any programmatic code | |- user1 # user1 assigned to the project | +- user2 # user2 assigned to the project | |- output # all output and results from workflows and analyses | |- figures/ # graphs, likely designated for manuscript figures | |- pictures/ # diagrams, images, and other non-graph graphics | +- analysis/ # generated reports for (e.g. rmarkdown output) | |- README.md # the top level description of content | |- Makefile # Makefile, if applicable |- .gitignore # git ignore file +- project.Rproj # RStudio project  In the main level there are data, src, and output folders. As well as *.Rproj, .gitignore, and potentially a Makefile files.\nThe .Rproj file Since we are primarily an R lab that runs an RStudio Server server, we use the Rproj files organize the various projects. There are a few benefits to this. When using the RProj file, it sets the working directory in RStudio to the location of the Rproj file automatically. This makes the project more reproducible by avoiding the setwd() command in R, and since multiple people work on the same project, referencing other people\u0026rsquo;s source code and data outputs all stem from a common location.\nThe .gitignore file The .gitignore file is there to ignore various outputs from the src code. This includes things like .pdf or .html output from knitr and rmarkdown documents, as well as things in the first level of the data folder. In general, the files and folders in the .gitignore file are things that can be reproduced/regenerated by running code from the src folder.\nThe src folder The src folder contains all the analysis and code for the project. It should only contain the code for the project and not any kind of output from the code, i.e., data, reports, etc.. Since all the projects int he lab are separate git repositories, each person working on the project creates a separate folder with his/her user name (e.g. user1, user2) under the src directory to minimize potential conflicts within the code.\nThe output folder Is there for any type of \u0026lsquo;final\u0026rsquo; non-data output. The useage is ambiguous on purpose, but typically is used for some kind of plot or table that will be used in a final publication or report. the figures, pictures, and analysis subfolders are just placeholders about what could potentially be placed in the folder, users have the freedom to adapt the contents to the project at hand.\nThings in the output folder are, by default, ignored since the they should be able to be re-created with one of the src scripts.\nThe only thing that should not be in the output folder are any datasets. Those should all be under of the the data subfolders described below.\nThe data folder Since the data folder is part of the code repository, (i.e., it comes when you git clone the repository), the contents of the folder are, by default, ignored in the .gitignore file. Additionally, because of data privacy concerns, all of our project data are on separate (encrypted) LUKS (Linux Unified Key Setup) partitions. The data folder contains a shortcut to the relevant encrypted data container. This is one way to prevent data from being checked into the code repository, and potentially leaving the server.\nWithin the encrypted data folder, there are 3 main folders: original, working, and final. The original data are the rawest datasets available. Typically theses are datasets we are given by sponsors, or found online. These datasets, in combination with the code in src, should be able to regenerate any of the datasets in working and final.\nData provenance is the chronology of how data is transformed through the cleaning and analysis phase. It\u0026rsquo;s important for reproducibility/reproducibility, and means that original data should never be altered directly. original data should only be modified by the code in src. Also, because the original dataset is never altered, and bugs or alterations in the code can be fixed without compromising the integrity of the dataset.\nThe working data folder is mainly used for intermediate datasets. For example, when a particular data step takes \u0026ldquo;a long time\u0026rdquo; to run, the output of that datastep can be saved in the working directory and be used in a new R script to resume any additional data cleaning steps.\nThe final data folder is usually used for datasets that have been cleaned and ready for analysis. No dataset is every fully cleaned, you can probably always perform some other data transformation on it, but this folder is mainly reserved for datasets where an analysis, report, or plot is generated from.\nConclusion Project templates provide a standard for one to share code with other people. With a standardized folder structures, a new member of a project can easily start to understand where the data, code, documentation, and results are.\nIt also makes code reproducible/replicable and provides a common location (working directory) to run the code.\nFinally, because there are specifically designated areas for various components of a project, things become easier to find because everything is not simply placed in the same folder for \u0026ldquo;convenience\u0026rdquo;.\n","date":1496102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"4a223136a519cdbb81a996ea250a69a7","permalink":"/2017/05/30/project-templates/","publishdate":"2017-05-30T00:00:00Z","relpermalink":"/2017/05/30/project-templates/","section":"post","summary":"Project templates provide some standardized way to organize files. Our lab uses a template that is based off the Noble 2009 Paper, \u0026ldquo;A Quick Guide to Organizing Computational Biology Projects\u0026rdquo;. I\u0026rsquo;ve created a simple shell script that automatically generates this folder structure here, and there\u0026rsquo;s an rr-init project by the Reproducible Science Curriculum folks.\nThe structure we have in our lab looks like this:\nproject | |- data # raw and primary data, are not changed once created | | | |- project_data # subfolder that links to an encrypted data storage container | | | | | |- original # raw data, will not be altered | | |- working # intermediate datasets from src code | + +- final # datasets used in analysis | |- src / # any programmatic code | |- user1 # user1 assigned to the project | +- user2 # user2 assigned to the project | |- output # all output and results from workflows and analyses | |- figures/ # graphs, likely designated for manuscript figures | |- pictures/ # diagrams, images, and other non-graph graphics | +- analysis/ # generated reports for (e.","tags":["infrastructure","dspg","dspg17","project_template","sdal"],"title":"Project Templates","type":"post"},{"authors":null,"categories":null,"content":"As a PhD student who already has a Master\u0026rsquo;s degree, it\u0026rsquo;s safe to say that I\u0026rsquo;ve been in school for a long time. One of the things in higher education that I started to dislike over the years are the ways professors assess students in the classroom.\nAs a graduate student, more specifically, a PhD student, a lot of my time should be spent on research. However, when classes are structured the same way as they were in my undergraduate days, memorization is the primary form of assessment, then the point of why I need to take classes as a graduate student seem missed.\nSpecifically, I feel like almost any form of assessment in a PhD program should be open-everything. Mainly because, in practice the world of knowledge is always available as a reference manual that can be consulted. Having to brute memorize reference tables (like statistical distributions), almost seem like wasted effort. More important or frequently used bits of information will naturally be remembered as they are used, but to blindly memorize facts do not assess whether or not a student actually knows the take-away skills from a class.\nClass projects and homework assignments seem like a good way to assess students. At the very least, they have \u0026lsquo;correct\u0026rsquo; answers to a set of problems the course aims to teach, which becomes a set of reference material for the student in the future. I have bounded books of old class notes and homeworks that I constantly reference when I am working on statistical analysis.\n","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"8e7e8da1f61d92d5e7d6a00566f87225","permalink":"/2017/05/01/changes-in-higher-education/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/2017/05/01/changes-in-higher-education/","section":"post","summary":"As a PhD student who already has a Master\u0026rsquo;s degree, it\u0026rsquo;s safe to say that I\u0026rsquo;ve been in school for a long time. One of the things in higher education that I started to dislike over the years are the ways professors assess students in the classroom.\nAs a graduate student, more specifically, a PhD student, a lot of my time should be spent on research. However, when classes are structured the same way as they were in my undergraduate days, memorization is the primary form of assessment, then the point of why I need to take classes as a graduate student seem missed.","tags":["teaching","higher ed","pfps17"],"title":"Changes in Higher Education","type":"post"},{"authors":null,"categories":null,"content":"Just got back from the 3rd annual NYC R Conference this past weekend. I have been honored to be one of the few speakers for the 3rd year in a row. This year\u0026rsquo;s talk, \u0026ldquo;So You Want to be a Data Scientist\u0026rdquo; gave a whirlwind tour of the tools and skills needed to be a Data Scientist. I conveyed all this information in 56 slides and did it in 20 minutes.\nThe day before the conference, I also ended up presenting my current work on behavior diffusion in social netowrks and a little of my other work to the NewYork-Presbyterian Hospital’s Value Institute. This was probably the more nerve wracking things I\u0026rsquo;ve had to do recently, presenting my research to a few extremely talented and smart PhDs doing health analytics for New York Presbyterian Hospital. But, we had a lot of good meaningful discussion during the talk, that went overtime and we were kicked out of the room. That has to be a good sign, right?\n","date":1492992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"cb508a024324928127481b9ae8e158de","permalink":"/2017/04/24/nyc-r-conference/","publishdate":"2017-04-24T00:00:00Z","relpermalink":"/2017/04/24/nyc-r-conference/","section":"post","summary":"Just got back from the 3rd annual NYC R Conference this past weekend. I have been honored to be one of the few speakers for the 3rd year in a row. This year\u0026rsquo;s talk, \u0026ldquo;So You Want to be a Data Scientist\u0026rdquo; gave a whirlwind tour of the tools and skills needed to be a Data Scientist. I conveyed all this information in 56 slides and did it in 20 minutes.","tags":["teaching","higher ed","pfps17-journal"],"title":"NYC R Conference","type":"post"},{"authors":null,"categories":null,"content":"As the semester comes to an end, preparing for my lab\u0026rsquo;s Data Science for the Public Good Program begins. I\u0026rsquo;ve started a GitHub group to dump the various components we will be using during the summer.\nAaron Schroeder and I will be the main trainers for the students this summer. It\u0026rsquo;s our job to teach the students the basic tools needed to be functional in the lab. We put our initial syllabus in the workshop repository.\nAs soon as the semester ends I should have about 2 weeks to finalize all the things to be taught. Ideally, this workshop repository will be come the basis for a university course in the future. I know the university library is currently getting numerous people Software-Carpentry Instructors so they can be more independent in teaching the Software Carpentry materials. Hopefully by the end of the summer, the DSPG syllabus will be worked out to a point that I can present and show the newly minted SWC instructors at VT, for them to adapt for their own needs.\nIn the meantime, There are a lot of readings and things I need to familiarize myself with before the summer program starts\n tidyverse R for Data Science bookdown purrr forcats tidyr  There\u0026rsquo;s probably more things I\u0026rsquo;m forgetting to list, but that\u0026rsquo;s more than enough for now.\nNot to mention I need to prepare my poster for Society for Epidemiologic Research (SER), and pretty much finish up my book that will be the basis for my SciPy 2017 Tutorial\n","date":1492992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"c6441d7dc582c267ddd1a96ffa7ab174","permalink":"/2017/04/24/preparing-for-the-summer/","publishdate":"2017-04-24T00:00:00Z","relpermalink":"/2017/04/24/preparing-for-the-summer/","section":"post","summary":"As the semester comes to an end, preparing for my lab\u0026rsquo;s Data Science for the Public Good Program begins. I\u0026rsquo;ve started a GitHub group to dump the various components we will be using during the summer.\nAaron Schroeder and I will be the main trainers for the students this summer. It\u0026rsquo;s our job to teach the students the basic tools needed to be functional in the lab. We put our initial syllabus in the workshop repository.","tags":["teaching","higher ed","pfps17"],"title":"Preparing for the Summer","type":"post"},{"authors":null,"categories":null,"content":"My lab has extended me an opportunity to be a research scientist and helping out our current senior data scientist with the daily analytics and IT support the lab needs. It\u0026rsquo;s a very enticing opportunity, but I need to stop and think about my options\nI\u0026rsquo;d get paid more! What kind of graduate student would not want that? But I would really need to consider and clarify with my advisor and rest of my lab how this can fit in with my PhD. I don\u0026rsquo;t want to be doing my PhD longer than I have to, and ideally I would be put on a series of projects that can be used for my final dissertation topic. However, if I am a staff member of the lab, I will have less freedom of what my topic would be. Of course, if I spent the after hours working on my dissertation, I could do whatever I wanted, but that might not be a feasible solution.\nThe job will give me financial stability, but currently as a PhD student, I have a lot of academic freedom that I might loose. Since most of my work is computational, it can easily be done remotely.\nLuckily my advisor will becoming to down in a few weeks. We\u0026rsquo;ll discuss this further then.\n","date":1492387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"d6d7638cb6e24e9298b7eec785d7f101","permalink":"/2017/04/17/working-while-pursuing-a-phd/","publishdate":"2017-04-17T00:00:00Z","relpermalink":"/2017/04/17/working-while-pursuing-a-phd/","section":"post","summary":"My lab has extended me an opportunity to be a research scientist and helping out our current senior data scientist with the daily analytics and IT support the lab needs. It\u0026rsquo;s a very enticing opportunity, but I need to stop and think about my options\nI\u0026rsquo;d get paid more! What kind of graduate student would not want that? But I would really need to consider and clarify with my advisor and rest of my lab how this can fit in with my PhD.","tags":["teaching","higher ed","pfps17-journal"],"title":"Working While Pursuing a PhD","type":"post"},{"authors":null,"categories":null,"content":"As people start sharing their educational experiences from around the world, I realized how lucky I am to have been educated in New York City, and how much less the United States focuses on education when compared to many other countries around the world.\n","date":1491782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"2657f38447c66df872825f9b0e4fab61","permalink":"/2017/04/10/education-in-the-united-states/","publishdate":"2017-04-10T00:00:00Z","relpermalink":"/2017/04/10/education-in-the-united-states/","section":"post","summary":"As people start sharing their educational experiences from around the world, I realized how lucky I am to have been educated in New York City, and how much less the United States focuses on education when compared to many other countries around the world.","tags":["teaching","higher ed","pfps17-journal"],"title":"Education in the United States","type":"post"},{"authors":null,"categories":null,"content":"\u0026ldquo;Open\u0026rdquo; has played in important role in my life the last few years. It all began when I was an attendee at a Software-Carpentry workshop back in 2013. Before then, I only knew about Open Access and Open Source, but wasn\u0026rsquo;t active in any Open community.\nThis week is Open Data Week at Virginia Tech, and it begins with an \u0026ldquo;Open Research/Open Data Forum [on] Transparency, Sharing, and Reproducibility in Scholarship\u0026rdquo;, which I was honored to be apart of.\nOpen Access typically refers to providing research publications free from restrictions on access. In academic publishing, this is mainly aimed towards paywalls, where an individual needs to pay to see a published paper.\nThe debate around Open Access is highly debated, mainly because there is a lot of money in the scholarly publication business. I\u0026rsquo;ve had the mindset that Open Access or not, it is just a business funding model. The other components of Open (data, source, education) are better and faster ways of disseminate information to the masses without a fee.\nI am honored to be one of the panelists on today\u0026rsquo;s Open Data Forum:\n Daniel Chen (Ph.D. candidate in Genetics, Bioinformatics, and Computational Biology) Karen DePauw (Vice President and Dean for Graduate Education) Sally Morton (Dean, College of Science) Jon Petters (Data Management Consultant, University Libraries) David Radcliffe (Professor, English, College of Liberal Arts and Human Sciences) Laura Sands (Professor of Human Development, Center for Gerontology)  \u0026lsquo;Daunting\u0026rsquo; might not be the right word, but that\u0026rsquo;s quite the line-up of speakers + random graduate student. Someone at the libraries thought it was a good idea, and has faith in me, so that\u0026rsquo;s a plus.\nThe question presented to the panel:\n “One of the main arguments in favor of open research and open data is that the collective benefits to the research system far outweigh the drawbacks. Such benefits include helping to address the problem of reproducibility and opening up new research pathways. Do you agree with this argument? And what are the principal barriers you see in moving us towards this system?”\n It\u0026rsquo;s an interesting question, mainly because it seems like we are preaching to the choir, but one way to see the other side of the argument is looking at the current system of incentives. Currently, the way academic incentives are structured for promotion are not conducive for open collaboration and open data. Mainly because by freely diving out data, one may loose the ability for a paper authorship. Granted there are counter arguments for this, but fear is a powerful motivator, and when productivity is measured by number of publications, potentially losing publications is enough of a detractor for many to not pursuit Openness.\n","date":1491782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"5251e149ef3b208ed838f84b70d7be3b","permalink":"/2017/04/10/open-access/","publishdate":"2017-04-10T00:00:00Z","relpermalink":"/2017/04/10/open-access/","section":"post","summary":"\u0026ldquo;Open\u0026rdquo; has played in important role in my life the last few years. It all began when I was an attendee at a Software-Carpentry workshop back in 2013. Before then, I only knew about Open Access and Open Source, but wasn\u0026rsquo;t active in any Open community.\nThis week is Open Data Week at Virginia Tech, and it begins with an \u0026ldquo;Open Research/Open Data Forum [on] Transparency, Sharing, and Reproducibility in Scholarship\u0026rdquo;, which I was honored to be apart of.","tags":["teaching","higher ed","pfps17"],"title":"Open Access","type":"post"},{"authors":null,"categories":null,"content":"How faculty (higher education) are using and/or reacting to social media, MOOCs, and/or other \u0026ldquo;disruptive\u0026rdquo; technologies.\nOne of my favorite talks is the one given by Greg Wilson at Scipy 2014 about the lessons learned from Software-Carpentry. Software-Carpentry aims to teach the Best Practices for Scientific Computing.\nScientific computing is a skill lacking in education, especially in higher education where research is being performed. What is lacking in scientific computing education is the lack of courses for the researcher, rather than the computer scientist. I think this is why MOOCs came to existence, people found the current system of teaching courses too restrictive so they went outside the university and created the course they wanted. For example, trying to get a Software-Carpentry workshop as a university course is difficult because different departments around the university feel like its their jurisdiction to create the course (I\u0026rsquo;m looking at you computer science). However, faculty are already heavily burdened with trying to fund themselves with grants and performing other responsibilities as faculty, that it\u0026rsquo;s hard to create a generic multi-disciplinary course on scientific research. Also, assessing students is tricky in the software-carpentry context because of how many hours a class can meet per week. SWC works because it\u0026rsquo;s a very condensed 2 day workshop aimed to teach all the basics in Bash, Git, Python or R, and SQL. To formulate and re-arrange the workshop into a weekly course means many skills will not be integrated until the very end. In which case you risk loosing students. MOOCs are just a natural evolution to address the limitations in a university setting.\nMOOCs have their own problems, mainly attrition. Only a small fraction of the students who begin a MOOC complete it. I am guilty of starting courses and not finishing the course, or only hand-picking a few topics relevant to myself. But, I am happy to know the content is there for when I need to go back to it one day.\nI think many of the backlash towards MOOCs are the same criticisms regarding open eduction and open textbooks: how can something \u0026lsquo;free\u0026rsquo; be good? My response:\n How many teachers begin teaching Calculus I every September when the new school year begins? How many of those teachers are writing his/her own curriculum? How many of those teachers lean the same mistakes on their first year to fix on the next iteration? How many hours of resources were wasted for all the teachers to learn the same mistake? What if all that knowledge was already compiled into an open curriculum to be refined?  There are companies that teach MOOCs. DataCamp is an example. They have a subscription service where experts in a field aim to teach a programming concept. This allows companies to purchase bulk subscriptions for their employees to give them targeted training without having to come up with their own in-house resources.\nMaybe if we have people pay a dollar for a MOOC, they will feel obligated to finish the course?\n","date":1491177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"359ecdd65f97b4577ed69c0a5bdc1e13","permalink":"/2017/04/03/tech-innovation-in-higher-ed/","publishdate":"2017-04-03T00:00:00Z","relpermalink":"/2017/04/03/tech-innovation-in-higher-ed/","section":"post","summary":"How faculty (higher education) are using and/or reacting to social media, MOOCs, and/or other \u0026ldquo;disruptive\u0026rdquo; technologies.\nOne of my favorite talks is the one given by Greg Wilson at Scipy 2014 about the lessons learned from Software-Carpentry. Software-Carpentry aims to teach the Best Practices for Scientific Computing.\nScientific computing is a skill lacking in education, especially in higher education where research is being performed. What is lacking in scientific computing education is the lack of courses for the researcher, rather than the computer scientist.","tags":["teaching","higher ed","pfps17"],"title":"Tech \u0026 Innovation in Higher Ed","type":"post"},{"authors":null,"categories":null,"content":"Coming from CUNY Hunter College, I never really had the typical \u0026lsquo;college experience\u0026rsquo;. Going to college for me was almost no different than going to high school since so many people were commuters; there was no campus, especially when compared to Virginia Tech.\nI snowboard. After not going during the 2 years of my master\u0026rsquo;s, I decided the first thing I did when I started Tech was to find the ski/snowboarding club and go on a big winter trip. So I joined VT Snow my first year at VT, and I got go fly out to Colorado for the first time ever to go Snowboarding. I was the only graduate student who was in the club, and it was great. I met a bunch of new friends who all knew how to snowboard and have fun.\nI also SCUBA dive. My second year, I ended up getting involved with the SCUBA club (SCVT) because my family had a winter break trip planned out to Australia. I ended up getting my SDI Advanced Adventure Diver last semester to prepare for the trip to see the Great Barrier Reefs. This semester I\u0026rsquo;m planning to finish my Rescue Diver, and hope to start a Dive Master course over the summer. Maybe by the time I finish my PhD I\u0026rsquo;ll also be a SCUBA instructor.\nIt\u0026rsquo;s important to have friends outside of your discipline, especially in a PhD program. As isolating as life can be, you definitely do not want to be constantly reminded about the work you know you should be doing when you\u0026rsquo;re giving yourself a well needed mental break.\nThe fact that these sports require so much mental focus, allows me to just forget about everything else. Plus the views on top of a mountain really makes you feel like you\u0026rsquo;re on top of the world, and the calm weightless feeling while diving is meditative.\n","date":1491177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"3233235c103db2a6a31a6e44566bea3c","permalink":"/2017/04/03/the-people-that-keep-me-sane/","publishdate":"2017-04-03T00:00:00Z","relpermalink":"/2017/04/03/the-people-that-keep-me-sane/","section":"post","summary":"Coming from CUNY Hunter College, I never really had the typical \u0026lsquo;college experience\u0026rsquo;. Going to college for me was almost no different than going to high school since so many people were commuters; there was no campus, especially when compared to Virginia Tech.\nI snowboard. After not going during the 2 years of my master\u0026rsquo;s, I decided the first thing I did when I started Tech was to find the ski/snowboarding club and go on a big winter trip.","tags":["teaching","higher ed","pfps17-journal"],"title":"The People that Keep Me Sane","type":"post"},{"authors":null,"categories":null,"content":" My previous blog post summarized the main points of the Watts 2002 paper. At the very end of the post I mentioned ways we can extend the Watts model with neural-networks. Here I outline the steps of the neural-network simulation.\nGenerally the model goes though an initiation step, a simulation step, and a cleanup step. In the initiation step, the agents are created and connected in a network. Then selected agents are seeded with a value before continuing onto the simulation step. In the simulation step, \u0026lsquo;steps\u0026rsquo; for each time tick are performed. This includes picking agents to be updated, making the necessary calculations for an update, and updating agents in a simultaneous or sequential manner. Agent states in teach time tick is saved to be written to a (CSV) file. The cleanup step is mainly there when programming the model. It saves any unwritten states to the file, and any other related tasks of that nature.\nMANN The neural networks used in MANN use the \u0026ldquo;the light, efficient network simulator\u0026rdquo; (LENS). I created a (inefficient) python wrapper around LENS, pylens. This mainly stems from the fact my advisor used LENS for his work in the past, and interface with it via the command line with input files. The inefficient stems from the fact that I am having python write our the necessary input files for the neural network, and making command-line subprocess calls to LENS.\nInitiation $n$ agents are created for the model. When an agent is created, a feed-forward neural network with randomized weights to represent its state. A prototype that will be used to train the neural network is assigned to the agent.\nThe agent\u0026rsquo;s will all be connected to a network. For example in the networkx list of graph generators, the [fast_gnp_random_graph][5] takes the number of agents, $n$, and a probability, $p$, of an edge being created.\nTo train the neural network, it needs a set of example files to train. The example file consists of $k$ (50) examples. Each example is a randomized form of the prototype. The amount of randomization is determined by parameter $\\delta$. To incorporate heterogeneity in the model, we also mutate the prototype that is used to create the example files. This mutation rate is designated as $\\epsilon$.\nHow well the agent gets trained is parameterized with the criterion variable. It is analogous to the amount of error the neural network can make before it finishes training. That is, the smaller the criterion, the smaller the error, the more accurate it can represent the training set. All agents stop training after 1000 epochs (training cycles for the neural network).\nOnce the neural network is trained, its weights are output to a file to be loaded during the simulation step.\nAll agents begin with a state of 0. That is the first layer of the neural network all have values of 0. We then select agents to be seeded. The seeded agent is presented (via an example file) the non-$\\epsilon$-mutated prototype. We cycle this value through the neural network with one epoch, and the output of the neural network is assigned to the agent\u0026rsquo;s state.\nWe then begin the simulation part of the model.\nSimulation The simulation runs in steps. These steps designate the amount of time ticks the simulation runs for. The same process occurs during each time tick until the simulation ends.\nAll agents will be randomly selected (without replacement) to undergo an update process. The update process looks at all the agent\u0026rsquo;s neighbors with a directed edge coming into the selected agent (i.e., neighbors or predecessors), and randomly selects one of the agents to be the influencer.\nThe influencer writes our its current state as an example file. This example file is used by the agent\u0026rsquo;s neural network and the output is assigned as the agent\u0026rsquo;s new state.\nIn a sequential update process, the agent\u0026rsquo;s new state is assigned as described above. However, during simultaneous updating, the new state will be stored into a temporary state value, so that if the agent is used as an influencer to another agent, the original agent state value will be written out to the example file. In simultaneous updating, only after all agents have undergone the update process will they assign the temporary state as their new state.\nThe agent states are saved to a file at the end of the initiation step and the end of each simulation time tick.\n","date":1490832600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"11f13946754f78fb33d00898c437fd1e","permalink":"/2017/03/30/a-simple-neural-network-model/","publishdate":"2017-03-30T00:10:00Z","relpermalink":"/2017/03/30/a-simple-neural-network-model/","section":"post","summary":"My previous blog post summarized the main points of the Watts 2002 paper. At the very end of the post I mentioned ways we can extend the Watts model with neural-networks. Here I outline the steps of the neural-network simulation.\nGenerally the model goes though an initiation step, a simulation step, and a cleanup step. In the initiation step, the agents are created and connected in a network. Then selected agents are seeded with a value before continuing onto the simulation step.","tags":["research","complexity science","network science","notes","mann","watts model"],"title":"A 'Simple' Neural-Network Model","type":"post"},{"authors":null,"categories":null,"content":"Software-Carpentry played an integral part of who I am today. I am always trying to learn and follow best practices in the context of scientific programming, which I think is a neglected area in research.\nThe fundamental problem is the incentive structure in academia, where productivity is measured by the number of papers published. The downside of tis pressure is that quality of analytics and code will suffer to get the results for the paper.\nThose of us who are doing research and realized how much coding is needed for analytics see programming as a means to get results. Teaching yourself a technical skill is extremely difficult without mentorship. It\u0026rsquo;s very easy to learn and pick up bad habits.\nI\u0026rsquo;ve seen and heard multiple times that the code is written in a particular way solely because it \u0026lsquo;works\u0026rsquo;, without considering that it can potentially have bugs in it and might need to be patched. It\u0026rsquo;s extremely frustrating when I try to provide guidance and try to teach a skill, but am confronted with \u0026ldquo;No. I don\u0026rsquo;t have time for that\u0026rdquo;.\nIt only makes me wonder how much this happens in scholarly research, and can also explain why science has a reproducibility problem.\n","date":1490572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"08d1de6c0f519889ad77a32729a29c09","permalink":"/2017/03/27/scientific-programming/","publishdate":"2017-03-27T00:00:00Z","relpermalink":"/2017/03/27/scientific-programming/","section":"post","summary":"Software-Carpentry played an integral part of who I am today. I am always trying to learn and follow best practices in the context of scientific programming, which I think is a neglected area in research.\nThe fundamental problem is the incentive structure in academia, where productivity is measured by the number of papers published. The downside of tis pressure is that quality of analytics and code will suffer to get the results for the paper.","tags":["teaching","higher ed","pfps17"],"title":"Scientific Programming","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ve been asked a few times about whether one should attend graduate school. I usually follow up by asking whether they are planning to do a Master\u0026rsquo;s or a Doctorate.\nIf it\u0026rsquo;s a master\u0026rsquo;s, sure, why not. Master\u0026rsquo;s programs are typically no longer than 2 years and very well structured. You come in with a cohort of students, and everyone struggles together and becomes friends.\nPhD programs are much more variable and different. First, the cohort of students are much smaller. Second, unless there is Master\u0026rsquo;s along the way, PhD programs will be unique to each student. Meaning you\u0026rsquo;ll struggle on your own. It\u0026rsquo;s important to consider your current support network when thinking about PhD programs. I cannot stress this enough. It\u0026rsquo;s a stressful, lonely, and isolating world out there, and you really need to make sure you have a solid support network to help you though the ups and downs. There is an awful cost to getting a PhD, and there\u0026rsquo;s a reason why mental health disorders are common among PhD students\n","date":1490572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"3c8e769c65d11a116531c15a5990d59b","permalink":"/2017/03/27/should-you-attend-grad-school/","publishdate":"2017-03-27T00:00:00Z","relpermalink":"/2017/03/27/should-you-attend-grad-school/","section":"post","summary":"I\u0026rsquo;ve been asked a few times about whether one should attend graduate school. I usually follow up by asking whether they are planning to do a Master\u0026rsquo;s or a Doctorate.\nIf it\u0026rsquo;s a master\u0026rsquo;s, sure, why not. Master\u0026rsquo;s programs are typically no longer than 2 years and very well structured. You come in with a cohort of students, and everyone struggles together and becomes friends.\nPhD programs are much more variable and different.","tags":["teaching","higher ed","pfps17-journal"],"title":"Should You Attend Grad School?","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m from New York City. Born and raised in Queens, and went to school in Manhattan for high school, college, and masters. So, coming down to rural Blacksburg, VA has been a big change in scenery.\nBlacksburg is a beautiful town. There are many aspects of the area that I love: clean air, quiet, and plenty of outdoor activities. As someone pursuing a PhD, I am essentially a professional student, so the lack of distractions is much appreciated.\nI do love the fact that many outdoor activities are so easily accessible in the area. Hiking trails are as close as a 15 minute drive away, The New River and Blacksburg Quarry for SCUBA diving are within 15 minutes away as well.\nThe biggest things I miss about NYC is the food and pizza. There is so much good ethnic cuisine that Blacksburg does not have. The way around it is to try and cook it yourself.\nI had subscribed to Blue Apron last semester, and it really kickstarted cooking meals at home versus buying and ordering food. The 30-60 minutes to cook and eat is a nice distraction from the daily stress from school.\n","date":1489968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"72cc0b89ff4c8c59ca0ed6ae67f7b211","permalink":"/2017/03/20/from-nyc-to-blacksburg-va/","publishdate":"2017-03-20T00:00:00Z","relpermalink":"/2017/03/20/from-nyc-to-blacksburg-va/","section":"post","summary":"I\u0026rsquo;m from New York City. Born and raised in Queens, and went to school in Manhattan for high school, college, and masters. So, coming down to rural Blacksburg, VA has been a big change in scenery.\nBlacksburg is a beautiful town. There are many aspects of the area that I love: clean air, quiet, and plenty of outdoor activities. As someone pursuing a PhD, I am essentially a professional student, so the lack of distractions is much appreciated.","tags":["teaching","higher ed","pfps17-journal"],"title":"From NYC to Blacksburg, VA","type":"post"},{"authors":null,"categories":null,"content":"How did I get to where I am today.\nThis is my story.\nEarly I've always been surrounded by computers. My parents both studied Computer Science, and my dad holds a Master's in Computer Science and works as a software developer. Growing up, all my computers were company hand-me-downs, but it allowed me to be lucky enough to always have a computer in the house. I liked to tinker, not as a programmer, or hacker, but more of a user. As a child, one of the first things I'd do when I open a program would to find the preferences tab of a program, and learn what settings were available, and learn how to use a piece of software. I competed in the ThinkQuest Internet Challenge in the 5th grade, and co-authored an award winning website on Epilepsy. High School I was fortunate to attend Stuyvesant High School, a specialized math and science high school in NYC. One semester of programming was a mandatory for all sophomores. Looking back, I'm amazed the amount of material covered in a semester of school: Netlogo (where I co-wrote a BlackJack game), Scheme (where I wrote a visual calculator that worked with negative values, e.g., xx - xxx = -x), and Python (where I wrote a strategy for Prisoner's Dilemma). College My undergraduate years was focused on getting into medical school. I ended up graduating with a major in Psychology with a concentration in Behavioral Neuroscience, and minors in Biology and Computer Science. My high school experience in computer science was a bit traumatic, and prevented me from programming courses until my final couple of years in college. Bioinformatics was a major I was highly interested in, but the math and computer science requirements scared me into Psychology. I ended up in Behavioral Neuroscience because it was a small and new program mainly aimed towards the Honors Students. Still focused on getting into medical school, this was the best choice at the time since it gave me wet-lab experience and the chance to work closely with professors for letters of recommendation. During all of this I worked with Dr. Samuel Sigal in Hepatology at Wiel Cornell and NYU. I was on the clinical trials coordination team and got to interact with patients undergoing various Phase III clinical trials to cure Hepatitis C. Masters I got my Master's in Public Health because of the clinical research skills it would give me. It allowed me to build on the basic statistics knowledge I had from college, and learn the epidemiology and biostatistics skills for clinical and observational study design and regression analysis. I was lucky to be in the Department of Epidemiology when Sandro Galea was Chair of the department. Under his guide, the department had a big push to study complex systems approaches to public health. Here I got to work with Kerry Keyes, Magna Certa, and Melissa Tracy in a R01 studying injury from alcohol consumption in NYC. I also met Mark Orr, who became my Master's Thesis Advisor on measuring and simulating how behaviors spread in social networks. I learned to love teaching when I was a Quantitative Methods (Epidemiology I and Biostat I) Teaching Assistant for Dana March. This let me to join Software Carpentry when I after a workshop led by Justin Ely and David Wade Farley. I attended a Software Carpentry workshop when I took Introduction to Data Science during my Masters. The course was taught by Rachael Shutt, Kayur Patel, and Jared Lander. I learned R, and it changed my life. What started out as a Master's in Public Health to gain clinical research skills before going to medical school quickly evolved into Data Science where data analytics applied to health became my new interest. Now I\u0026rsquo;m currently at Virginia Tech in the Genetics, Bioinformatics, and Computational Biology program. My lab, the Social and Decision Analytics Laboratory, is in Arlington, VA, and I occasionally have small consulting and side projects to help pay the bills and put guacamole on my Chipotle.\n","date":1489968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"e903a859542c85f379db83c2fdfec84d","permalink":"/2017/03/20/my-story/","publishdate":"2017-03-20T00:00:00Z","relpermalink":"/2017/03/20/my-story/","section":"post","summary":"How did I get to where I am today.\nThis is my story.\nEarly I've always been surrounded by computers. My parents both studied Computer Science, and my dad holds a Master's in Computer Science and works as a software developer. Growing up, all my computers were company hand-me-downs, but it allowed me to be lucky enough to always have a computer in the house. I liked to tinker, not as a programmer, or hacker, but more of a user.","tags":["teaching","higher ed","pfps17"],"title":"My Story","type":"post"},{"authors":null,"categories":null,"content":"I am a PhD Student, not a candidate\u0026hellip; yet.\n","date":1489363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"632074577cc3d1e08ae63c7c66cb80ff","permalink":"/2017/03/13/what-you-call-yourself-is-important/","publishdate":"2017-03-13T00:00:00Z","relpermalink":"/2017/03/13/what-you-call-yourself-is-important/","section":"post","summary":"I am a PhD Student, not a candidate\u0026hellip; yet.","tags":["teaching","higher ed","pfps17-journal"],"title":"What You Call Yourself is Important","type":"post"},{"authors":null,"categories":null,"content":" The Office of Research Integrity (ORI) oversees and directs Public Health Service (PHS) research integrity activities on behalf of the Secretary of Health and Human Services with the exception of the regulatory research integrity activities of the Food and Drug Administration.\n There is a location on the ORI website on \u0026ldquo;Misconduct Case Summaries\u0026ldquo;. There are currently 7 cases for 2016:\n Case Summary: Cullinane, Andrew R. Case Summary: D\u0026rsquo;Souza, Karen M. Case Summary: Forbes, Meredyth M. Case Summary: Li, Zhiyu Case Summary: Malhotra, Ricky Case Summary: Pastorino, John G. Case Summary: Walker, Kenneth  All the cases involved \u0026ldquo;ORI [finding that the] Respondent engaged in research misconduct by falsifying and/or fabricating data\u0026rdquo;. This is a symptom of the problems in academic publications:\n Only \u0026ldquo;Significant\u0026rdquo; results get published. Provoking results get published in \u0026ldquo;better\u0026rdquo; journals. Academic productivity is measured in number of publications.  It creates a feedback loop with perverse intensives. In the worst case scenario, self-governance in the form of peer review is also inadequate. The story of Yoshitaka Fujii, comes to mind in this scenario. It\u0026rsquo;s easy for me to criticize the system, and I can\u0026rsquo;t say I have a particularly good solution, either.\nWhat is interesting about the cases is the fact that most of them are related to medicine. This can profound consequences on how the public will perceive new medical treatments. As far as consequences go, all the cases ended up in a 3 year probation for the offending member, and can no longer serve as an advisor. Additionally, all papers related needed to be retracted or corrected.\n","date":1489276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"db3be8cea930bf02260e4cdb0a256fff","permalink":"/2017/03/12/research-ethics/","publishdate":"2017-03-12T00:00:00Z","relpermalink":"/2017/03/12/research-ethics/","section":"post","summary":"The Office of Research Integrity (ORI) oversees and directs Public Health Service (PHS) research integrity activities on behalf of the Secretary of Health and Human Services with the exception of the regulatory research integrity activities of the Food and Drug Administration.\n There is a location on the ORI website on \u0026ldquo;Misconduct Case Summaries\u0026ldquo;. There are currently 7 cases for 2016:\n Case Summary: Cullinane, Andrew R. Case Summary: D\u0026rsquo;Souza, Karen M.","tags":["teaching","higher ed","pfps17"],"title":"Research Ethics","type":"post"},{"authors":null,"categories":null,"content":"Susan Ambrose\u0026rsquo;s book \u0026ldquo;How Learning Works\u0026rdquo; is probably one of the best investments for people who teach. The next best (time) investment is reading Greg Wilson\u0026rsquo;s paper on \u0026ldquo;Software Carpentry: lessons learned\u0026rdquo;.\nI\u0026rsquo;ve watched Greg Wilson\u0026rsquo;s corresponding PyCon 2014 and SciPy 2014 talk numerous times, and is well worth the listen.\n","date":1488758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"fb151b7973d41566f99eb6b6605ebafd","permalink":"/2017/03/06/how-learning-works/","publishdate":"2017-03-06T00:00:00Z","relpermalink":"/2017/03/06/how-learning-works/","section":"post","summary":"Susan Ambrose\u0026rsquo;s book \u0026ldquo;How Learning Works\u0026rdquo; is probably one of the best investments for people who teach. The next best (time) investment is reading Greg Wilson\u0026rsquo;s paper on \u0026ldquo;Software Carpentry: lessons learned\u0026rdquo;.\nI\u0026rsquo;ve watched Greg Wilson\u0026rsquo;s corresponding PyCon 2014 and SciPy 2014 talk numerous times, and is well worth the listen.","tags":["teaching","higher ed","pfps17-journal"],"title":"How Learning Works","type":"post"},{"authors":null,"categories":null,"content":"Some Modest Advice for Graduate Students by Stephen C. Stearns\ntl;dr:\n Always Prepare for the Worst Nobody cares about you You Must Know Why Your Work is Important Psychological Problems are the Biggest Barrier Avoid Taking Lectures - They’re Usually Inefficient Write a Proposal and Get It Criticized Manage Your Advisors Types of Theses Start Publishing Early Don’t Look Down on a Master’s Thesis Publish Regularly, But Not Too Much  This is a writeup that\u0026rsquo;s always worth a read every now and then. It helps put things into perspective and gets you thinking about how your graduate career is advancing.\nI\u0026rsquo;ve learned in my undergraduate years how to navigate the academic system and carve out my own path. It\u0026rsquo;s served me well and gives my advisors confidence that I know what I am doing. I\u0026rsquo;m also lucky to work with people that gives me a lot of academic freedom to explore and work on problems that are interesting to me, but also gives me advice and point me in the right away when I wonder too far off course.\nNow that I am finishing up my coursework, I personally know I need to address my (lack of) writing. The first step of solving a problem is acknowledging the problem:\n I need to write more.\n ","date":1488758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"ae0e708539fa0770df8db30f203b743a","permalink":"/2017/03/06/some-modest-advice-for-graduate-students/","publishdate":"2017-03-06T00:00:00Z","relpermalink":"/2017/03/06/some-modest-advice-for-graduate-students/","section":"post","summary":"Some Modest Advice for Graduate Students by Stephen C. Stearns\ntl;dr:\n Always Prepare for the Worst Nobody cares about you You Must Know Why Your Work is Important Psychological Problems are the Biggest Barrier Avoid Taking Lectures - They’re Usually Inefficient Write a Proposal and Get It Criticized Manage Your Advisors Types of Theses Start Publishing Early Don’t Look Down on a Master’s Thesis Publish Regularly, But Not Too Much  This is a writeup that\u0026rsquo;s always worth a read every now and then.","tags":["teaching","higher ed","pfps17"],"title":"Some Modest Advice for Graduate Students","type":"post"},{"authors":null,"categories":null,"content":"Software-Carpentry1 has made be a better teacher because of the way we collect feedback during our workshops. When teaching a technical class, we use low-tech sticky notes to signify whether individuals need help or not: Green for no help needed, Red for help needed. This is extremely effective while teaching. As an instructor all you need to look for is a red sticky note and you know someone has a question, or if everyone has a green sticky note, the class is following along.\nThe python community is proactive in mentoring. During the PyData Carolinas 2016 conference, Carol Willing talked about how small nudges and discussions can be enough to mentor someone. I\u0026rsquo;ve tried to be a mentor to my students I teach though my lab\u0026rsquo;s summer program. There is a misconception that mentoring is a 24\u0026frasl;7 job. It\u0026rsquo;s not. The right piece of advice at the right time, can change an individual\u0026rsquo;s life.\nI think about the most influential people in my life, and it\u0026rsquo;s mainly a series of seemingly random encounters and discussions that helped shaped the person I am today.\n","date":1488153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"81383a29cd93992a86a8c0d754c02e81","permalink":"/2017/02/27/teaching-and-mentoring/","publishdate":"2017-02-27T00:00:00Z","relpermalink":"/2017/02/27/teaching-and-mentoring/","section":"post","summary":"Software-Carpentry1 has made be a better teacher because of the way we collect feedback during our workshops. When teaching a technical class, we use low-tech sticky notes to signify whether individuals need help or not: Green for no help needed, Red for help needed. This is extremely effective while teaching. As an instructor all you need to look for is a red sticky note and you know someone has a question, or if everyone has a green sticky note, the class is following along.","tags":["teaching","higher ed","pfps17-journal"],"title":"Teaching and Mentoring","type":"post"},{"authors":null,"categories":null,"content":"It\u0026rsquo;s always a good idea to take a break and think about all that\u0026rsquo;s going on in life in perspective. As I approach the end of the coursework phase of my PhD program, I need to think about what\u0026rsquo;s next.\nMy preliminary exam\u0026hellip;\nbut beyond that, I need to think about the (glorious) life after I graduate. Do I go to industry? or stay in Academia?\nCurrently, my duties are closer aligned to that of a scientific research software engineer. I enjoy building scientific software and tools, and enjoy doing data analysis. What I dread is writing. Mainly because I am slow and terrible at it. While this is a feedback loop that I need to break, my disdain for writing means that I am not academically \u0026ldquo;productive\u0026rdquo;. No grants, no papers.\nThe past year at Virginia Tech and taking the Future Professoriate Class, working in the library seems like a good academic fit for my academic toolkit, or my love for teaching has directed me into looking into Professor of Practice positions.\n","date":1487548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"95569a8647cb69f63abaeec0bb0e3985","permalink":"/2017/02/20/where-do-i-fit-in-academia/","publishdate":"2017-02-20T00:00:00Z","relpermalink":"/2017/02/20/where-do-i-fit-in-academia/","section":"post","summary":"It\u0026rsquo;s always a good idea to take a break and think about all that\u0026rsquo;s going on in life in perspective. As I approach the end of the coursework phase of my PhD program, I need to think about what\u0026rsquo;s next.\nMy preliminary exam\u0026hellip;\nbut beyond that, I need to think about the (glorious) life after I graduate. Do I go to industry? or stay in Academia?\nCurrently, my duties are closer aligned to that of a scientific research software engineer.","tags":["teaching","higher ed","pfps17-journal"],"title":"Where Do I Fit in Academia?","type":"post"},{"authors":null,"categories":null,"content":"The communication science workshop at class reminds me of the various acting warm-up exercise I did when I took an acting class in high school. The drills require individuals to coordinate and communicate with other people in the room, but also serves as an icebreaker by doing \u0026lsquo;silly\u0026rsquo; things.\nOne of the first exercise we did in class was to describe our current research. This is a skill that I have refined over the past few years as a PhD student. I remember when I first started my PhD program, I would try to describe my department, program, and current work \u0026lsquo;correctly\u0026rsquo;, only to see a blank stare from the person I would be talking to.\n\u0026ldquo;Genetics, Bioinformatics, and Computational Biology\u0026rdquo; is a lot of big words. Furthermore, \u0026ldquo;Interdisciplinary between Mathematics, Statistics, Life Science, and Computer Science\u0026rdquo; is also a lot of words to describe department.\nI\u0026rsquo;ve cut my introductions down to a short sentence, that is open to more questions, and gives me the ability to cater the detail in my responses depending on the questions I\u0026rsquo;m being asked.\n \u0026ldquo;I study social networks and how things like diseases spread in them.\u0026rdquo;\n The class was fun and a good mental change from daily graduate school life.\n","date":1486944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"b43c3ab34a46c76fab596f22ad844d64","permalink":"/2017/02/13/communicating-science/","publishdate":"2017-02-13T00:00:00Z","relpermalink":"/2017/02/13/communicating-science/","section":"post","summary":"The communication science workshop at class reminds me of the various acting warm-up exercise I did when I took an acting class in high school. The drills require individuals to coordinate and communicate with other people in the room, but also serves as an icebreaker by doing \u0026lsquo;silly\u0026rsquo; things.\nOne of the first exercise we did in class was to describe our current research. This is a skill that I have refined over the past few years as a PhD student.","tags":["teaching","higher ed","pfps17-journal"],"title":"Communicating Science","type":"post"},{"authors":null,"categories":null,"content":"In the United States, higher education aims to be accessible to everyone. Although historically, college and university degrees were mainly for privileged, white males, throughout the years, in order to decrease the socioeconomic gap, more colleges aimed towards local regions and demographics were created.\nFrom Historically Black Colleges and Universities, to Land Grant Schools, to Community Colleges, each `type\u0026rsquo; of College/University was aimed to be more inclusive.\nThis puts things into perspective and I\u0026rsquo;ve considered teaching at community colleges.\n","date":1486339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"3853125a75c1df536cdb8f4dbebcf168","permalink":"/2017/02/06/accessible-education-in-the-united-states/","publishdate":"2017-02-06T00:00:00Z","relpermalink":"/2017/02/06/accessible-education-in-the-united-states/","section":"post","summary":"In the United States, higher education aims to be accessible to everyone. Although historically, college and university degrees were mainly for privileged, white males, throughout the years, in order to decrease the socioeconomic gap, more colleges aimed towards local regions and demographics were created.\nFrom Historically Black Colleges and Universities, to Land Grant Schools, to Community Colleges, each `type\u0026rsquo; of College/University was aimed to be more inclusive.\nThis puts things into perspective and I\u0026rsquo;ve considered teaching at community colleges.","tags":["teaching","higher ed","pfps17-journal"],"title":"Accessible Education in the United States","type":"post"},{"authors":null,"categories":null,"content":" Impostor syndrome (also known as impostor phenomenon or fraud syndrome) is a concept describing high-achieving individuals who are marked by an inability to internalize their accomplishments and a persistent fear of being exposed as a \u0026ldquo;fraud\u0026rdquo;. - Wikipedia\n I really shouldn\u0026rsquo;t feel impostor syndrome. There are many things I can list that point to my accomplishments, yet, I still fall victim to impostor syndrome. It\u0026rsquo;s a phenomenon that is constantly talked about and written about. There was an article in the New York Times back in 2015, and a blog post by Mike Zamansky, a former computer science teacher at my high school, and Distinguished Lecturer at my alma mater.\n\u0026ldquo;High-achieving individuals\u0026rdquo; strive to better themselves by surrounding themselves with people who are better than they are. We all have our idols and heroes. By following them, and listening to their mistakes and journeys, we slowly improve ourselves.\nLast year, I was listening to the [Partially Derivative Podcast]. At the end of their June 09, 2016 episode (S2E12), \u0026ldquo;Descartes Before the Horse\u0026rdquo;, Chris Albon talks about the friendship paradox. The general gist of it goes something along the lines of: people tend to observe and be in contact with people with more friends than they do. This idea can be extrapolated to essentially say, we observe people doing things that are better than ourselves. This perpetuates imposer syndrome and can explain why we always feel inferior to other people; we see people doing things that they are good at and we are not.\n","date":1486339200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"8529d33e6425ca0f1eea0ac15643f2f1","permalink":"/2017/02/06/impostor-syndrome-am-i-qualified/","publishdate":"2017-02-06T00:00:00Z","relpermalink":"/2017/02/06/impostor-syndrome-am-i-qualified/","section":"post","summary":"Impostor syndrome (also known as impostor phenomenon or fraud syndrome) is a concept describing high-achieving individuals who are marked by an inability to internalize their accomplishments and a persistent fear of being exposed as a \u0026ldquo;fraud\u0026rdquo;. - Wikipedia\n I really shouldn\u0026rsquo;t feel impostor syndrome. There are many things I can list that point to my accomplishments, yet, I still fall victim to impostor syndrome. It\u0026rsquo;s a phenomenon that is constantly talked about and written about.","tags":["teaching","higher ed","pfps17"],"title":"Impostor Syndrome: Am I Qualified?","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m taking \u0026ldquo;Preparing the Future Professoriate\u0026rdquo; this semester at Virginia Tech, which is one of the core classes for the \u0026ldquo;Future Professoriate Certificate\u0026rdquo;.\nThe class has us writing blog posts along with weekly journals. Since I\u0026rsquo;m now transitioning into the phase in my doctoral program where I need to transition into writing papers, just getting in to the habit of writing more can\u0026rsquo;t hurt.\nLet\u0026rsquo;s work on just writing more, then focus on tailoring my voice towards different audiences.\n","date":1485734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"03026693a12f8c3c9536e36d5b0206df","permalink":"/2017/01/30/journal-why-am-i-journaling/","publishdate":"2017-01-30T00:00:00Z","relpermalink":"/2017/01/30/journal-why-am-i-journaling/","section":"post","summary":"I\u0026rsquo;m taking \u0026ldquo;Preparing the Future Professoriate\u0026rdquo; this semester at Virginia Tech, which is one of the core classes for the \u0026ldquo;Future Professoriate Certificate\u0026rdquo;.\nThe class has us writing blog posts along with weekly journals. Since I\u0026rsquo;m now transitioning into the phase in my doctoral program where I need to transition into writing papers, just getting in to the habit of writing more can\u0026rsquo;t hurt.\nLet\u0026rsquo;s work on just writing more, then focus on tailoring my voice towards different audiences.","tags":["teaching","higher ed","pfps17-journal"],"title":"Journal: Why am I Journaling?","type":"post"},{"authors":null,"categories":null,"content":" Mission statements and reflections on higher education institutions (plus one high school) that I\u0026rsquo;ve attended.\nReflection It\u0026rsquo;s not surprising that the institutions that have a more specific academic focus, will have a more specific mission statement. The Mailman school of public health\u0026rsquo;s mission is devoted in improving public health, and Stuyvesant High school, is focused on science, mathematics, and technology.\nFor the other Colleges and Universities, the missions are much more general, and the ones located in New York City, mention the city, whereas Virginia Tech mentions the entire Commonwealth of Virginia.\nSome background information about the Macaulay Honors College, it is a college, but the students who attend it have a \u0026lsquo;home\u0026rsquo; college in one of the various colleges in the City University of New York (CUNY) system. I attended in the Macaulay Honors College at Hunter College of the City University of New York. The fact that all the other mission statements are multiple sentences long, makes me wonder if Macaulay\u0026rsquo;s statement is meant to extend which ever home college the student goes to.\nAll the colleges and universities reference research, it would be interesting to hear from the other members in the class who may not have attended research institutions and how those mission statements differ.\n Mission statements Virginia Tech Ut Prosim - That I May Serve\n Virginia Polytechnic Institute and State University (Virginia Tech) is a public land-grant university serving the Commonwealth of Virginia, the nation, and the world community. The discovery and dissemination of new knowledge are central to its mission. Through its focus on teaching and learning, research and discovery, and outreach and engagement, the university creates, conveys, and applies knowledge to expand personal growth and opportunity, advance social and community development, foster economic competitiveness, and improve the quality of life.\n Mailman School of Public Health   To address the most important issues that challenge the health of populations To identify effective approaches to the prevention of disease and disability To create health equity in a diverse world To educate the next generation of public health leaders. We aim to achieve our mission locally and globally through innovative science, education, and leadership.  We are guided by the fundamental principle that health is a human right.\n Columbia University In lumine Tuo videbimus lumen - In Thy light shall we see light\n Columbia University is one of the world\u0026rsquo;s most important centers of research and at the same time a distinctive and distinguished learning environment for undergraduates and graduate students in many scholarly and professional fields. The University recognizes the importance of its location in New York City and seeks to link its research and teaching to the vast resources of a great metropolis. It seeks to attract a diverse and international faculty and student body, to support research and teaching on global issues, and to create academic relationships with many countries and regions. It expects all areas of the university to advance knowledge and learning at the highest level and to convey the products of its efforts to the world.\n Macaulay Honors College  Macaulay Honors College offers exceptional students transformative opportunities to develop their potential beyond what they ever imagined.\n CUNY Hunter College Mihi cura futuri - The care of the future is mine\u0026rdquo;\n Hunter College of the City University of New York, a distinguished public university, values learning in the liberal arts and sciences as a cornerstone of individual development and a vital foundation for a more just and inclusive society. Continuing our long tradition of expanding opportunity, we seek students from all backgrounds to engage in a rigorous educational experience that prepares them to become leaders and innovators in their communities and in the world. Hunter also contributes to intellectual discourse by supporting excellent scholarship and creative activity by its accomplished faculty.\u0026gt;\nHunter undergraduate, graduate, and professional curricula challenge students to think critically - to approach problems from multiple perspectives, distinguish the questions each raises, and recognize the kinds of evidence each values. The College\u0026rsquo;s academic programs stress the significance of human diversity, emphasize research and artistic creation, and invite students to extend their education beyond campus. We cultivate the qualities our graduates need to thrive in their chosen careers and make a difference as active citizens.\u0026gt;\nWe embrace our setting at the heart of New York City - we seek to draw on its energy, capitalize on its remarkable resources, weave it into the fabric of our teaching, research, and creative expression, and give back to it through our service and citizenship.\n Stuyvesant High School Pro Scientia Atque Sapientia - For knowledge and wisdom\n Stuyvesant High School has been a symbol of excellence in education for over a century. Our mission is to continue and enhance that commitment by providing an environment which will nurture and enhance the special academic talents of the students admitted to Stuyvesant. The educational heritage of Stuyvesant is deeply rooted in the tradition of Science, Mathematics and Technology. This has been the foundation of our educational success and must remain the cornerstone of our educational program. Within this context, the goal of this institution is to instill the intellectual, moral and humanistic values necessary for each child to achieve his/her maximum potential as a student and as a caring citizen of the world.\n ","date":1485734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"904fb1d89dabe49eb5a9c4197303463d","permalink":"/2017/01/30/mission-statements/","publishdate":"2017-01-30T00:00:00Z","relpermalink":"/2017/01/30/mission-statements/","section":"post","summary":"Mission statements and reflections on higher education institutions (plus one high school) that I\u0026rsquo;ve attended.\nReflection It\u0026rsquo;s not surprising that the institutions that have a more specific academic focus, will have a more specific mission statement. The Mailman school of public health\u0026rsquo;s mission is devoted in improving public health, and Stuyvesant High school, is focused on science, mathematics, and technology.\nFor the other Colleges and Universities, the missions are much more general, and the ones located in New York City, mention the city, whereas Virginia Tech mentions the entire Commonwealth of Virginia.","tags":["teaching","higher ed","pfps17"],"title":"Mission Statements","type":"post"},{"authors":null,"categories":null,"content":"https://software-carpentry.org/blog/2016/10/what_swc_means_to_me.html\n","date":1477440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"0a77483b1f786f359129157da128e789","permalink":"/2016/10/26/swc-what-the-carpentries-mean-to-me/","publishdate":"2016-10-26T00:00:00Z","relpermalink":"/2016/10/26/swc-what-the-carpentries-mean-to-me/","section":"post","summary":"https://software-carpentry.org/blog/2016/10/what_swc_means_to_me.html","tags":["external","swc"],"title":"SWC: What the Carpentries Mean To Me","type":"post"},{"authors":null,"categories":null,"content":" I wrote a blog post and gave a seminar talk about the Watts 2002 paper. The next step is thinking about expanding the binary decisions with externalities model to a psychological plausible decisions with externalities model. This has been the goal of the multi-agent neural-network (MANN) project.\nContext from the Watts model From an information diffusion perspective, a cascade is the spread of information from an initial set (seed) of individuals (nodes). Watts talks about features of a network that create a global cascade, which is simply a cascade of \u0026lsquo;sufficient\u0026rsquo; size. The ability for a node to receive and pass on a bit of information is it\u0026rsquo;s vulnerability.\nCascades are triggered by the interaction between 2 properties \u0026ndash; a network property (degree centrality) and a node property (threshold). If we look at these 2 properties as mutually exclusive from one another, then nodes with smaller degree centrality will be less vulnerable compared to nodes with higher degree centrality, and nodes with lower threshold values will be more vulnerable than nodes with higher threshold values. This is because nodes with fewer connections have a smaller probability of getting information passed onto it, and nodes with lower thresholds need fewer bits of information to propagate a signal.\nAlthough degree centrality (local dependencies) and (fractional) thresholds are integral parts of the binary decisions with externalities model, another key aspect is heterogeneity. For a given network, not every node will have the same degree centrality, and not every node will have the same fractional threshold value. As a matter of fact, any combination of high/low degree centrality and high/low fractional threshold is possible, and each combination has an effect on a particular node\u0026rsquo;s vulnerability.\n   Degree (k) Threshold ($\\phi$) Vulnerability     High High Low   High Low High   Low High Low   Low Low High    Expanding the Watts model with neural networks The next step is to explore ways the Watts model can be expanded. The rationale being decisions, and similarly, behaviors, are binary, but the process of making a decision and performing an action are not. Mark Orr, Roxanne Thrush, and David C. Plaut suggest incorporating the Theory of Reasoned Action (TRA) as the theoretical framework to describe how behaviors emerge and an auto-associator neural-network as the computational framework to simulate nodes.\nA neural network allows for a multi-dimensional node, not just a simple binary node. The key feature of an auto associative neural network is its ability to learn and reproduce an identity (a.k.a, prototype) given a portion of inputs. From the Wikipedia page, if we are presented the quote \u0026ldquo;I came, I saw\u0026hellip;\u0026rdquo;, we should be able to complete the rest of the quote: \u0026ldquo;\u0026hellip; I conquered\u0026rdquo;.\nIf we re-conceptualize the Watts model, the prototype is the bit of information each node can pass to its neighbors (i.e., a state of 1). Since this is a binary model, then each node can be thought to be trained to have an output of 1, but has a state of 0 until the proper inputs are met (local dependencies and fractional thresholds), and the node has a state of 1.\nFrom the neural network perspective, every node is trained to a particular prototype (e.g., the vector: [1, 1, 1, 0, 0, 0]), and has a state of 0 (e.g., the vector: [0, 0, 0, 0, 0, 0]) until proper inputs are met, the neural network will output the prototype (e.g., the vector: [1, 1, 1, 0, 0, 0]). Additionally, the same output should be returned, if only a portion of the prototype vector is presented.\nCreating a neural network model analogous to the Watts model While there are many questions and solutions on how to relate the Watts model to this newly defined neural-network model, two immediately come to mind.\n How do we measure if information has been passed from one node to another? How do we map fractional thresholds between the two models?  Since we know the trained/learned prototype for each node, one way to know if information passed from the seed node (and subsequent nodes) to a new node is to look at how \u0026lsquo;similar\u0026rsquo; the output of the new node is to the prototype. Similarity can be measured in many ways, one such way is to use cosine similarity. Once the similarity is calculated, we can apply another threshold, $\\theta$, that signals the propagation of information. This new threshold can be used to calculate a binary state, $S_1$ that is analogous to the Watts model state, $S_0$. We can use $\\theta$ to calculate how many nodes are different from the node of interest (this can also be thought of as some function of $S_1$), and use the same threshold, $\\phi$, from the Watts model to determine whether the node of interest will propagate our signal in the network.\nHowever, since our definition of \u0026lsquo;similarity\u0026rsquo; and \u0026lsquo;information propagation\u0026rsquo; relies on the output of our neural network, we need to give the neural network a set of inputs. This leads to anther question:\n How do we provide inputs to nodes that have been designated to carry and propagate information in the network?  We can do this a few ways. For each node that will propagate information we can assign the input as\n the prototype (e.g., the vector: [1, 1, 1, 0, 0, 0]) the state (a.k.a output) of 1 of the \u0026lsquo;different\u0026rsquo; connected nodes the average state of all of the \u0026lsquo;different\u0026rsquo; connected nodes the average state of all connected nodes present each \u0026lsquo;different\u0026rsquo; node in a random order and have the neural network cycle though them present all connected nodes in a random order and have the neural network cycle though them  Other than the first option of using the prototype, the other methods will still preserve the differences in degree centrality between the nodes. For example, if a node only has one neighbor and it will be updated with 1 neighbor, it will always be presented with the same inputs. However if a node has multiple neighbors, the probability of the same neighbor picked every time is proportional to the number neighbors.\nCreating a simpler neural-network model A simpler approach would be to have each node in the network trained against the prototype. Seed a single node with the prototype, and run the model without any additional thresholding rule ($\\theta$), and have each node use a random 1 neighbor\u0026rsquo;s state (i.e., output) and its input.\n","date":1475107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"8f7a61ba35a4e95e34d80b8b7165714b","permalink":"/2016/09/29/expanding-the-watts-model/","publishdate":"2016-09-29T00:00:00Z","relpermalink":"/2016/09/29/expanding-the-watts-model/","section":"post","summary":"I wrote a blog post and gave a seminar talk about the Watts 2002 paper. The next step is thinking about expanding the binary decisions with externalities model to a psychological plausible decisions with externalities model. This has been the goal of the multi-agent neural-network (MANN) project.\nContext from the Watts model From an information diffusion perspective, a cascade is the spread of information from an initial set (seed) of individuals (nodes).","tags":["research","complexity science","network science","notes","mann","watts model"],"title":"Expanding the Watts Model","type":"post"},{"authors":null,"categories":null,"content":" Duncan J Watts wrote a paper that was published in 2002 titled \u0026ldquo;A simple model of global cascades on random networks\u0026rdquo; in the Proceedings of the National Academy of Sciences (PNAS). It\u0026rsquo;s a seminal paper in my current work on information diffusion in (social) networks. Watts shows how the interactions between local dependencies, fractional threshold, and heterogeneity relate to information cascades in networks. My work builds on these ideas, so it\u0026rsquo;s important to have a strong understanding of the terms and model specifications outlined in the paper.\nI\u0026rsquo;ve been working with my Masters and Doctoral advisor, Mark Orr on a simulation platform that incorporates artificial neural networks within an agent-based simulation. The project is called \u0026ldquo;Multi-Agent Neural Network\u0026rdquo; (MANN) and it was my first open source project; the code can be found in two parts:\n The Python module The simulation code  I\u0026rsquo;m currently working on a refactoring a lot of the old code here:\n MANN2 Python module MANN2 simulation code  The paper  These are my notes on the paper. In no way shape or form am I claiming the content below as my own work, it all comes from the paper\n Binary decisions with externalities is a general class of problems that can be used to model a wide variety of real-world problems.\n Cascades limited by connectivity\n Connectivity can play a role in global cascades. Fewer connections mean a lesser chance of information spreading within a network. The abstract references  power law cluster size distribution in standard percolation theory avalanches in self-organized criticality   Cascades limited by local stability\n When a network is highly connected, cascades will occur when a threshold number of connected nodes incorporate the information. Additionally, these types of networks have a bimodal distribution of a global cascade, either one will happen or it will not. When the threshold of agent\u0026rsquo;s to incorporate a bit of information is heterogeneous, the entire system has a higher chance of a global information cascade.  When the agent\u0026rsquo;s threshold is heterogeneous, the ones who have a lower threshold are more vulnerable.  Conversely, when the degree distribution is heterogeneous, the system is less likely to have a information cascade.  When an agent\u0026rsquo;s degree centrality is heterogeneous, the ones who have a higher degree will be less vulnerable.    The model Watts outlines has 3 unique features:\n local dependencies fractional thresholds heterogeneity  Network heterogeneity and threshold heterogeneity are not equivalent.\nGoals of the paper:\n Probability of a global cascade from a single node  Definitions  cascade: event of any size triggered by an initial seed global cascades: a cascade that occupies a finite fraction of an infinite network. A sufficiently large cascade. More than a fixed fraction of a large, but finite network. diffusion random network agents: nodes of a network neighbors: all relevant incoming signals to an agent (usually other agents who have a directed edge to the agent in question) connectivity power law distribution cluster size local stability $\\phi$: threshold  $f(\\phi)$: distribution where $\\phi$ is drawn from  unit interval and normalized such that the area under the distribution is 1   $n$: number of agents in the network $k$: number of neighbors each agent is connected to  $p_k$: probability of an agent being connected to $k$ neighbors (degree distribution) $z$: average number of neighbors $\\langle k \\rangle$ (coordination number)  $\\phi_0$: the number of agents that are seeded with 1 at the start of the simulation  $\\phi_0 \\ll 1$ See definition on small seed  vulnerable vertex: an agent that has a small threshold ($\\phi \\le \\frac{1}{k}$) or has a degree, $k$ such that $k \\le K = [\\frac{1}{\\phi}]$  $\\rho_k$: probability that an agent with degree $k$ is vulnerable  $\\rho_k = P[\\phi \\le \\frac{1}{k}]$   stable vertex: one that is not vulnerable. Stable vertices do not flip states at the start of a simulation small seed: three orders of magnitude less than the system size, $n$ innovator: an agent that is seeded early adopter: a vulnerable agent early and late majority: agents who can be influenced if exposed to multiple early adopters  Model Specification How the Simulation runs  $n$ agents in a network start off with a state of 0 Individual agents can only have a state that is either 0 or 1 Each agent has $k$ neighbors An agent gets a new state of 1, if a fraction of its neighbors, $\\phi$ are also 1.  Otherwise an agent gets a new state of 0.  During each time step, the population evolves:  Update states in random, asynchronous order using the threshold rule Once an agent has a state of 1, it will stay at 1 for the remainder of the simulation   How the model is parameterized  $\\phi$ and $k$ may be heterogeneous  To simplify the simulations, the paper has a homogeneous threshold, $\\phi$  $f(\\phi) = \\delta(\\phi - \\phi_*)$   The network is a uniform random graph A small seed Any pair of vertices is connected with probability $p = \\frac{z}{n}$  in uniform random graphs $p_k = \\text{the Poisson distribution}$  $n = 10,000$ 100 random runs of each simulation  Values used in figures Fig 1 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F1/\nShows for a given threshold, $\\phi$ and average number of neighbors, $z$, where the cascade condition is satisfied. This is an analytical solution, not a simulated solution\n $\\phi$: range from 0.10 to 0.25 in increments of 0.01 $z$: range from 0 to 15  Fig 2 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F2/\nLooking at a specific threshold value, $\\phi = 0.18$\n $n$ = 10,000 1,000 random realizations of the network and initial conditions  Fig 3 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F3/\na log-log plot of the cascade size vs cumulative distribution\nFig 4 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F4/\nReferences http://www.ncbi.nlm.nih.gov/pubmed/16578874\n","date":1472601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"537a8ac94166e1fe38a990ad3d46c4c3","permalink":"/2016/08/31/a-simple-model-of-global-cascades-on-random-networks/","publishdate":"2016-08-31T00:00:00Z","relpermalink":"/2016/08/31/a-simple-model-of-global-cascades-on-random-networks/","section":"post","summary":"Duncan J Watts wrote a paper that was published in 2002 titled \u0026ldquo;A simple model of global cascades on random networks\u0026rdquo; in the Proceedings of the National Academy of Sciences (PNAS). It\u0026rsquo;s a seminal paper in my current work on information diffusion in (social) networks. Watts shows how the interactions between local dependencies, fractional threshold, and heterogeneity relate to information cascades in networks. My work builds on these ideas, so it\u0026rsquo;s important to have a strong understanding of the terms and model specifications outlined in the paper.","tags":["research","complexity science","network science","scientific paper","notes","paper review"],"title":"A Simple Model of Global Cascades on Random Networks","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"I just started reading Six Degrees: The Science of a Connected Age by Duncan J. Watts. It\u0026rsquo;s a book on network theory and related to the Multi-Agent Neural Network project I\u0026rsquo;ve been working on.\nThe first chapter, \u0026ldquo;The Connected Age\u0026rdquo;, reminded me of this XKCD comic:\nWhen first saw the comic as an Psychology / Neuroscience undergrad, I found it extremely amusing \u0026ndash; I wasn\u0026rsquo;t on the bottom! To my delight, the book chapter further highlights the need for \u0026ldquo;less pure\u0026rdquo; disciplines.\nIn the section on \u0026ldquo;Emergence\u0026rdquo;, a topic I\u0026rsquo;ve become familiar with since applying applying Complexity Science to population health, Watts references a quote from Nobel laureate, Phillip Anderson\u0026rsquo;s 1971 paper, \u0026ldquo;More is Different\u0026rdquo;:\n \u0026hellip; physics has been reasonably successful in classifying the fundamental particles, and in describing their individual behavior and interactions, up to the scale of single atom. But throw a bunch of atoms together, and suddenly the story is entirely different. That\u0026rsquo;s why chemistry is a science of its own, not just a branch of physics. Moving farther up the chain of organization, molecular biology cannot be reduced simply to organic chemistry, and medical science is much more than the direct application of the biology of molecules. At a higher level still \u0026ndash; that of interacting organisms \u0026ndash; we encounter now a host of disciplines, from ecology and epidemiology, to sociology and economics, each of which comes with its own rules and principles that are not reducible to a mere knowledge of psychology and biology\n This juxtaposition depicts why studying complexity science and understanding system dynamics is crucial. Watts uses the 1996 blackouts in the United States as examples of how optimizing local infrastructure makes the entire system more fragile.\nThe more I progress academically and professionally, the more I realize the importance of Mathematics. As a statistician in training, I wish I wasn\u0026rsquo;t so focused in the life sciences in the past \u0026ndash; then again I had entirely different goals then. However, working backwards from Psychology to Biology and Epidemiology, I now have greater appreciation for purer disciplines and a drive to learn them.\nHappy Pi day.\n","date":1457913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"852d001e144c838b479b5c405e94daa1","permalink":"/2016/03/14/scientific-purity-in-the-connected-age/","publishdate":"2016-03-14T00:00:00Z","relpermalink":"/2016/03/14/scientific-purity-in-the-connected-age/","section":"post","summary":"I just started reading Six Degrees: The Science of a Connected Age by Duncan J. Watts. It\u0026rsquo;s a book on network theory and related to the Multi-Agent Neural Network project I\u0026rsquo;ve been working on.\nThe first chapter, \u0026ldquo;The Connected Age\u0026rdquo;, reminded me of this XKCD comic:\nWhen first saw the comic as an Psychology / Neuroscience undergrad, I found it extremely amusing \u0026ndash; I wasn\u0026rsquo;t on the bottom! To my delight, the book chapter further highlights the need for \u0026ldquo;less pure\u0026rdquo; disciplines.","tags":["research","complexity science","6 degrees","xkcd","quote","reflection"],"title":"Scientific 'Purity' in the 'Connected Age'","type":"post"},{"authors":null,"categories":null,"content":"https://software-carpentry.org/blog/2016/02/swc-as-a-university-course.html\n","date":1454630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"438ddd46be8e61a08493e29b03eafecc","permalink":"/2016/02/05/swc-software-carpentry-as-a-university-course/","publishdate":"2016-02-05T00:00:00Z","relpermalink":"/2016/02/05/swc-software-carpentry-as-a-university-course/","section":"post","summary":"https://software-carpentry.org/blog/2016/02/swc-as-a-university-course.html","tags":["external","swc"],"title":"SWC: Software Carpentry as a University Course","type":"post"},{"authors":null,"categories":null,"content":"https://software-carpentry.org/blog/2015/11/teaching-extreme-ranges.html\n","date":1446681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"1b8a724be87b1cc2c6caab7150cb8399","permalink":"/2015/11/05/swc-teaching-bimodal-workshops-with-a-large-range/","publishdate":"2015-11-05T00:00:00Z","relpermalink":"/2015/11/05/swc-teaching-bimodal-workshops-with-a-large-range/","section":"post","summary":"https://software-carpentry.org/blog/2015/11/teaching-extreme-ranges.html","tags":["external","swc"],"title":"SWC: Teaching Bimodal Workshops with a Large Range","type":"post"},{"authors":["Daniel Chen","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":null,"categories":null,"content":"https://software-carpentry.org/blog/2015/06/learner-assessment-part-01.html\n","date":1435017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"c99de342a970b8937ed39cbdf75d6af6","permalink":"/2015/06/23/swc-assessing-our-learners-part-i/","publishdate":"2015-06-23T00:00:00Z","relpermalink":"/2015/06/23/swc-assessing-our-learners-part-i/","section":"post","summary":"https://software-carpentry.org/blog/2015/06/learner-assessment-part-01.html","tags":["external","swc"],"title":"SWC: Assessing Our Learners Part I","type":"post"},{"authors":null,"categories":null,"content":" I\u0026rsquo;ve been an instructor for Software-Carpentry (SWC) over a year now. It\u0026rsquo;s been a facinating experience and I\u0026rsquo;m proud to be a part of an open source movement promoting best practices. Typically when looking to start learing data science/analysis the first things people look up is something along the lines of: \u0026ldquo;learn python\u0026rdquo;, \u0026ldquo;free online r course\u0026rdquo;, \u0026ldquo;data science python\u0026rdquo;, \u0026ldquo;r jobs\u0026rdquo;, etc. Or scan through the coursera offerings. I\u0026rsquo;m a bit biased, but I think the SWC material is one of the best ways to just get familiar with the basics. This isn\u0026rsquo;t a blog post about SWC per se, but how one might go about learing and navigating some of the material on your own without attending a workshop.\nSoftware Carptentry Material SWC has a page of lessons that link to the various lessons taught during workshops. The core material covers:\n Unix Some programming language like Python, R, MATLAB, etc Version control using Git or Mercurial Databases and SQL  If you take a look and listen (there is sound) to the introductory browsercase. You will see that we say we teach the above material, but in essense we try to convey:\n Automation of repetitive tasks Tracking and sharing work Building modular code Manage data  It\u0026rsquo;s a little bait-and-switch :)\nInstalling Things For each SWC workshop is accompanied by a website. The website Contains information about location, instructors, helpers, syllabus, etc. For people reading this post, the most important part may be the installation instructions towards the middle/bottom of the page. There are separate instructions depending on your operating system.\nEssentially:\n Python: Anaconda R: R and RStudio  Navigating the Material For people who have not been to a workshop trying to navigate the lesson material can be tricky. And I would highly suggest going through at least the first 3 Unix lessons. This is mainly because loading files in a programming language uses the concept of relative and absolute paths, and it\u0026rsquo;s important to know where your data is and how to load it.\nUsing the R material as an example. The instructors typically teach from the Novice R Inflammation lessons. There has been some discussion and plans to use the Gapminder data to teach lessons. The beta can be found in the R Novice Gapminder lesson.\nOnce you\u0026rsquo;re on the github page of the lesson, there are a few ways to get the lessons plan\n If you know your way around git, you can clone the repository Otherwise, you will see an option to \u0026ldquo;Download ZIP\u0026rdquo; on the right panel. View them in the browser  The first 2 methods get you do the same place \u0026ndash; have the lessons on your computer. In order to view the lessons, open up the folder and all the lessons are numbered and end in .html. Click, read, and code through them in order.\nIf you want to view the lessons online, you can click the .Rmd documents, and github should be able to render the lesson for you in the browser. For Python and other materials, use the .md documents. The caveat with viewing the .Rmd and .md (aka R markdown, and markdown respectively) is that it does not render the documently exactly the same way as the .html files. There is a potential for things to be rendered incorrectly, and you\u0026rsquo;ll see some bizzare (YAML) header on the top. It\u0026rsquo;s great for a quick reference when you\u0026rsquo;re not at your computer, but I would suggest using one of the .html methods.\nData Carpentry Material Software Carpentry\u0026rsquo;s sister organization, Data Carpentry also has a set of lesson plans. From their about us page:\n Data Carpentry is a sister organization of Software Carpentry designed to teach basic concepts, skills and tools for working more effectively with data. We develop curricula and run workshops that are 1) domain specific; 2) target fundamental data analysis and data management challenges; and 3) require little or no prior programming experience. In many domains of research the rapid generation of large amounts of data is fundamentally changing how research is done. The deluge of data presents great opportunities, but also many challenges in managing, analyzing and sharing data. Data Carpentry aims to teach the data skills that will enable researchers to be more effective and productive.\n Pick a language you want to do the lesson with. clone or Download ZIP, and go through the lessons using the .html, .Rmd, or .md as mentioned above.\nHappy Learning! Hopefully this has been clear enough. If not, post a comment, and I\u0026rsquo;ll respond and update this post accordinly.\n","date":1430784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"37fe490384cff41614e6919927ea233a","permalink":"/2015/05/05/getting-started-with-data-science-and-analysis/","publishdate":"2015-05-05T00:00:00Z","relpermalink":"/2015/05/05/getting-started-with-data-science-and-analysis/","section":"post","summary":"I\u0026rsquo;ve been an instructor for Software-Carpentry (SWC) over a year now. It\u0026rsquo;s been a facinating experience and I\u0026rsquo;m proud to be a part of an open source movement promoting best practices. Typically when looking to start learing data science/analysis the first things people look up is something along the lines of: \u0026ldquo;learn python\u0026rdquo;, \u0026ldquo;free online r course\u0026rdquo;, \u0026ldquo;data science python\u0026rdquo;, \u0026ldquo;r jobs\u0026rdquo;, etc. Or scan through the coursera offerings. I\u0026rsquo;m a bit biased, but I think the SWC material is one of the best ways to just get familiar with the basics.","tags":["tutorials","opensource","python","r","data analysis","data science"],"title":"Getting Started with Data Science and Analysis","type":"post"},{"authors":null,"categories":null,"content":"https://software-carpentry.org/blog/2015/02/cookie-cutter.html\n","date":1423526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"3b7674fceefbaafa365b4d2ffff2bf72","permalink":"/2015/02/10/swc-cookie-cutter/","publishdate":"2015-02-10T00:00:00Z","relpermalink":"/2015/02/10/swc-cookie-cutter/","section":"post","summary":"https://software-carpentry.org/blog/2015/02/cookie-cutter.html","tags":["external","swc"],"title":"SWC: Cookie Cutter","type":"post"},{"authors":null,"categories":null,"content":" I started my first open source project towards the end of summer 2014 I began working at the Social Decision Analytics Laboratory at the Virginia Informatics Institutee. The project involves creating a simulation environment where we can observe how an idea or belief spreads within a social network. The initial thought was to look for and extend an agent-based model package in Python. I did manage to find the pyabm package by Alex Zvoleff, but for simplicity\u0026rsquo;s sake I wrote my own package instead of extending his existing code base.\nI took it as an opportunity to learn and \u0026lsquo;do things right\u0026rsquo;:\n Write a module that contains all program logic Write functions that only do only one thing, but does it well Unit test everything Test builds on different versions of Python Document your code Get it on PyPI PEP8  This series of posts are essentially notes to myself, and to other programmers who are starting out from where I was. In this post I discuss the background, rational, and inital steps in my open source struggles. In part II I discuss automatic project and code documentation.\nBackground I had already been involved with Software Carpentry (SWC) for about 8 months when I started the project, so many of the concepts I just listed were not entirely foreign, just a matter of implementation. Thanks to a workshop by Gabriel Perez-Giz at NYU earlier that summer, I took it upon myself to practice my EMACS and setup elpy as my IDE. A development environment that can be used within a terminal was especially important, because the simulations I would be running would all be on a remote server.\nFor git, it was getting into the habit of not committing directly to master. For Python, I\u0026rsquo;d write a suite of unit tests for the first time, and I figured if I write some comments and docstrings I can get a nice document from it (I was wrong about that). Finally, I wanted to get those cool little badges people have on their github repo about build status, coverage, etc. That\u0026rsquo;s when I found an awesome blog post by Jeff Knupp titled \u0026lsquo;Open Sourcing a Python Project the Right Way\u0026rsquo;.\nIt\u0026rsquo;s an amazing read if you are ready to take your programming practices to the next step. Jeff references a cookie-cutter that will setup the python project boilerplate, but I opted to just follow the blog post and do everything manually so I can have a better understanding as to what is going on in the background. Plus, this lets me slowly add features, rather than have an entire repo loaded with unknown files. More important, I added a few other things to make my project \u0026lsquo;better\u0026rsquo;:\n Use the git-flow paradigm to add new features Continuous integration (with TravisCI) Test your package with other versions of Python (using Tox) Code documentation with Sphinx and Read the Docs  I opted not to use Tox locally (at least not yet). TravisCI is handling my Python compatibility since I was working with Python 3.4, and was not going to have Python 2 support. I added a build for Python 3.4 and 3.3, and called it a day. Also I opted to use nosetests instead of pytest since that\u0026rsquo;s what I was shown when I helped out at the MIT SWC workshop. That, plus there was SWC material, and other big open source projects use it, was my rational to stick with nosetests.\n My project The original Multi-Agent Neural Network (MANN) project had a main.py script that loaded in my modules for the individual agents and the network structure. Everything was placed under the mann folder in the repo, with no subfolders. When I eventually realized that I wanted the project to be PyPI ready, I wanted to separate the main program logic (the MANN code) from the actual script that sets up the simulation. I eventually moved the main.py script (and all required files) into the Multidisciplinary Diffusion Model Experiments (MDME) repo.\nPrerequisites I\u0026rsquo;ve realized the more I program in python, the more invaluable virtual environments are when developing packages. A Virtual Environment is a tool to keep dependencies required by different projects in separate places while simultaneously keeping your base python distribution clean and working should something go awry. They also allow you to flip between Python 2 and Python 3 depending on what version a piece of code you are trying to run was written in. Pretty cool stuff.\nThe Python distribution I use is called Anaconda.\nSetting up virtual environments using conda:\nconda create -n VIRTUAL_ENV_NAME python=3.4\nYou can specify different versions of python and/or pre-create environments with a set of modules if needed.\nSwitching between environments: source activate VIRTUAL_ENV_NAME\nTo exit out of an environment: source deactivate.\nYou can read up more about creating environments on the conda documentation\n Createing Python 2\u0026frasl;3 environments conda create Python Packages and Environments with conda  Issues Turning a current python project into a \u0026lsquo;module\u0026rsquo; can break a few things. It is as simple as putting a __init__.py into a directory to signify that the contents of the folder is now a Python module, but there can be some weird side-effects.\nUnit tests When you turn your project into a package, you will find that if you run nosetests it will start running the unit tests for all the modules you load (if they have any).\nFor example I was initially using\nnosetests --cover-branches --with-coverage\nto test my code, but I had to add the --cover-package=MODULE_NAME to get it to only test the code in my module:\nnosetests --cover-branches --with-coverage --cover-erase --cover-package=mann\nImporting Modules Since I\u0026rsquo;ve moved out my simulation code from the code that defines that MANN module, I had to install the MANN module. One way to do it is to upload the code to PyPI and pip install the package. The problem is when you want to load a module to PyPI you essentially need to have a git tag associated with the version you want. This is problematic when you are rapidly prototyping since you\u0026rsquo;ll need to either delete tags, or constantly increment the tag. You\u0026rsquo;ll end up with a v.0.314.0 very quickly. You can do a local install of your module by going to where you have the setup.py file and doing:\npython setup.py install\nThis is why virtual environments are really helpful.\n You won\u0026rsquo;t have to worry about cluttering the base Python distribution and modules with your \u0026lsquo;test\u0026rsquo; code. You can test your code before uploading it to PyPI (or anywhere else). This is really helpful becuase PyPI requires you to have a tag. Doing a local install allows you to workout any potential bugs before submitting a release so you won\u0026rsquo;t have to have 15 release numbers for your first release.  ","date":1423094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"89956901a7beb98b37bc3ba864366996","permalink":"/2015/02/05/open-source-a-python-project-i/","publishdate":"2015-02-05T00:00:00Z","relpermalink":"/2015/02/05/open-source-a-python-project-i/","section":"post","summary":"I started my first open source project towards the end of summer 2014 I began working at the Social Decision Analytics Laboratory at the Virginia Informatics Institutee. The project involves creating a simulation environment where we can observe how an idea or belief spreads within a social network. The initial thought was to look for and extend an agent-based model package in Python. I did manage to find the pyabm package by Alex Zvoleff, but for simplicity\u0026rsquo;s sake I wrote my own package instead of extending his existing code base.","tags":["tutorials","opensource","python","nosetests","pip install"],"title":"Open Source A Python Project I","type":"post"},{"authors":null,"categories":null,"content":"https://software-carpentry.org/blog/2014/11/revised-instructor-survey.html\n","date":1414886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"fe9a7c89fd744fe408e6c73eccf6fcc4","permalink":"/2014/11/02/swc-revamping-the-instructor-survey/","publishdate":"2014-11-02T00:00:00Z","relpermalink":"/2014/11/02/swc-revamping-the-instructor-survey/","section":"post","summary":"https://software-carpentry.org/blog/2014/11/revised-instructor-survey.html","tags":["external","swc"],"title":"SWC: Revamping the Instructor Survey","type":"post"},{"authors":null,"categories":null,"content":"Earlier this month, Ben Wellington wrote a blog post about How Memorizing “$19.05” Can Help You Outsmart the MTA. A few days later, after the post went viral, the MTA responded by saying this is so there will be enough change in the machine for cash users and this will be addressed next year after the new fare increase.\nBen suggested to us all to remember $9.55, $19.05, and $38.10 to get a MetroCard that will yield us a $0.00 card at the end. Among all the comments I saw going around my social media feeds, one of the comments that I saw was how remembering these magic numbers are great, but people are still annoyed that there will always be a few cents left over depending on when you read about the magic fares.\nIf we look closer at the table Ben shared (partially reproduced below), and take in to account the fact that the kiosks will only take in whole nickel values (i.e., values that end in either 5 or 0), we can see one thing\u0026hellip;\n   # Rides You Pay ($) On Card ($) Remainder ($)     1 2.40 2.52 0.02   2 4.80 5.04 0.04   3 7.15 7.51 0.01   4 9.55 10.03 0.03   5 11.90 12.50 0.00   8 19.05 20.00 0.00   13 30.95 32.50 0.00    \u0026hellip; there are ways for us to generate $0.01, $0.02, $0.03, and $0.04 remainders in our card! What does that mean for us? It means regardless of how many leftover cents your card will have, there is always a way to get it up to a whole nickel value. From there we can input one of the three magical values plus the amount needed to get you to a whole nickel value + how much you are currently short.\nLet\u0026rsquo;s walk through an example.\nYou buy a \u0026lsquo;Fast $9 MetroCard\u0026rsquo;, You end up with $9.45 on it. By the time you\u0026rsquo;ve used it to exhaustion, you will have $1.95 on it. You are \u0026lsquo;short\u0026rsquo; $2.50 - $1.95 = $0.55. So you go to the kiosk and put in $19.05 + $0.55 = $19.60, and voilà. You now have an 9 ride MetroCard.\nLet\u0026rsquo;s say you have a MetroCard laying around with $0.01 on it. We know we need $0.04 to get a whole nickel value. We can look up on the table some value that gives us $0.04, so we add $4.80 on the card. This gives us a $5.05 MetroCard. Nice! we have a whole nickel amount. If we use this card to exhaustion, we will end with $0.05, leaving us short $2.45 for the next full ride. We can now use one of the 3 magical numbers in conjunction with the $2.45 to get us a MetroCard bound for $0.00.\nIf you wanted to do this all in one go\u0026hellip; is we added $4.80 + $2.45 + $19.05 = $26.30 to to our card.\nI haven\u0026rsquo;t accounted for any weird 5% bonuses the kiosks may/may not give you. But the premise is simple, use one of the pre-calculated values to \u0026lsquo;nickel-out\u0026rsquo; your card, then use one of the magic values + difference and you\u0026rsquo;ll have one fewer thing to be worried about.\n","date":1412023200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"1fbb0d367cc5ef21170be0205d6d562c","permalink":"/2014/09/29/hacking-your-metrocard-to-0.00/","publishdate":"2014-09-29T20:40:00Z","relpermalink":"/2014/09/29/hacking-your-metrocard-to-0.00/","section":"post","summary":"Earlier this month, Ben Wellington wrote a blog post about How Memorizing “$19.05” Can Help You Outsmart the MTA. A few days later, after the post went viral, the MTA responded by saying this is so there will be enough change in the machine for cash users and this will be addressed next year after the new fare increase.\nBen suggested to us all to remember $9.55, $19.05, and $38.10 to get a MetroCard that will yield us a $0.","tags":["lifehack","mta","metrocard"],"title":"Hacking Your MetroCard to $0.00","type":"post"},{"authors":null,"categories":null,"content":"You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve --watch, which launches a web server and auto-regenerates your site when a file is updated.\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\nJekyll also offers powerful support for code snippets:\n{% highlight ruby %} def print_hi(name) puts \u0026ldquo;Hi, #{name}\u0026rdquo; end print_hi(\u0026lsquo;Tom\u0026rsquo;) #=\u0026gt; prints \u0026lsquo;Hi, Tom\u0026rsquo; to STDOUT. {% endhighlight %}\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll’s dedicated Help repository.\n","date":1410104337,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"7a00e7dcddbc78e7e879e57c1a1a132c","permalink":"/2014/09/07/welcome-to-jekyll/","publishdate":"2014-09-07T15:38:57Z","relpermalink":"/2014/09/07/welcome-to-jekyll/","section":"post","summary":"You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve --watch, which launches a web server and auto-regenerates your site when a file is updated.\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.","tags":["jekyll","update"],"title":"Welcome to Jekyll!","type":"post"},{"authors":null,"categories":null,"content":"https://software-carpentry.org/blog/2014/07/high-school-students-at-rockefeller.html\n","date":1404691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"67de8cd0832ecfea31f171423befb176","permalink":"/2014/07/07/swc-our-first-high-school-workshop-at-rockefeller-university/","publishdate":"2014-07-07T00:00:00Z","relpermalink":"/2014/07/07/swc-our-first-high-school-workshop-at-rockefeller-university/","section":"post","summary":"https://software-carpentry.org/blog/2014/07/high-school-students-at-rockefeller.html","tags":["external","swc"],"title":"SWC: Our First High School Workshop at Rockefeller University","type":"post"},{"authors":null,"categories":null,"content":"One of the most common tasks (for me at least) is saving or getting data from another directory from where the current python script is running. However, for many of the file I/O functions, it assumes the current directory or you need to give it an absolute directory. Using something like ../other_directory will not work.\nHere is one way you can get the current script directory, and then append to the string the relative paths.\nAdapted from user Al Cramer: http://stackoverflow.com/questions/4934806/python-how-to-find-scripts-directory\nIn this example I have a pandas dataframe I want to save to another folder\nabs_dir = os.path.dirname(__file__) print abs_dir rel_dir = os.path.join(abs_dir, '../data') print rel_dir data = ''.join([rel_dir, '/nhtsa.csv']) print data df.to_csv(data, sep=',', encoding='utf-8')  ","date":1400198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"9b3b72fa61f39fb84f22dfb8eba0b300","permalink":"/2014/05/16/absolute-and-relative-directories-in-python/","publishdate":"2014-05-16T00:00:00Z","relpermalink":"/2014/05/16/absolute-and-relative-directories-in-python/","section":"post","summary":"One of the most common tasks (for me at least) is saving or getting data from another directory from where the current python script is running. However, for many of the file I/O functions, it assumes the current directory or you need to give it an absolute directory. Using something like ../other_directory will not work.\nHere is one way you can get the current script directory, and then append to the string the relative paths.","tags":["tutorial","tips","python"],"title":"Absolute and Relative Directories in Python","type":"post"},{"authors":null,"categories":null,"content":"Adapted answer from Shubhmay and TimD on askubuntu on installing Eclipse.\n Extract the eclipse.XX.YY.tar.gz file: tar -zxvf eclipse.XX.YY.tar.gz\n Copy the extracted folder to /opt: sudo cp -r eclipse /opt\n Create a desktop file gedit eclipse.desktop\n copy the following to the eclipse.desktop file\n[Desktop Entry] Name=Eclipse Type=Application #Exec=eclipse Exec=env UBUNTU_MENUPROXY=0 eclipse # for menubar bug in 13.10 Terminal=false Icon=eclipse Comment=Integrated Development Environment NoDisplay=false Categories=Development;IDE; Name[en]=Eclipse  Execute the following command to automatically install it in the unity: sudo desktop-file-install eclipse.desktop\n Create a symlink in /usr/local/bin using\ncd /usr/local/bin ln -s /opt/eclipse/eclipse  according to user ortang:\n Use the eclipse version when creating a the symlink (eg: ln -s /opt/eclipse/eclipse /usr/local/bin/eclipse42), and use Exec=eclipse42 at the desktop entry. That way you will be able to install multiple different versions of eclipse\n For eclipse icon to be displayed in dash, eclipse icon can be added with\ncp /opt/eclipse/icon.xpm /usr/share/pixmaps/eclipse.xpm\n  ","date":1391472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"1436fac999ce03c22cbd511d8f97f79b","permalink":"/2014/02/04/installing-eclipse-in-ubuntu-13.10/","publishdate":"2014-02-04T00:00:00Z","relpermalink":"/2014/02/04/installing-eclipse-in-ubuntu-13.10/","section":"post","summary":"Adapted answer from Shubhmay and TimD on askubuntu on installing Eclipse.\n Extract the eclipse.XX.YY.tar.gz file: tar -zxvf eclipse.XX.YY.tar.gz\n Copy the extracted folder to /opt: sudo cp -r eclipse /opt\n Create a desktop file gedit eclipse.desktop\n copy the following to the eclipse.desktop file\n[Desktop Entry] Name=Eclipse Type=Application #Exec=eclipse Exec=env UBUNTU_MENUPROXY=0 eclipse # for menubar bug in 13.10 Terminal=false Icon=eclipse Comment=Integrated Development Environment NoDisplay=false Categories=Development;IDE; Name[en]=Eclipse  Execute the following command to automatically install it in the unity: sudo desktop-file-install eclipse.","tags":["tutorials","Eclipse","Install","Linux","Setup"],"title":"Installing Eclipse in Ubuntu 13.10","type":"post"},{"authors":null,"categories":null,"content":"Repast Simphony is a Java based collection of agent-based modeling and simulation software.\nRepast Simphony 2.1 installs its library files into ~/.eclipse/ECLIPSEVERSION/plugins/ and features/\nwhen you create the model it looks for these files in /opt/eclipse/plugins/ and /features\nThis issue has been brought up and added to the issue tracker as of 2013-09-30\nUser mijael posted a solution to the Repast Interest Mailing list about creating symlinks to the /opt directory:\ncd .eclipse/org.eclipse.platform_4.3.0_1473617060_linux_gtk_x86_64/plugins/ ls | xargs -I targ sudo ln -s ~/.eclipse/org.eclipse.platform_4.3.0_1473617060_linux_gtk_x86_64/plugins/targ /opt/eclipse/plugins/targ  repeat the same for the features directory:\ncd .eclipse/org.eclipse.platform_4.3.0_1473617060_linux_gtk_x86_64/features/ ls | xargs -I targ sudo ln -s ~/.eclipse/org.eclipse.platform_4.3.0_1473617060_linux_gtk_x86_64/features/targ /opt/eclipse/features/targ  ","date":1391472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"6de2a959c8fc007b0b82ffdea2cf4d3d","permalink":"/2014/02/04/setting-up-repast-simphony-2.1-in-linux/","publishdate":"2014-02-04T00:00:00Z","relpermalink":"/2014/02/04/setting-up-repast-simphony-2.1-in-linux/","section":"post","summary":"Repast Simphony is a Java based collection of agent-based modeling and simulation software.\nRepast Simphony 2.1 installs its library files into ~/.eclipse/ECLIPSEVERSION/plugins/ and features/\nwhen you create the model it looks for these files in /opt/eclipse/plugins/ and /features\nThis issue has been brought up and added to the issue tracker as of 2013-09-30\nUser mijael posted a solution to the Repast Interest Mailing list about creating symlinks to the /opt directory:","tags":["tutorials","Linux","Repast Simphony","Setup"],"title":"Setting Up Repast Simphony 2.1 in Linux","type":"post"},{"authors":null,"categories":null,"content":"Columbia University\u0026rsquo;s Courseworks website allows you to download files for a registered class. A problem arises when multiple files needed to be downloaded simultaneously. One simply cannot just select the files and bulk download them. Courseworks have instructions to circumvent this limitation in the web interface by implementing a WebDAV client. However, other than Cyberduck under their Mac instructions, the Windows suggestion is quite limited.\nIn this post I discuss setting up a WebDAV Client for Windows and Mac. If you are on Linux, I\u0026rsquo;ve only used AnyClient, which is Java based but is not as comprehensive as the Windows/Mac alternatives suggested in this post (and this is the one suggested by Courseworks). If anyone knows of a good Linux WebDAV client, let me know!\nThis post shows how to setup BitKinex and Cyberduck in Windows. You can also use the Cyberduck instructions for Mac.\nCourseworks also has instructions on setting up Cyberduck under their Mac section; the same instructions can also be used for the Windows version of Cyberduck.\nBe careful when accessing a directory that you do not own. There is the potential that you may be able to upload files to the directory, or worse, change, overwrite, or delete the files.\nStep 1: Accessing your Courseworks files and resources links.\nWhat you have to do first is log into your Coureworks, click on the course number for your class and then in the left frame of the page click \u0026ldquo;Files \u0026amp; Resources\u0026rdquo;. On the top of the page you will see a link for \u0026ldquo;upload-download multiple resources\u0026rdquo;, see figure 1.1\n Figure 1.1: Upload-Download Multiple Resources Link It will direct you do a link to copy in \u0026ldquo;step 1\u0026rdquo; and directions to set up WebDAV in \u0026ldquo;step 2\u0026rdquo;. In \u0026ldquo;step 2\u0026rdquo; you will find the instructions for setting up Cyberduck for Mac, which is the same for windows (like I mentioned above).\nThe link that you need essentially has 2 parts, the part that is highlighted in Figure 1.2 is what you will need when you are using BitKinex, the entire link is needed for Cyberduck,\n Figure 1.2: components of your WebDAV directory Step 2a Windows:\nFor Windows I use Bitkinex. You can also use Cyberduck (which I describe in the Mac section below).\nYou can download BitKinex here. At the time of this post Current Version: 3.2.3 Release Date: 07/11/2010. Do not worry about the \u0026lsquo;older\u0026rsquo; release date.\nAfter you have BitKinex installed, you will right click \u0026ldquo;Http/WebDAV\u0026rdquo; and go to New \u0026gt; Http/WebDav, See figure 2.1 (sorry my mouse is highlighted over the wrong option).\n[][5]\nFigure 2.1: Creating a new WebDAV connection in BitKinex A popup window will appear for a name, you can simply put \u0026ldquo;Columbia\u0026rdquo; or \u0026ldquo;newcourseworks.columbia.edu\u0026rdquo; or \u0026ldquo;courseworks.columbia.edu\u0026rdquo;.\nNext, it should direct you to a settings menu under the \u0026ldquo;Server\u0026rdquo; option on the left, See Figure 2.2. for server address enter \u0026ldquo;courseworks.columbia.edu\u0026rdquo;, I used \u0026ldquo;newcourseworks.columbia.edu\u0026rdquo;, but I believe the two should be the same, and the former may be preferred. You can also refer to Step 1 and copy the text I showed that was not highlighted in blue (Figure 1.2). Under Authentication, enter your UNI for \u0026ldquo;User\u0026rdquo; and password for \u0026ldquo;Password\u0026rdquo;\n[][6]\nFigure 2.2: WebDAV Setup Now go back to the Step 1, and Figure 1.2, and select the part of the link that I have selected in blue. It should start with a \"/dav/...\". On the left hand panel under \"Server\" there should be a section called \"Site Map\". Paste the text you just copied into the textfild and select \"Directory (WebDAV complient)\" and click Add. Repeat this process for all your courses. See figure 2.3 for more details.  Figure 2.3: Course Folder Directory Setup Here is what BitKinex looks like after you double click on \"newcourseworks.columbia.edu\" under the \"Http/WebDAV\" section of the initial main application screen (See figure 2.4). The application has 3 main panels. The one on the far left shows a directory tree to everything you input shown in Figure 2.3. The middle shows the file contents as you navigate the folders, you can double click the folder with two dots \"..\" to move up to the parent folder (the folder you just came from). The right panel shows contents on your computer. There is a small area with green and blue buttons that may/may not be greyed out. These let you move files to your computer or upload to the server. You can hover over them for more details.  Figure 2.4: Using BitKinex Now you should be able to go to your course's files and select everything (or everything you need) and just click the green button, and it will automatically download everything over. Just take note that sometimes professors have very long names for their PDF's and depending where on your computer you are downloading the file to, it may not save. Most of the time it is because the entire file path and name is over 256 characters. What you have to do then is just navigate to your desktop on the right panel, download the file there, and then rename it before moving it to the directory of your choice. On the top of the window you can also set folder favorites, so you can easily navigate, to say, your \"Columbia\" folder quicker. Step 2b Mac:\nCyberduck can be found here. I am using the Windows version of Cyberduck, the Mac should look almost the same.\nCourseworks have the following instructions on their site:\n Click the Open Connection button. In the *Server* dialog box, type (or copy and paste) the path as shown above. Type in your Sakai username and password and click OK.  You will now see a window on your Mac screen that represents the resources that are in your site. Simply drag and drop between this window and other Finder windows on your Mac to transfer files to and from your Sakai site\u0026rsquo;s resources folder.\nI will be showing a different method, all have the same results.\nOn the bottom left of the application, click on the \u0026ldquo;+\u0026rdquo; sign (Figure 3.1)\n Figure 3.1: Adding a bookmark Next, choose \"WebDAV(HTTP/SSL)\" from the drop down menu, then give your course a name in \"Nickname\" and you can paste everything from the Courseworks link under \"Server\". Not everything will paste, but click the little arrow on the bottom left and the menu will expand and the remaining of the pasted text will be shown under \"Path\", if not you can manually paste the non-highlighted text I showed in Figure 1.2 to \"Server\", and the highlighted text to \"Path\". Your results should look similar to figure 3.2 below.  Figure 3.2: Setting up Cyberduck Connections Finally, just close that window and you should see your class listed in the Bookmarks section, figure 3.3. You can now double click the \"drive\" and it will open up the Courseworks files and you can click and drag to your folders on your computer.  Figure 3.3: Bookmarks View    [5]: {{ site.baseurl }}/wp-content/uploads/2013/09/webdav03.png [6]: {{ site.baseurl }}/wp-content/uploads/2013/09/webdav04.png\n","date":1378252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"c519ec352ce7d0f4ef8e313adc26d502","permalink":"/2013/09/04/setting-up-a-webdav-client-for-courseworks/","publishdate":"2013-09-04T00:00:00Z","relpermalink":"/2013/09/04/setting-up-a-webdav-client-for-courseworks/","section":"post","summary":"Columbia University\u0026rsquo;s Courseworks website allows you to download files for a registered class. A problem arises when multiple files needed to be downloaded simultaneously. One simply cannot just select the files and bulk download them. Courseworks have instructions to circumvent this limitation in the web interface by implementing a WebDAV client. However, other than Cyberduck under their Mac instructions, the Windows suggestion is quite limited.\nIn this post I discuss setting up a WebDAV Client for Windows and Mac.","tags":["tutorials","Client","Linux","Mac","Setup","WebDAV","Windows"],"title":"Setting up a WebDAV Client for Courseworks","type":"post"},{"authors":["Daniel Chen","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561957487,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"Edit: March 31, 2014: As an assignment for Software Carpentry I made a screencast to follow along with these steps, here\nInstalling Windows XP in VirtualBox:\nFiles needed:\n1. VirtualBox (VB) Intaller here:\nhttps://www.virtualbox.org/wiki/Downloads\n CD or ISO of OS (for my CUMC/CU friends you can find the windows XP ISO here:\nhttp://cuit.columbia.edu/cuit/software-downloads/operating-system-software/windows-xp  The VB version I am using is 4.2.6\nAfter Installing VB. You want to click \u0026ldquo;New\u0026rdquo; in the toolbar. You will be given a setup dialogue.\nName and operating System:\nGive your virtual machine (VM) a name (i.e. Windows XP). Make sure Type is \u0026ldquo;Microsoft Windows\u0026rdquo; and Version is \u0026ldquo;Windows XP\u0026rdquo; (Note: you can install other operating systems and/or different version of windows, just set those dropdown menu\u0026rsquo;s accordingly).\nMemory size:\nBy default, VB set my WinXP VM to have 192 MB. Depending on how much RAM (memory) your computer has and programs you want to run you can increase it. For the most part you can leave it as is.\nHard drive:\nYour VM is essentially a computer, and computers need hard drives!\nclick the radio button for \u0026ldquo;create a virtual hard drive now\u0026rdquo;\nanother setup window will popup. Just leave the Hard drive file type as VDI (VirtualBox Disk Image). Leave the storage on physical hard drive as \u0026ldquo;dynamically allocated\u0026rdquo;. This will allow the VM to take up only enough space as needed and allow it to grow/shrink as needed. The file location and size should default the same as your VM name and 10.00GB. You can leave this at its defaults unless you know you will be saving and emulating a lot of programs in your VM. We will be setting up shared folders later so you can access your regular files on your computer without having copies in your VM. You just need enough space to install your OS and whatever programs you will be running. 10GB should be more than enough.\nYour new VM will show up on the left panel in VB. Select it and click the Start button in the toolbar. Your VM will load and a dialogue box will appear telling you that when your VM is active it will capture your mouse so it can be used within the VM. Dismiss the message with \u0026ldquo;OK\u0026rdquo;.\nSet up Start up Disk. On my computer it defaulted to G:, just hit OK and let your VM load. You will eventually run into a message saying \u0026ldquo;FATAL: No bootable medium found! System halted.\u0026rdquo; In your VM go to Devices \u0026gt; CD/DVD Devices \u0026gt; \u0026ldquo;Choose a virtual CD/DVD disk file\u0026rdquo;.\n[][3]\nNavigate to and select your OS ISO file you downloaded earlier. Then go to Machine \u0026gt; Reset. Your VM will restart and if you are installing Windows XP, The screen will Turn Blue and will begin the Windows XP installation process.\nYou will eventually end up on this screen:\n[][4]\nOn the bottom you will see the keyboard commands to navigate through the menus. Press \u0026ldquo;ENTER\u0026rdquo; to install. Then select the first option \u0026ldquo;Format the partition using the NTFS file system \u0026rdquo; and hit ENTER. Windows will begin installing your VM. Sit tight. get some coffee. The system will restart on its own when it is ready to setup your OS. Eventually you will end up here:\n[][5]\nClick Next, and accept the agreement, and just keep clicking next (unless you know what you are doing). Give your name, you can give your VM a name, leave your administrator password blank, set up your time, and NEXT your way through the install. More coffee!\nEventually, you will go into the initial Windows user setup pages. You don\u0026rsquo;t need automatic updating, skip the internet setup, and enter your username. Click Finish and windows will load up.\nFirst think you may have noticed is that when you resize the VM window, it does not scale nicely. To fix this and also setup folder and USB access, and other things to make your VM integrate seamlessly, you have to install Guest Additions.\nIf you go to Devices \u0026gt; CD/DVD Devices (as you did earlier) your OS .ISO should be selected. Then go to Devices \u0026gt; \u0026ldquo;Install Guest Additions. Click next and install the files. When a dialogue box pops up saying that the software has not passed Windows Logo testing, just click \u0026ldquo;Continue Anyway\u0026rdquo;. Click FINISH and reboot your VM afterwards.\nWhen your VM reloads, go to View \u0026gt; Auto-resize Guest Display is checked. Your VM will now auto-resize.\nSetting up Shared folders. This will allow you to access your main OS files and folders within your VM without having to use programs such as dropbox to sync files in your VM. This also saves you a lot of space since you do not need to have duplicated files taking up HDD space. To do this, go to Devices \u0026gt; Shared Folders\u0026hellip;\n[][6]\nClick on the folder with a + icon. Select \u0026ldquo;Other\u0026hellip;\u0026rdquo; in the Folder Path dropdown and navigate to a folder you want access to in your VM. I suggest you at least pick your main HDD folder (C:\\ in windows). When you are done Click OK and make sure \u0026ldquo;Auto-mount\u0026rdquo; and \u0026ldquo;Make Permanent\u0026rdquo; check boxes are checked in your Add Share Window. This will automatically mount these folders every time you run your VM. Add as many folders as you need, especially folders that you commonly access.\nFinally under Devices menu bar, make sure \u0026ldquo;Shared Clipboard\u0026rdquo; and \u0026ldquo;Drag\u0026rsquo;n\u0026rsquo;Drop\u0026rdquo; have \u0026ldquo;Bi-directional\u0026rdquo; checked. Finally Restart the VM to apply your settings. Your shared folders will appear as network drives after you restart when you open up \u0026ldquo;My Computer\u0026rdquo;. You will also be able to move files around between VM easily, and things you copy will paste in your main OS. This is useful if you are running SAS in your VM and want to paste the output into your word document in your main OS.\nIf you want access to a USB drive that is plugged into your computer in your VM, go to Devices \u0026gt; USB Devices and select your drive. It will unmount from your main OS and mount it in your VM.\nFinally, if you want you can go to View \u0026gt; Switch to seamless mode to just have the app you are running in your VM without having the rest of your VM. Just pay attention to the dialogue box that pops up after you do this, it will tell you how to get back to your regular VM window.\n[3]: {{ site.baseurl }}/wp-content/uploads/2013/02/01-pickISO.png [4]: {{ site.baseurl }}/wp-content/uploads/2013/02/02-setup01.png [5]: {{ site.baseurl }}/wp-content/uploads/2013/02/03-setup02.png [6]: {{ site.baseurl }}/wp-content/uploads/2013/02/04-VMsetup.png\n","date":1361404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563929150,"objectID":"cac6edae1fe0f113dc18bae4a0461e37","permalink":"/2013/02/21/setting-up-virtualbox/","publishdate":"2013-02-21T00:00:00Z","relpermalink":"/2013/02/21/setting-up-virtualbox/","section":"post","summary":"Edit: March 31, 2014: As an assignment for Software Carpentry I made a screencast to follow along with these steps, here\nInstalling Windows XP in VirtualBox:\nFiles needed:\n1. VirtualBox (VB) Intaller here:\nhttps://www.virtualbox.org/wiki/Downloads\n CD or ISO of OS (for my CUMC/CU friends you can find the windows XP ISO here:\nhttp://cuit.columbia.edu/cuit/software-downloads/operating-system-software/windows-xp  The VB version I am using is 4.2.6\nAfter Installing VB. You want to click \u0026ldquo;New\u0026rdquo; in the toolbar.","tags":["tutorials","Setup","Tutorial","VirtualBox","Windows XP"],"title":"Setting up VirtualBox","type":"post"}]