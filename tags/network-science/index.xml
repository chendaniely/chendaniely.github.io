<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>network science on Academic</title>
    <link>/tags/network-science/</link>
    <description>Recent content in network science on Academic</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Mar 2017 00:10:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/network-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A &#39;Simple&#39; Neural-Network Model</title>
      <link>/post/2017/2017-03-30-simple_neural_network_model/</link>
      <pubDate>Thu, 30 Mar 2017 00:10:00 +0000</pubDate>
      
      <guid>/post/2017/2017-03-30-simple_neural_network_model/</guid>
      <description>

&lt;p&gt;My previous &lt;a href=&#34;http://chendaniely.github.io/research/2016/09/29/expanding_watts_to_lens/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; summarized the main points of the &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/16578874&#34; target=&#34;_blank&#34;&gt;Watts 2002 paper&lt;/a&gt;.
At the very end of the post I mentioned ways we can extend the Watts model with neural-networks.
Here I outline the steps of the neural-network simulation.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;Generally the model goes though an initiation step, a simulation step, and a cleanup step.
In the initiation step, the agents are created and connected in a network.
Then selected agents are seeded with a value before continuing onto the simulation step.
In the simulation step, &amp;lsquo;steps&amp;rsquo; for each time tick are performed.
This includes picking agents to be updated, making the necessary calculations for an update,
and updating agents in a simultaneous or sequential manner.
Agent states in teach time tick is saved to be written to a (CSV) file.
The cleanup step is mainly there when programming the model.
It saves any unwritten states to the file, and
any other related tasks of that nature.&lt;/p&gt;

&lt;h1 id=&#34;mann&#34;&gt;MANN&lt;/h1&gt;

&lt;p&gt;The neural networks used in MANN use the &lt;a href=&#34;http://web.stanford.edu/group/mbc/LENSManual/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;the light, efficient network simulator&amp;rdquo; (LENS)&lt;/a&gt;.
I created a (inefficient) python wrapper around LENS, &lt;a href=&#34;https://github.com/chendaniely/pylens&#34; target=&#34;_blank&#34;&gt;pylens&lt;/a&gt;.
This mainly stems from the fact my advisor used LENS for his work in the past,
and interface with it via the command line with input files.
The inefficient stems from the fact that I am having python write our the necessary input files for the neural network,
and making command-line &lt;code&gt;subprocess&lt;/code&gt; calls to LENS.&lt;/p&gt;

&lt;h2 id=&#34;initiation&#34;&gt;Initiation&lt;/h2&gt;

&lt;p&gt;$n$ agents are created for the model.
When an agent is created, a feed-forward neural network with randomized weights to represent its state.
A prototype that will be used to train the neural network is assigned to the agent.&lt;/p&gt;

&lt;p&gt;The agent&amp;rsquo;s will all be connected to a network.
For example in the &lt;code&gt;networkx&lt;/code&gt; list of graph generators,
the [&lt;code&gt;fast_gnp_random_graph&lt;/code&gt;][5] takes the number of agents, $n$,
and a probability, $p$, of an edge being created.&lt;/p&gt;

&lt;p&gt;To train the neural network, it needs a set of example files to train.
The example file consists of $k$ (50) examples.
Each example is a randomized form of the prototype.
The amount of randomization is determined by parameter $\delta$.
To incorporate heterogeneity in the model,
we also mutate the prototype that is used to create the example files.
This mutation rate is designated as $\epsilon$.&lt;/p&gt;

&lt;p&gt;How well the agent gets trained is parameterized with the &lt;em&gt;criterion&lt;/em&gt; variable.
It is analogous to the amount of error the neural network can make before it finishes training.
That is, the smaller the criterion, the smaller the error, the more accurate it can represent the training set.
All agents stop training after 1000 epochs (training cycles for the neural network).&lt;/p&gt;

&lt;p&gt;Once the neural network is trained, its weights are output to a file to be loaded during the simulation step.&lt;/p&gt;

&lt;p&gt;All agents begin with a state of 0.
That is the first layer of the neural network all have values of 0.
We then select agents to be seeded.
The seeded agent is presented (via an example file) the non-$\epsilon$-mutated prototype.
We cycle this value through the neural network with one epoch,
and the output of the neural network is assigned to the agent&amp;rsquo;s state.&lt;/p&gt;

&lt;p&gt;We then begin the simulation part of the model.&lt;/p&gt;

&lt;h2 id=&#34;simulation&#34;&gt;Simulation&lt;/h2&gt;

&lt;p&gt;The simulation runs in &lt;em&gt;steps&lt;/em&gt;.
These steps designate the amount of &lt;em&gt;time ticks&lt;/em&gt; the simulation runs for.
The same process occurs during each time tick until the simulation ends.&lt;/p&gt;

&lt;p&gt;All agents will be randomly selected (without replacement) to undergo an update process.
The update process looks at all the agent&amp;rsquo;s neighbors with a directed edge coming &lt;em&gt;into&lt;/em&gt; the selected agent
(i.e., neighbors or predecessors),
and randomly selects one of the agents to be the &lt;em&gt;influencer&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The influencer writes our its current state as an example file.
This example file is used by the agent&amp;rsquo;s neural network and the output is assigned as the agent&amp;rsquo;s new state.&lt;/p&gt;

&lt;p&gt;In a sequential update process,
the agent&amp;rsquo;s new state is assigned as described above.
However, during simultaneous updating,
the new state will be stored into a temporary state value,
so that if the agent is used as an influencer to another agent,
the original agent state value will be written out to the example file.
In simultaneous updating, only after all agents have undergone the update process will they
assign the temporary state as their new state.&lt;/p&gt;

&lt;p&gt;The agent states are saved to a file at the end of the initiation step and the
end of each simulation time tick.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Expanding the Watts Model</title>
      <link>/post/2016/2016-09-29-expanding_watts_to_lens/</link>
      <pubDate>Thu, 29 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/2016-09-29-expanding_watts_to_lens/</guid>
      <description>

&lt;p&gt;I wrote a &lt;a href=&#34;http://chendaniely.github.io/research/2016/08/31/a_simple_model_of_global_cascades_on_random_networks/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; and
gave a &lt;a href=&#34;www.google.com&#34; target=&#34;_blank&#34;&gt;seminar talk&lt;/a&gt; about the &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/16578874&#34; target=&#34;_blank&#34;&gt;Watts 2002 paper&lt;/a&gt;.
The next step is thinking about expanding the &lt;em&gt;binary decisions with externalities&lt;/em&gt; model to a
&lt;em&gt;psychological plausible decisions with externalities&lt;/em&gt; model.
This has been the goal of the &lt;a href=&#34;https://github.com/chendaniely/mann2&#34; target=&#34;_blank&#34;&gt;multi-agent neural-network (MANN) project&lt;/a&gt;.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h1 id=&#34;context-from-the-watts-model&#34;&gt;Context from the Watts model&lt;/h1&gt;

&lt;p&gt;From an information diffusion perspective,
a cascade is the spread of information from an initial set (seed) of individuals (nodes).
Watts talks about features of a network that create a &lt;em&gt;global cascade&lt;/em&gt;,
which is simply a cascade of &amp;lsquo;sufficient&amp;rsquo; size.
The ability for a node to receive and pass on a bit of information is it&amp;rsquo;s &lt;em&gt;vulnerability&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Cascades are triggered by the interaction between 2 properties &amp;ndash;
a network property (degree centrality) and a node property (threshold).
If we look at these 2 properties as mutually exclusive from one another,
then nodes with smaller degree centrality will be less vulnerable compared to nodes with higher degree centrality,
and nodes with lower threshold values will be more vulnerable than nodes with higher threshold values.
This is because nodes with fewer connections have a smaller probability of getting information passed onto it,
and nodes with lower thresholds need fewer bits of information to propagate a signal.&lt;/p&gt;

&lt;p&gt;Although degree centrality (&lt;em&gt;local dependencies&lt;/em&gt;) and &lt;em&gt;(fractional) thresholds&lt;/em&gt; are integral parts of
the binary decisions with externalities model,
another key aspect is &lt;em&gt;heterogeneity&lt;/em&gt;.
For a given network, not every node will have the same degree centrality,
and not every node will have the same fractional threshold value.
As a matter of fact, any combination of high/low degree centrality and high/low fractional threshold is possible,
and each combination has an effect on a particular node&amp;rsquo;s vulnerability.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Degree (k)&lt;/th&gt;
&lt;th&gt;Threshold ($\phi$)&lt;/th&gt;
&lt;th&gt;Vulnerability&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;expanding-the-watts-model-with-neural-networks&#34;&gt;Expanding the Watts model with neural networks&lt;/h1&gt;

&lt;p&gt;The next step is to explore ways the Watts model can be expanded.
The rationale being decisions, and similarly, behaviors, are binary,
but the process of making a decision and performing an action are not.
&lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0062490&#34; target=&#34;_blank&#34;&gt;Mark Orr, Roxanne Thrush, and David C. Plaut&lt;/a&gt;
suggest incorporating the &lt;a href=&#34;https://en.wikipedia.org/wiki/Theory_of_reasoned_action&#34; target=&#34;_blank&#34;&gt;Theory of Reasoned Action (TRA)&lt;/a&gt; as the
theoretical framework to describe how behaviors emerge and an
&lt;a href=&#34;https://en.wikipedia.org/wiki/Autoassociative_memory&#34; target=&#34;_blank&#34;&gt;auto&lt;/a&gt;-&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/009813549280051A&#34; target=&#34;_blank&#34;&gt;associator&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34; target=&#34;_blank&#34;&gt;neural-network&lt;/a&gt;
as the computational framework to simulate nodes.&lt;/p&gt;

&lt;p&gt;A neural network allows for a multi-dimensional node, not just a simple binary node.
The key feature of an &lt;em&gt;auto associative neural network&lt;/em&gt; is its ability to learn and reproduce an identity (a.k.a, prototype) given a portion of inputs.
From the Wikipedia &lt;a href=&#34;https://en.wikipedia.org/wiki/Autoassociative_memory&#34; target=&#34;_blank&#34;&gt;page&lt;/a&gt;, if we are presented the quote &amp;ldquo;I came, I saw&amp;hellip;&amp;rdquo;,
we should be able to complete the rest of the quote: &amp;ldquo;&amp;hellip; I conquered&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;If we re-conceptualize the Watts model, the prototype is the bit of information each node can pass to its neighbors (i.e., a state of 1).
Since this is a binary model, then each node can be thought to be trained to have an output of 1,
but has a state of 0 until the proper inputs are met (local dependencies and fractional thresholds), and the node has a state of 1.&lt;/p&gt;

&lt;p&gt;From the neural network perspective, every node is trained to a particular prototype (e.g., the vector: &lt;code&gt;[1, 1, 1, 0, 0, 0]&lt;/code&gt;),
and has a state of 0 (e.g., the vector: &lt;code&gt;[0, 0, 0, 0, 0, 0]&lt;/code&gt;)
until proper inputs are met, the neural network will output the prototype (e.g., the vector: &lt;code&gt;[1, 1, 1, 0, 0, 0]&lt;/code&gt;).
Additionally, the same output should be returned, if only a portion of the prototype vector is presented.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-neural-network-model-analogous-to-the-watts-model&#34;&gt;Creating a neural network model analogous to the Watts model&lt;/h2&gt;

&lt;p&gt;While there are many questions and solutions on how to relate the Watts model to this newly defined neural-network model,
two immediately come to mind.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How do we measure if information has been passed from one node to another?&lt;/li&gt;
&lt;li&gt;How do we map fractional thresholds between the two models?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since we know the trained/learned prototype for each node,
one way to know if information passed from the seed node (and subsequent nodes) to a new node is to look at how &amp;lsquo;similar&amp;rsquo;
the output of the new node is to the prototype.
Similarity can be measured in many ways, one such way is to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity&#34; target=&#34;_blank&#34;&gt;cosine similarity&lt;/a&gt;.
Once the similarity is calculated, we can apply another threshold, $\theta$,
that signals the propagation of information.
This new threshold can be used to calculate a binary state, $S_1$ that is analogous to the Watts model state, $S_0$.
We can use $\theta$ to calculate how many nodes are different from the node of interest
(this can also be thought of as some function of $S_1$),
and use the same threshold, $\phi$, from the Watts model to determine whether the node of interest will propagate our signal in the network.&lt;/p&gt;

&lt;p&gt;However, since our definition of &amp;lsquo;similarity&amp;rsquo; and &amp;lsquo;information propagation&amp;rsquo; relies on the &lt;em&gt;output&lt;/em&gt; of our neural network,
we need to give the neural network a set of &lt;em&gt;inputs&lt;/em&gt;.
This leads to anther question:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How do we provide inputs to nodes that have been designated to carry and propagate information in the network?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can do this a few ways.
For each node that will propagate information we can assign the input as&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the prototype (e.g., the vector: &lt;code&gt;[1, 1, 1, 0, 0, 0]&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;the state (a.k.a output) of 1 of the &amp;lsquo;different&amp;rsquo; connected nodes&lt;/li&gt;
&lt;li&gt;the average state of all of the &amp;lsquo;different&amp;rsquo; connected nodes&lt;/li&gt;
&lt;li&gt;the average state of all connected nodes&lt;/li&gt;
&lt;li&gt;present each &amp;lsquo;different&amp;rsquo; node in a random order and have the neural network cycle though them&lt;/li&gt;
&lt;li&gt;present all connected nodes in a random order and have the neural network cycle though them&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Other than the first option of using the prototype,
the other methods will still preserve the differences in degree centrality between the nodes.
For example, if a node only has one neighbor and it will be updated with 1 neighbor,
it will always be presented with the same inputs.
However if a node has multiple neighbors, the probability of the same neighbor picked every time is proportional to the number neighbors.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-simpler-neural-network-model&#34;&gt;Creating a simpler neural-network model&lt;/h2&gt;

&lt;p&gt;A simpler approach would be to have each node in the network trained against the prototype.
Seed a single node with the prototype, and run the model without any additional thresholding rule ($\theta$),
and have each node use a random 1 neighbor&amp;rsquo;s state (i.e., output) and its input.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Simple Model of Global Cascades on Random Networks</title>
      <link>/post/2016/2016-08-31-a_simple_model_of_global_cascades_on_random_networks/</link>
      <pubDate>Wed, 31 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016/2016-08-31-a_simple_model_of_global_cascades_on_random_networks/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://twitter.com/duncanjwatts&#34; target=&#34;_blank&#34;&gt;Duncan J Watts&lt;/a&gt; wrote a paper that
was published in 2002 titled &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/16578874&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;A simple model of global
cascades on random networks&amp;rdquo;&lt;/a&gt; in the Proceedings of the National Academy of Sciences (PNAS).
It&amp;rsquo;s a seminal paper in my current
work on information diffusion in (social) networks.  Watts shows how
the interactions between local dependencies, fractional threshold, and
heterogeneity relate to information cascades in networks.  My work
builds on these ideas, so it&amp;rsquo;s important to have a strong
understanding of the terms and model specifications outlined in the
paper.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;I&amp;rsquo;ve been working with my Masters and Doctoral advisor,
&lt;a href=&#34;http://mark-orr.github.io/&#34; target=&#34;_blank&#34;&gt;Mark Orr&lt;/a&gt; on a simulation platform that
incorporates artificial neural networks within an agent-based
simulation.  The project is called &amp;ldquo;Multi-Agent Neural Network&amp;rdquo; (MANN)
and it was my
&lt;a href=&#34;http://chendaniely.github.io/tutorials/2015/02/05/open-sourcing-a-python-project/&#34; target=&#34;_blank&#34;&gt;first&lt;/a&gt;
open source project; the code can be found in two parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chendaniely/multi-agent-neural-network&#34; target=&#34;_blank&#34;&gt;The Python module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chendaniely/multidisciplinary-diffusion-model-experiments&#34; target=&#34;_blank&#34;&gt;The simulation code&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;rsquo;m currently working on a refactoring a lot of the old code here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chendaniely/mann2&#34; target=&#34;_blank&#34;&gt;MANN2 Python module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chendaniely/mann2_simulations&#34; target=&#34;_blank&#34;&gt;MANN2 simulation code&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;the-paper&#34;&gt;The paper&lt;/h1&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;These are my notes on the paper.  In no way shape or form am I claiming the content below as my own work, it all comes from &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/16578874&#34; target=&#34;_blank&#34;&gt;the paper&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;Binary decisions with externalities&lt;/em&gt; is a general class of problems
that can be used to model a wide variety of real-world problems.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Cascades limited by connectivity&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Connectivity can play a role in global cascades.
Fewer connections mean a lesser chance of information spreading within a network.
The abstract references

&lt;ul&gt;
&lt;li&gt;power law&lt;/li&gt;
&lt;li&gt;cluster size distribution in standard percolation theory&lt;/li&gt;
&lt;li&gt;avalanches in self-organized criticality&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cascades limited by local stability&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When a network is highly connected, cascades will occur when a
threshold number of connected nodes incorporate the information.
Additionally, these types of networks have a bimodal distribution
of a global cascade, either one will happen or it will not.&lt;/li&gt;
&lt;li&gt;When the &lt;em&gt;threshold&lt;/em&gt; of agent&amp;rsquo;s to incorporate a bit of
information is heterogeneous, the entire system has a higher
chance of a global information cascade.

&lt;ul&gt;
&lt;li&gt;When the agent&amp;rsquo;s threshold is heterogeneous, the ones who
have a lower threshold are &lt;em&gt;more&lt;/em&gt; &lt;em&gt;vulnerable&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Conversely, when the &lt;em&gt;degree&lt;/em&gt; distribution is heterogeneous, the
system is less likely to have a information cascade.

&lt;ul&gt;
&lt;li&gt;When an agent&amp;rsquo;s degree centrality is heterogeneous, the ones
who have a higher degree will be &lt;em&gt;less&lt;/em&gt; vulnerable.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The model Watts outlines has 3 unique features:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;local dependencies&lt;/li&gt;
&lt;li&gt;fractional thresholds&lt;/li&gt;
&lt;li&gt;heterogeneity&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Network heterogeneity and threshold heterogeneity are not equivalent.&lt;/p&gt;

&lt;p&gt;Goals of the paper:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Probability of a global cascade from a single node&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;definitions&#34;&gt;Definitions&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;cascade&lt;/strong&gt;: event of any size triggered by an initial seed&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;global cascades&lt;/strong&gt;: a cascade that occupies a finite fraction of an infinite network.  A sufficiently large cascade.  More than a fixed fraction of a large, but finite network.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;diffusion&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;random network&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;agents&lt;/strong&gt;: nodes of a network&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;neighbors&lt;/strong&gt;: all relevant incoming signals to an agent (usually other agents who have a directed edge to the agent in question)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;connectivity&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;power law distribution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cluster size&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;local stability&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$\phi$&lt;/strong&gt;: threshold

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;$f(\phi)$&lt;/strong&gt;: distribution where $\phi$ is drawn from

&lt;ul&gt;
&lt;li&gt;unit interval and normalized such that the area under the distribution is 1&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$n$&lt;/strong&gt;: number of agents in the network&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$k$&lt;/strong&gt;: number of neighbors each agent is connected to

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;$p_k$&lt;/strong&gt;: probability of an agent being connected to $k$ neighbors (degree distribution)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$z$&lt;/strong&gt;: average number of neighbors $\langle k \rangle$ (coordination number)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$\phi_0$&lt;/strong&gt;: the number of agents that are seeded with 1 at the start of the simulation

&lt;ul&gt;
&lt;li&gt;$\phi_0 \ll 1$&lt;/li&gt;
&lt;li&gt;See definition on &lt;em&gt;small seed&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;vulnerable vertex&lt;/strong&gt;:  an agent that has a small threshold ($\phi \le \frac{1}{k}$) or has a degree, $k$ such that $k \le K = [\frac{1}{\phi}]$

&lt;ul&gt;
&lt;li&gt;$\rho_k$: probability that an agent with degree $k$ is vulnerable

&lt;ul&gt;
&lt;li&gt;$\rho_k = P[\phi \le \frac{1}{k}]$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;stable vertex&lt;/strong&gt;: one that is not vulnerable. Stable vertices do not flip states at the start of a simulation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;small seed&lt;/strong&gt;: three orders of magnitude less than the system size, $n$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;innovator&lt;/strong&gt;: an agent that is seeded&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;early adopter&lt;/strong&gt;: a vulnerable agent&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;early and late majority&lt;/strong&gt;: agents who can be influenced if exposed to multiple early adopters&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;model-specification&#34;&gt;Model Specification&lt;/h1&gt;

&lt;h2 id=&#34;how-the-simulation-runs&#34;&gt;How the Simulation runs&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;$n$ agents in a network start off with a state of 0&lt;/li&gt;
&lt;li&gt;Individual agents can only have a state that is either 0 or 1&lt;/li&gt;
&lt;li&gt;Each agent has $k$ neighbors&lt;/li&gt;
&lt;li&gt;An agent gets a new state of 1, if a fraction of its neighbors, $\phi$ are also 1.

&lt;ul&gt;
&lt;li&gt;Otherwise an agent gets a new state of 0.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;During each time step, the population evolves:

&lt;ol&gt;
&lt;li&gt;Update states in random, asynchronous order using the threshold rule&lt;/li&gt;
&lt;li&gt;Once an agent has a state of 1, it will stay at 1 for the remainder of the simulation&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;how-the-model-is-parameterized&#34;&gt;How the model is parameterized&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;$\phi$ and $k$ &lt;em&gt;may&lt;/em&gt; be heterogeneous

&lt;ul&gt;
&lt;li&gt;To simplify the simulations, the paper has a homogeneous threshold, $\phi$

&lt;ul&gt;
&lt;li&gt;$f(\phi) = \delta(\phi - \phi_*)$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The network is a uniform random graph&lt;/li&gt;
&lt;li&gt;A small seed&lt;/li&gt;
&lt;li&gt;Any pair of vertices is connected with probability $p = \frac{z}{n}$

&lt;ol&gt;
&lt;li&gt;in uniform random graphs $p_k = \text{the Poisson distribution}$&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;$n = 10,000$&lt;/li&gt;
&lt;li&gt;100 random runs of each simulation&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;values-used-in-figures&#34;&gt;Values used in figures&lt;/h2&gt;

&lt;h3 id=&#34;fig-1&#34;&gt;Fig 1&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F1/&#34; target=&#34;_blank&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F1/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Shows for a given threshold, $\phi$ and average number of neighbors, $z$,
where the cascade condition is satisfied.
This is an analytical solution, not a simulated solution&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$\phi$: range from 0.10 to 0.25 in increments of 0.01&lt;/li&gt;
&lt;li&gt;$z$: range from 0 to 15&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;fig-2&#34;&gt;Fig 2&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F2/&#34; target=&#34;_blank&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F2/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Looking at a specific threshold value, $\phi = 0.18$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$n$ = 10,000&lt;/li&gt;
&lt;li&gt;1,000 random realizations of the network and initial conditions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;fig-3&#34;&gt;Fig 3&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F3/&#34; target=&#34;_blank&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F3/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;a log-log plot of the cascade size vs cumulative distribution&lt;/p&gt;

&lt;h3 id=&#34;fig-4&#34;&gt;Fig 4&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F4/&#34; target=&#34;_blank&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC122850/figure/F4/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/16578874&#34; target=&#34;_blank&#34;&gt;http://www.ncbi.nlm.nih.gov/pubmed/16578874&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
